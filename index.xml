<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Connor Greenwell</title>
    <link>https://connorgreenwell.github.io/</link>
    <description>Recent content on Connor Greenwell</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 04 May 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://connorgreenwell.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>One paper published at IGARSS 2018</title>
      <link>https://connorgreenwell.github.io/news/what_goes_where/</link>
      <pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/what_goes_where/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What Goes Where: Predicting Object Distributions From Above</title>
      <link>https://connorgreenwell.github.io/publication/what_goes_where/</link>
      <pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/publication/what_goes_where/</guid>
      <description>Abstract In this work, we propose a cross-view learning approach, in which images captured from a ground-level view are used as weakly supervised annotations for interpreting overhead imagery. The outcome is a convolutional neural network for overhead imagery that is capable of predicting the type and count of objects that are likely to be seen from a ground-level perspective. We demonstrate our approach on a large dataset of geotagged ground-level and overhead imagery and find that our network captures semantically meaningful features, despite being trained without manual annotations.</description>
    </item>
    
    <item>
      <title>An Analysis of Fouls, By and Drawn, From One Game in the 2016 NBA Regular Season</title>
      <link>https://connorgreenwell.github.io/blog/fouls_by_and_drawn/</link>
      <pubDate>Tue, 21 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/blog/fouls_by_and_drawn/</guid>
      <description>req = requests.get(&amp;quot;https://www.basketball-reference.com/boxscores/201610260IND.html&amp;quot;) box_html = str(req.content) boxes = pd.read_html(box_html)  minutes_played = pd.concat([boxes[0][[0, 1]], boxes[2][[0, 1]]]) minutes_played.columns = [&amp;quot;Name&amp;quot;, &amp;quot;MP&amp;quot;] minutes_played.drop( minutes_played.index[minutes_played[&amp;quot;MP&amp;quot;].str.contains(&amp;quot;MP&amp;quot;)], inplace=True) minutes_played.drop( minutes_played.index[minutes_played[&amp;quot;MP&amp;quot;].str.contains(&amp;quot;Did Not Play&amp;quot;)], inplace=True) minutes_played.drop( minutes_played.index[minutes_played[&amp;quot;Name&amp;quot;].str.contains(&amp;quot;Team Totals&amp;quot;)], inplace=True) # convert &amp;quot;time remaining in quarter&amp;quot; to &amp;quot;time elapsed in game&amp;quot; def stime_to_float(stime): import datetime dt = datetime.datetime.strptime(stime, &amp;quot;%M:%S&amp;quot;) return dt.minute + (dt.second / 60.0) minutes_played[&amp;quot;MP&amp;quot;] = minutes_played[&amp;quot;MP&amp;quot;].apply(stime_to_float) def short_name(name): first, last = name.split() return &amp;quot;{}. {}&amp;quot;.</description>
    </item>
    
    <item>
      <title>Started PhD at UK</title>
      <link>https://connorgreenwell.github.io/news/started_phd/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/started_phd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Fast Method for Estimating Transient Scene Attributes</title>
      <link>https://connorgreenwell.github.io/publication/deeptransient/</link>
      <pubDate>Tue, 28 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/publication/deeptransient/</guid>
      <description>Abstract We propose to use deep convolutional neural networks to estimate the transient attributes of a scene from a single image. Transient scene attributes describe both the objective conditions, such as the weather, time of day, and the season, and subjective properties of a scene, such as whether or not the scene seems busy. Recently, convolutional neural networks have been used to achieve state-of-the-art results for many vision problems, from object detection to scene classification, but have not previously been used for estimating transient attributes.</description>
    </item>
    
    <item>
      <title>A Fast Method for Estimating Transient Scene Attributes accepted to WACV</title>
      <link>https://connorgreenwell.github.io/news/deeptransient/</link>
      <pubDate>Tue, 28 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/deeptransient/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graduated from UK CS Undergrad</title>
      <link>https://connorgreenwell.github.io/news/graduated_undergrad/</link>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/graduated_undergrad/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeepFocal: A Method for Direct Focal Length Estimation</title>
      <link>https://connorgreenwell.github.io/publication/deepfocal/</link>
      <pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/publication/deepfocal/</guid>
      <description>âŠ•   Abstract Estimating the focal length of an image is an important preprocessing step for many applications. Despite this, existing methods for single-view focal length estimation are limited in that they require particular geometric calibration objects, such as orthogonal vanishing points, co-planar circles, or a calibration grid, to occur in the field of view. In this work, we explore the application of a deep convolutional neural network, trained on natural images obtained from Internet photo collections, to directly estimate the focal length using only raw pixel intensities as input features.</description>
    </item>
    
    <item>
      <title>DeepFocal: A Method for Direct Focal Length Estimation accepted to ICIP</title>
      <link>https://connorgreenwell.github.io/news/deepfocal/</link>
      <pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/deepfocal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Large-Scale Geo-Facial Image Analysis</title>
      <link>https://connorgreenwell.github.io/publication/geofacial/</link>
      <pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/publication/geofacial/</guid>
      <description>Abstract While face analysis from images is a well-studied area, little work has explored the dependence of facial appearance on the geographic location from which the image was captured. To fill this gap, we constructed GeoFaces, a large dataset of geotagged face images, and used it to examine the geo-dependence of facial features and attributes, such as ethnicity, gender, or the presence of facial hair. Our analysis illuminates the relationship between raw facial appearance, facial attributes, and geographic location, both globally and in selected major urban areas.</description>
    </item>
    
    <item>
      <title>Large-Scale Geo-Facial Image Analysis accepted to EURASIP</title>
      <link>https://connorgreenwell.github.io/news/geofacial/</link>
      <pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/geofacial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GeoFaceExplorer: Exploring the Geo-Dependence of Facial Attributes</title>
      <link>https://connorgreenwell.github.io/publication/geofaceexplorer/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/publication/geofaceexplorer/</guid>
      <description>Abstract The images uploaded to social networking websites are a rich source of information about the appearance of people around the world. We present a system, GeoFaceExplorer, for collecting, processing, browsing, and analyzing this data. GeoFaceExplorer allows for the crowdsourced collection of human facial images, as well as automated and interactive visual analysis of the geo-dependence of facial appearance and visual attributes, such as ethnicity, gender, and whether or not a person is wearing glasses.</description>
    </item>
    
    <item>
      <title>GeoFaceExplorer: Exploring the Geo-Dependence of Facial Attributes accepted to ACM SIGSPATIAL (GEOCROWD)</title>
      <link>https://connorgreenwell.github.io/news/geofaceexplorer/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://connorgreenwell.github.io/news/geofaceexplorer/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>