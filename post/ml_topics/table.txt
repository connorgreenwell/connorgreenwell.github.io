<tr><td>Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Unsupervised_Learning_of_Probably_Symmetric_Deformable_3D_Objects_From_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shangzhe Wu,  Christian Rupprecht,  Andrea Vedaldi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Footprints and Free Space From a Single Color Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Watson_Footprints_and_Free_Space_From_a_Single_Color_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jamie Watson,  Michael Firman,  Aron Monszpart,  Gabriel J. Brostow</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0444</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0689</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0361</td> <td>0.0000</td></tr>
<tr><td>Dynamic Fluid Surface Reconstruction Using Deep Neural Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Thapa_Dynamic_Fluid_Surface_Reconstruction_Using_Deep_Neural_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Simron Thapa,  Nianyi Li,  Jinwei Ye</td> <td>0.0000</td> <td>0.0000</td> <td>0.0467</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0188</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CvxNet: Learnable Convex Decomposition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_CvxNet_Learnable_Convex_Decomposition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Boyang Deng,  Kyle Genova,  Soroosh Yazdani,  Sofien Bouaziz,  Geoffrey Hinton,  Andrea Tagliasacchi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0258</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>BSP-Net: Generating Compact Meshes via Binary Space Partitioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_BSP-Net_Generating_Compact_Meshes_via_Binary_Space_Partitioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiqin Chen,  Andrea Tagliasacchi,  Hao Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0004</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0770</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0096</td></tr>
<tr><td>Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction for Indoor Scenes From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nie_Total3DUnderstanding_Joint_Layout_Object_Pose_and_Mesh_Reconstruction_for_Indoor_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yinyu Nie,  Xiaoguang Han,  Shihui Guo,  Yujian Zheng,  Jian Chang,  Jian Jun Zhang</td> <td>0.1072</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0040</td></tr>
<tr><td>Generating and Exploiting Probabilistic Monocular Depth Estimates <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xia_Generating_and_Exploiting_Probabilistic_Monocular_Depth_Estimates_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihao Xia,  Patrick Sullivan,  Ayan Chakrabarti</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0154</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0045</td></tr>
<tr><td>Neural Cages for Detail-Preserving 3D Deformations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yifan_Neural_Cages_for_Detail-Preserving_3D_Deformations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wang Yifan,  Noam Aigerman,  Vladimir G. Kim,  Siddhartha Chaudhuri,  Olga Sorkine-Hornung</td> <td>0.0000</td> <td>0.0000</td> <td>0.0273</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Saito_PIFuHD_Multi-Level_Pixel-Aligned_Implicit_Function_for_High-Resolution_3D_Human_Digitization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shunsuke Saito,  Tomas Simon,  Jason Saragih,  Hanbyul Joo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0302</td> <td>0.0000</td> <td>0.0000</td> <td>0.0085</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Lighting-Invariant Point Processor for Shading <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Heal_A_Lighting-Invariant_Point_Processor_for_Shading_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kathryn Heal,  Jialiang Wang,  Steven J. Gortler,  Todd Zickler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ActiveMoCap: Optimized Viewpoint Selection for Active Human Motion Capture <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kiciroglu_ActiveMoCap_Optimized_Viewpoint_Selection_for_Active_Human_Motion_Capture_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sena Kiciroglu,  Helge Rhodin,  Sudipta N. Sinha,  Mathieu Salzmann,  Pascal Fua</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2897</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Peek-a-Boo: Occlusion Reasoning in Indoor Scenes With Plane Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Peek-a-Boo_Occlusion_Reasoning_in_Indoor_Scenes_With_Plane_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziyu Jiang,  Buyu Liu,  Samuel Schulter,  Zhangyang Wang,  Manmohan Chandraker</td> <td>0.0000</td> <td>0.0000</td> <td>0.0167</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0088</td></tr>
<tr><td>Multi-Modal Domain Adaptation for Fine-Grained Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Munro_Multi-Modal_Domain_Adaptation_for_Fine-Grained_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jonathan Munro,  Dima Damen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2477</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0276</td> <td>0.0000</td> <td>0.0725</td> <td>0.0000</td> <td>0.2419</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Evolving Losses for Unsupervised Video Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Piergiovanni_Evolving_Losses_for_Unsupervised_Video_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>AJ Piergiovanni,  Anelia Angelova,  Michael S. Ryoo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0528</td> <td>0.0000</td> <td>0.0020</td></tr>
<tr><td>Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Disentangling_and_Unifying_Graph_Convolutions_for_Skeleton-Based_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziyu Liu,  Hongwen Zhang,  Zhenghao Chen,  Zhiyong Wang,  Wanli Ouyang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1031</td> <td>0.0000</td> <td>0.0065</td></tr>
<tr><td>A Multigrid Method for Efficiently Training Video Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_A_Multigrid_Method_for_Efficiently_Training_Video_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chao-Yuan Wu,  Ross Girshick,  Kaiming He,  Christoph Feichtenhofer,  Philipp Krahenbuhl</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0157</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0358</td> <td>0.0000</td></tr>
<tr><td>Ego-Topo: Environment Affordances From Egocentric Video <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nagarajan_Ego-Topo_Environment_Affordances_From_Egocentric_Video_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tushar Nagarajan,  Yanghao Li,  Christoph Feichtenhofer,  Kristen Grauman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Generative Hybrid Representations for Activity Forecasting With No-Regret Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guan_Generative_Hybrid_Representations_for_Activity_Forecasting_With_No-Regret_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiaqi Guan,  Ye Yuan,  Kris M. Kitani,  Nicholas Rhinehart</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Skeleton-Based Action Recognition With Shift Graph Convolutional Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ke Cheng,  Yifan Zhang,  Xiangyu He,  Weihan Chen,  Jian Cheng,  Hanqing Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0213</td> <td>0.0000</td> <td>0.0000</td> <td>0.1635</td> <td>0.0000</td> <td>0.0215</td></tr>
<tr><td>Predicting Goal-Directed Human Attention Using Inverse Reinforcement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Predicting_Goal-Directed_Human_Attention_Using_Inverse_Reinforcement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhibo Yang,  Lihan Huang,  Yupei Chen,  Zijun Wei,  Seoyoung Ahn,  Gregory Zelinsky,  Dimitris Samaras,  Minh Hoai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0273</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>X3D: Expanding Architectures for Efficient Video Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Feichtenhofer_X3D_Expanding_Architectures_for_Efficient_Video_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christoph Feichtenhofer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0975</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0013</td></tr>
<tr><td>Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Dynamic_Multiscale_Graph_Neural_Networks_for_3D_Skeleton_Based_Human_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maosen Li,  Siheng Chen,  Yangheng Zhao,  Ya Zhang,  Yanfeng Wang,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0069</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0235</td></tr>
<tr><td>Use the Force, Luke! Learning to Predict Physical Forces by Simulating Effects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ehsani_Use_the_Force_Luke_Learning_to_Predict_Physical_Forces_by_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kiana Ehsani,  Shubham Tulsiani,  Saurabh Gupta,  Ali Farhadi,  Abhinav Gupta</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DaST: Data-Free Substitute Training for Adversarial Attacks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_DaST_Data-Free_Substitute_Training_for_Adversarial_Attacks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mingyi Zhou,  Jing Wu,  Yipeng Liu,  Shuaicheng Liu,  Ce Zhu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0794</td> <td>0.0000</td> <td>0.1688</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0066</td></tr>
<tr><td>Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mohapatra_Towards_Verifying_Robustness_of_Neural_Networks_Against_A_Family_of_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jeet Mohapatra,  Tsui-Wei Weng,  Pin-Yu Chen,  Sijia Liu,  Luca Daniel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_The_Secret_Revealer_Generative_Model-Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuheng Zhang,  Ruoxi Jia,  Hengzhi Pei,  Wenxiao Wang,  Bo Li,  Dawn Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0673</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2351</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1426</td> <td>0.0117</td></tr>
<tr><td>A Self-supervised Approach for Adversarial Robustness <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Naseer_A_Self-supervised_Approach_for_Adversarial_Robustness_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muzammal Naseer,  Salman Khan,  Munawar Hayat,  Fahad Shahbaz Khan,  Fatih Porikli</td> <td>0.0238</td> <td>0.0000</td> <td>0.0403</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2985</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Adversarial_Vertex_Mixup_Toward_Better_Adversarially_Robust_Generalization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Saehyung Lee,  Hyungyu Lee,  Sungroh Yoon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2132</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>How Does Noise Help Robustness? Explanation and Exploration under the Neural SDE Framework <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_How_Does_Noise_Help_Robustness_Explanation_and_Exploration_under_the_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuanqing Liu,  Tesi Xiao,  Si Si,  Qin Cao,  Sanjiv Kumar,  Cho-Jui Hsieh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0439</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unpaired Image Super-Resolution Using Pseudo-Supervision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Maeda_Unpaired_Image_Super-Resolution_Using_Pseudo-Supervision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shunta Maeda</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0498</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0091</td></tr>
<tr><td>Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kolouri_Universal_Litmus_Patterns_Revealing_Backdoor_Attacks_in_CNNs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Soheil Kolouri,  Aniruddha Saha,  Hamed Pirsiavash,  Heiko Hoffmann</td> <td>0.0000</td> <td>0.0000</td> <td>0.0975</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Robustness Guarantees for Deep Neural Networks on Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Robustness_Guarantees_for_Deep_Neural_Networks_on_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Min Wu,  Marta Kwiatkowska</td> <td>0.0000</td> <td>0.0000</td> <td>0.0920</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0696</td> <td>0.0000</td> <td>0.2206</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Benchmarking Adversarial Robustness on Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yinpeng Dong,  Qi-An Fu,  Xiao Yang,  Tianyu Pang,  Hang Su,  Zihao Xiao,  Jun Zhu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0460</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1977</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>What It Thinks Is Important Is Important: Robustness Transfers Through Input Gradients <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chan_What_It_Thinks_Is_Important_Is_Important_Robustness_Transfers_Through_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alvin Chan,  Yi Tay,  Yew-Soon Ong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0116</td></tr>
<tr><td>Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Transferable_Controllable_and_Inconspicuous_Adversarial_Attacks_on_Person_Re-identification_With_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hongjun Wang,  Guangrun Wang,  Ya Li,  Dongyu Zhang,  Liang Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0803</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0119</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Video Modeling With Correlation Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Video_Modeling_With_Correlation_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Heng Wang,  Du Tran,  Lorenzo Torresani,  Matt Feiszli</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0506</td> <td>0.0000</td> <td>0.1485</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Projection & Probability-Driven Black-Box Attack <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Projection__Probability-Driven_Black-Box_Attack_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Li,  Rongrong Ji,  Hong Liu,  Jianzhuang Liu,  Bineng Zhong,  Cheng Deng,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1381</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0128</td></tr>
<tr><td>Auxiliary Training: Towards Accurate and Robust Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Auxiliary_Training_Towards_Accurate_and_Robust_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Linfeng Zhang,  Muzhou Yu,  Tong Chen,  Zuoqiang Shi,  Chenglong Bao,  Kaisheng Ma</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0109</td></tr>
<tr><td>PaStaNet: Toward Human Activity Knowledge Engine <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_PaStaNet_Toward_Human_Activity_Knowledge_Engine_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yong-Lu Li,  Liang Xu,  Xinpeng Liu,  Xijie Huang,  Yue Xu,  Shiyi Wang,  Hao-Shu Fang,  Ze Ma,  Mingyang Chen,  Cewu Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Hierarchical Graph Network for 3D Object Detection on Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_A_Hierarchical_Graph_Network_for_3D_Object_Detection_on_Point_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jintai Chen,  Biwen Lei,  Qingyu Song,  Haochao Ying,  Danny Z. Chen,  Jian Wu</td> <td>0.2002</td> <td>0.1462</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0238</td></tr>
<tr><td>Learning Generative Models of Shape Handles <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gadelha_Learning_Generative_Models_of_Shape_Handles_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matheus Gadelha,  Giorgio Gori,  Duygu Ceylan,  Radomir Mech,  Nathan Carr,  Tamy Boubekeur,  Rui Wang,  Subhransu Maji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>One Man's Trash Is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiao_One_Mans_Trash_Is_Another_Mans_Treasure_Resisting_Adversarial_Examples_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chang Xiao,  Changxi Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0314</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.4451</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0031</td></tr>
<tr><td>Toward a Universal Model for Shape From Texture <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Verbin_Toward_a_Universal_Model_for_Shape_From_Texture_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dor Verbin,  Todd Zickler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HybridPose: 6D Object Pose Estimation Under Hybrid Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Song_HybridPose_6D_Object_Pose_Estimation_Under_Hybrid_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chen Song,  Jiaru Song,  Qixing Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0115</td> <td>0.0000</td> <td>0.0000</td> <td>0.0830</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Boundary-Aware 3D Building Reconstruction From a Single Overhead Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mahmud_Boundary-Aware_3D_Building_Reconstruction_From_a_Single_Overhead_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jisan Mahmud,  True Price,  Akash Bapat,  Jan-Michael Frahm</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0468</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Articulation-Aware Canonical Surface Mapping <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kulkarni_Articulation-Aware_Canonical_Surface_Mapping_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nilesh Kulkarni,  Abhinav Gupta,  David F. Fouhey,  Shubham Tulsiani</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_BiFuse_Monocular_360_Depth_Estimation_via_Bi-Projection_Fusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fu-En Wang,  Yu-Hsuan Yeh,  Min Sun,  Wei-Chen Chiu,  Yi-Hsuan Tsai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0184</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1622</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0028</td></tr>
<tr><td>Transformation GAN for Unsupervised Image Synthesis and Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Transformation_GAN_for_Unsupervised_Image_Synthesis_and_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiayu Wang,  Wengang Zhou,  Guo-Jun Qi,  Zhongqian Fu,  Qi Tian,  Houqiang Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2137</td> <td>0.0000</td> <td>0.0000</td> <td>0.0424</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0106</td></tr>
<tr><td>PPDM: Parallel Point Detection and Matching for Real-Time Human-Object Interaction Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_PPDM_Parallel_Point_Detection_and_Matching_for_Real-Time_Human-Object_Interaction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Liao,  Si Liu,  Fei Wang,  Yanjie Chen,  Chen Qian,  Jiashi Feng</td> <td>0.0226</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Height and Uprightness Invariance for 3D Prediction From a Single View <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Baradad_Height_and_Uprightness_Invariance_for_3D_Prediction_From_a_Single_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Manel Baradad,  Antonio Torralba</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0305</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0167</td></tr>
<tr><td>SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fayyaz_SCT_Set_Constrained_Temporal_Transformer_for_Set_Supervised_Action_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohsen Fayyaz,  Jurgen Gall</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0102</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>3DV: 3D Dynamic Voxel for Action Recognition in Depth Video <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_3DV_3D_Dynamic_Voxel_for_Action_Recognition_in_Depth_Video_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yancheng Wang,  Yang Xiao,  Fu Xiong,  Wenxiang Jiang,  Zhiguo Cao,  Joey Tianyi Zhou,  Junsong Yuan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0319</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1618</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adaptive Interaction Modeling via Graph Operations Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adaptive_Interaction_Modeling_via_Graph_Operations_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haoxin Li,  Wei-Shi Zheng,  Yu Tao,  Haifeng Hu,  Jian-Huang Lai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1089</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yao_Front2Back_Single_View_3D_Shape_Reconstruction_via_Front_to_Back_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuan Yao,  Nico Schertler,  Enrique Rosales,  Helge Rhodin,  Leonid Sigal,  Alla Sheffer</td> <td>0.0000</td> <td>0.0006</td> <td>0.0156</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0289</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_SDC-Depth_Semantic_Divide-and-Conquer_Network_for_Monocular_Depth_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lijun Wang,  Jianming Zhang,  Oliver Wang,  Zhe Lin,  Huchuan Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0411</td> <td>0.0000</td> <td>0.0271</td> <td>0.0000</td> <td>0.0275</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1274</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Single-View View Synthesis With Multiplane Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tucker_Single-View_View_Synthesis_With_Multiplane_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Richard Tucker,  Noah Snavely</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0942</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Parametric Shape Predictions Using Distance Fields <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Smirnov_Deep_Parametric_Shape_Predictions_Using_Distance_Fields_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dmitriy Smirnov,  Matthew Fisher,  Vladimir G. Kim,  Richard Zhang,  Justin Solomon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Leveraging Photometric Consistency Over Time for Sparsely Supervised Hand-Object Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hasson_Leveraging_Photometric_Consistency_Over_Time_for_Sparsely_Supervised_Hand-Object_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yana Hasson,  Bugra Tekin,  Federica Bogo,  Ivan Laptev,  Marc Pollefeys,  Cordelia Schmid</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0275</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0567</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Ensemble Generative Cleaning With Feedback Loops for Defending Adversarial Attacks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Ensemble_Generative_Cleaning_With_Feedback_Loops_for_Defending_Adversarial_Attacks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianhe Yuan,  Zhihai He</td> <td>0.0000</td> <td>0.0000</td> <td>0.0730</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0345</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0194</td></tr>
<tr><td>Temporal Pyramid Network for Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Temporal_Pyramid_Network_for_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ceyuan Yang,  Yinghao Xu,  Jianping Shi,  Bo Dai,  Bolei Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1055</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FaceScape: A Large-Scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FaceScape_A_Large-Scale_High_Quality_3D_Face_Dataset_and_Detailed_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haotian Yang,  Hao Zhu,  Yanru Wang,  Mingkai Huang,  Qiu Shen,  Ruigang Yang,  Xun Cao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0307</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.5159</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Structure-Guided Ranking Loss for Single Image Depth Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xian_Structure-Guided_Ranking_Loss_for_Single_Image_Depth_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ke Xian,  Jianming Zhang,  Oliver Wang,  Long Mai,  Zhe Lin,  Zhiguo Cao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0623</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction From 2D Landmarks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_In_Perfect_Shape_Certifiably_Optimal_3D_Shape_Reconstruction_From_2D_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Heng Yang,  Luca Carlone</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>When NAS Meets Robustness: In Search of Robust Architectures Against Adversarial Attacks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_When_NAS_Meets_Robustness_In_Search_of_Robust_Architectures_Against_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Minghao Guo,  Yuzhe Yang,  Rui Xu,  Ziwei Liu,  Dahua Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0471</td> <td>0.1138</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0633</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0315</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Transferable Targeted Attack <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Towards_Transferable_Targeted_Attack_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maosen Li,  Cheng Deng,  Tengjiao Li,  Junchi Yan,  Xinbo Gao,  Heng Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.4503</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Human Depth Estimation From Monocular Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_Self-Supervised_Human_Depth_Estimation_From_Monocular_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Feitong Tan,  Hao Zhu,  Zhaopeng Cui,  Siyu Zhu,  Marc Pollefeys,  Ping Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0325</td> <td>0.0010</td></tr>
<tr><td>Recursive Social Behavior Graph for Trajectory Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Recursive_Social_Behavior_Graph_for_Trajectory_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianhua Sun,  Qinhong Jiang,  Cewu Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0266</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td></tr>
<tr><td>Context-Aware and Scale-Insensitive Temporal Repetition Counting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Context-Aware_and_Scale-Insensitive_Temporal_Repetition_Counting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huaidong Zhang,  Xuemiao Xu,  Guoqiang Han,  Shengfeng He</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0057</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0149</td></tr>
<tr><td>OASIS: A Large-Scale Dataset for Single Image 3D in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_OASIS_A_Large-Scale_Dataset_for_Single_Image_3D_in_the_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weifeng Chen,  Shengyi Qian,  David Fan,  Noriyuki Kojima,  Max Hamilton,  Jia Deng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>VPLNet: Deep Single View Normal Estimation With Vanishing Points and Lines <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_VPLNet_Deep_Single_View_Normal_Estimation_With_Vanishing_Points_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Wang,  David Geraghty,  Kevin Matzen,  Richard Szeliski,  Jan-Michael Frahm</td> <td>0.0000</td> <td>0.0000</td> <td>0.0272</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0032</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0041</td></tr>
<tr><td>Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianlong Chen,  Sijia Liu,  Shiyu Chang,  Yu Cheng,  Lisa Amini,  Zhangyang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0514</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0176</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0052</td></tr>
<tr><td>Defending Against Universal Attacks Through Selective Feature Regeneration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Borkar_Defending_Against_Universal_Attacks_Through_Selective_Feature_Regeneration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tejas Borkar,  Felix Heide,  Lina Karam</td> <td>0.0000</td> <td>0.0000</td> <td>0.0466</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0260</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Universal Physical Camouflage Attacks on Object Detectors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Universal_Physical_Camouflage_Attacks_on_Object_Detectors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lifeng Huang,  Chengying Gao,  Yuyin Zhou,  Cihang Xie,  Alan L. Yuille,  Changqing Zou,  Ning Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0141</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Intra- and Inter-Action Understanding via Temporal Action Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_Intra-_and_Inter-Action_Understanding_via_Temporal_Action_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dian Shao,  Yue Zhao,  Bo Dai,  Dahua Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2474</td> <td>0.0409</td> <td>0.0206</td></tr>
<tr><td>Lightweight Photometric Stereo for Facial Details Recovery <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Lightweight_Photometric_Stereo_for_Facial_Details_Recovery_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xueying Wang,  Yudong Guo,  Bailin Deng,  Juyong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0109</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0099</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.5491</td> <td>0.0000</td> <td>0.0000</td> <td>0.0056</td></tr>
<tr><td>Bundle Pooling for Polygonal Architecture Segmentation Problem <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Bundle_Pooling_for_Polygonal_Architecture_Segmentation_Problem_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huayi Zeng,  Kevin Joseph,  Adam Vest,  Yasutaka Furukawa</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AvatarMe: Realistically Renderable 3D Facial Reconstruction "In-the-Wild" <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lattas_AvatarMe_Realistically_Renderable_3D_Facial_Reconstruction_In-the-Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alexandros Lattas,  Stylianos Moschoglou,  Baris Gecer,  Stylianos Ploumpis,  Vasileios Triantafyllou,  Abhijeet Ghosh,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1165</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.5738</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Defending Against Model Stealing Attacks With Adaptive Misinformation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kariyappa_Defending_Against_Model_Stealing_Attacks_With_Adaptive_Misinformation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sanjay Kariyappa,  Moinuddin K. Qureshi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0356</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Generate 3D Training Data Through Hybrid Gradient <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_to_Generate_3D_Training_Data_Through_Hybrid_Gradient_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dawei Yang,  Jia Deng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0694</td> <td>0.0000</td></tr>
<tr><td>Cascaded Refinement Network for Point Cloud Completion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Cascaded_Refinement_Network_for_Point_Cloud_Completion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaogang Wang,  Marcelo H. Ang Jr.,  Gim Hee Lee</td> <td>0.0000</td> <td>0.0967</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0618</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Enhancing_Intrinsic_Adversarial_Robustness_via_Feature_Pyramid_Decoder_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guanlin Li,  Shuya Ding,  Jun Luo,  Chang Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0700</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0484</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Discriminate Information for Online Action Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Eun_Learning_to_Discriminate_Information_for_Online_Action_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hyunjun Eun,  Jinyoung Moon,  Jongyoul Park,  Chanho Jung,  Changick Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1086</td></tr>
<tr><td>Adversarial Examples Improve Image Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Adversarial_Examples_Improve_Image_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cihang Xie,  Mingxing Tan,  Boqing Gong,  Jiang Wang,  Alan L. Yuille,  Quoc V. Le</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3782</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_PQ-NET_A_Generative_Part_Seq2Seq_Network_for_3D_Shapes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rundi Wu,  Yixin Zhuang,  Kai Xu,  Hao Zhang,  Baoquan Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0571</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Actor-Transformers for Group Activity Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gavrilyuk_Actor-Transformers_for_Group_Activity_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kirill Gavrilyuk,  Ryan Sanford,  Mehrsan Javan,  Cees G. M. Snoek</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dai_SG-NN_Sparse_Generative_Neural_Networks_for_Self-Supervised_Scene_Completion_of_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Angela Dai,  Christian Diller,  Matthias Niessner</td> <td>0.0000</td> <td>0.0000</td> <td>0.0381</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td></tr>
<tr><td>Geometry-Aware Satellite-to-Ground Image Synthesis for Urban Areas <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Geometry-Aware_Satellite-to-Ground_Image_Synthesis_for_Urban_Areas_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaohu Lu,  Zuoyue Li,  Zhaopeng Cui,  Martin R. Oswald,  Marc Pollefeys,  Rongjun Qin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0149</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0072</td></tr>
<tr><td>Action Modifiers: Learning From Adverbs in Instructional Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Doughty_Action_Modifiers_Learning_From_Adverbs_in_Instructional_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hazel Doughty,  Ivan Laptev,  Walterio Mayol-Cuevas,  Dima Damen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0046</td></tr>
<tr><td>ZSTAD: Zero-Shot Temporal Activity Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_ZSTAD_Zero-Shot_Temporal_Activity_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lingling Zhang,  Xiaojun Chang,  Jun Liu,  Minnan Luo,  Sen Wang,  Zongyuan Ge,  Alexander Hauptmann</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td></tr>
<tr><td>Geometric Structure Based and Regularized Depth Estimation From 360 Indoor Imagery <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_Geometric_Structure_Based_and_Regularized_Depth_Estimation_From_360_Indoor_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Jin,  Yanyu Xu,  Jia Zheng,  Junfei Zhang,  Rui Tang,  Shugong Xu,  Jingyi Yu,  Shenghua Gao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2234</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Kinematics Analysis for Monocular 3D Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Deep_Kinematics_Analysis_for_Monocular_3D_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingwei Xu,  Zhenbo Yu,  Bingbing Ni,  Jiancheng Yang,  Xiaokang Yang,  Wenjun Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td> <td>0.0000</td> <td>0.0000</td> <td>0.0762</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0273</td></tr>
<tr><td>TEA: Temporal Excitation and Aggregation for Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_TEA_Temporal_Excitation_and_Aggregation_for_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yan Li,  Bin Ji,  Xintian Shi,  Jianguo Zhang,  Bin Kang,  Limin Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1378</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Oops! Predicting Unintentional Action in Video <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Epstein_Oops_Predicting_Unintentional_Action_in_Video_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dave Epstein,  Boyuan Chen,  Carl Vondrick</td> <td>0.0000</td> <td>0.0000</td> <td>0.0313</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Scene Recomposition by Learning-Based ICP <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Izadinia_Scene_Recomposition_by_Learning-Based_ICP_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hamid Izadinia,  Steven M. Seitz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0064</td></tr>
<tr><td>Enhancing Cross-Task Black-Box Transferability of Adversarial Examples With Dispersion Reduction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Enhancing_Cross-Task_Black-Box_Transferability_of_Adversarial_Examples_With_Dispersion_Reduction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yantao Lu,  Yunhan Jia,  Jianyu Wang,  Bai Li,  Weiheng Chai,  Lawrence Carin,  Senem Velipasalar</td> <td>0.0169</td> <td>0.0000</td> <td>0.0098</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0430</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1469</td> <td>0.1263</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Non-Line-of-Sight Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chopite_Deep_Non-Line-of-Sight_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Javier Grau Chopite,  Matthias B. Hullin,  Michael Wand,  Julian Iseringhausen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0789</td> <td>0.0000</td></tr>
<tr><td>SSRNet: Scalable 3D Surface Reconstruction Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mi_SSRNet_Scalable_3D_Surface_Reconstruction_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenxing Mi,  Yiming Luo,  Wenbing Tao</td> <td>0.0000</td> <td>0.1195</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Progressive Relation Learning for Group Activity Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Progressive_Relation_Learning_for_Group_Activity_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guyue Hu,  Bo Cui,  Yuan He,  Shan Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0173</td></tr>
<tr><td>Cooling-Shrinking Attack: Blinding the Tracker With Imperceptible Noises <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Cooling-Shrinking_Attack_Blinding_the_Tracker_With_Imperceptible_Noises_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bin Yan,  Dong Wang,  Huchuan Lu,  Xiaoyun Yang</td> <td>0.0014</td> <td>0.0000</td> <td>0.0159</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0644</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0116</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adversarial Camouflage: Hiding Physical-World Attacks With Natural Styles <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Duan_Adversarial_Camouflage_Hiding_Physical-World_Attacks_With_Natural_Styles_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ranjie Duan,  Xingjun Ma,  Yisen Wang,  James Bailey,  A. K. Qin,  Yun Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0280</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.4924</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td></tr>
<tr><td>Weakly-Supervised Action Localization by Generative Attention Modeling <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Weakly-Supervised_Action_Localization_by_Generative_Attention_Modeling_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Baifeng Shi,  Qi Dai,  Yadong Mu,  Jingdong Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0166</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Addepalli_Towards_Achieving_Adversarial_Robustness_by_Enforcing_Feature_Consistency_Across_Bit_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sravanti Addepalli,  Vivek B.S.,  Arya Baburaj,  Gaurang Sriramanan,  R. Venkatesh Babu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0316</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0272</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Polishing Decision-Based Adversarial Noise With a Customized Sampling <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Polishing_Decision-Based_Adversarial_Noise_With_a_Customized_Sampling_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yucheng Shi,  Yahong Han,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Large Yet Imperceptible Adversarial Image Perturbations With Perceptual Color Distance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Towards_Large_Yet_Imperceptible_Adversarial_Image_Perturbations_With_Perceptual_Color_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengyu Zhao,  Zhuoran Liu,  Martha Larson</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Something-Else: Compositional Action Recognition With Spatial-Temporal Interaction Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Materzynska_Something-Else_Compositional_Action_Recognition_With_Spatial-Temporal_Interaction_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Joanna Materzynska,  Tete Xiao,  Roei Herzig,  Huijuan Xu,  Xiaolong Wang,  Trevor Darrell</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1962</td> <td>0.0000</td> <td>0.0030</td></tr>
<tr><td>Learning Unsupervised Hierarchical Part Decomposition of 3D Objects From a Single RGB Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Paschalidou_Learning_Unsupervised_Hierarchical_Part_Decomposition_of_3D_Objects_From_a_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Despoina Paschalidou,  Luc Van Gool,  Andreas Geiger</td> <td>0.0308</td> <td>0.0000</td> <td>0.0492</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Focus on Defocus: Bridging the Synthetic to Real Domain Gap for Depth Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Maximov_Focus_on_Defocus_Bridging_the_Synthetic_to_Real_Domain_Gap_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maxim Maximov,  Kevin Galim,  Laura Leal-Taixe</td> <td>0.0000</td> <td>0.0000</td> <td>0.0307</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0557</td> <td>0.0000</td> <td>0.0000</td> <td>0.0209</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Active Vision for Early Recognition of Human Actions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Active_Vision_for_Early_Recognition_of_Human_Actions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Boyu Wang,  Lihan Huang,  Minh Hoai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0164</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0032</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SmallBigNet: Integrating Core and Contextual Views for Video Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_SmallBigNet_Integrating_Core_and_Contextual_Views_for_Video_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xianhang Li,  Yali Wang,  Zhipeng Zhou,  Yu Qiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0034</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Gate-Shift Networks for Video Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Swathikiran Sudhakaran,  Sergio Escalera,  Oswald Lanz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1896</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Semantics-Guided_Neural_Networks_for_Efficient_Skeleton-Based_Human_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pengfei Zhang,  Cuiling Lan,  Wenjun Zeng,  Junliang Xing,  Jianru Xue,  Nanning Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0358</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2396</td> <td>0.0000</td> <td>0.0067</td></tr>
<tr><td>Exploiting Joint Robustness to Adversarial Perturbations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ali Dabouei,  Sobhan Soleymani,  Fariborz Taherkhani,  Jeremy Dawson,  Nasser M. Nasrabadi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0134</td></tr>
<tr><td>From Image Collections to Point Clouds With Self-Supervised Shape and Pose Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Navaneet_From_Image_Collections_to_Point_Clouds_With_Self-Supervised_Shape_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>K L Navaneet,  Ansu Mathew,  Shashank Kashyap,  Wei-Chih Hung,  Varun Jampani,  R. Venkatesh Babu</td> <td>0.0172</td> <td>0.1268</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0177</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0296</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Searching for Actions on the Hyperbole <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Long_Searching_for_Actions_on_the_Hyperbole_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Teng Long,  Pascal Mettes,  Heng Tao Shen,  Cees G. M. Snoek</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0456</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0221</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ColorFool: Semantic Adversarial Colorization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shamsabadi_ColorFool_Semantic_Adversarial_Colorization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ali Shahin Shamsabadi,  Ricardo Sanchez-Matilla,  Andrea Cavallaro</td> <td>0.0000</td> <td>0.0000</td> <td>0.0454</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0746</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1033</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Boosting the Transferability of Adversarial Samples via Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Boosting_the_Transferability_of_Adversarial_Samples_via_Attention_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weibin Wu,  Yuxin Su,  Xixian Chen,  Shenglin Zhao,  Irwin King,  Michael R. Lyu,  Yu-Wing Tai</td> <td>0.0018</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1595</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0074</td></tr>
<tr><td>ActionBytes: Learning From Trimmed Videos to Localize Actions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jain_ActionBytes_Learning_From_Trimmed_Videos_to_Localize_Actions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mihir Jain,  Amir Ghodrati,  Cees G. M. Snoek</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Efficient Adversarial Training With Transferable Adversarial Examples <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Efficient_Adversarial_Training_With_Transferable_Adversarial_Examples_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haizhong Zheng,  Ziqi Zhang,  Juncheng Gu,  Honglak Lee,  Atul Prakash</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3765</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Alleviation of Gradient Exploding in GANs: Fake Can Be Real <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Alleviation_of_Gradient_Exploding_in_GANs_Fake_Can_Be_Real_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Song Tao,  Jia Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2986</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On Isometry Robustness of Deep 3D Point Cloud Models Under Adversarial Attacks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_On_Isometry_Robustness_of_Deep_3D_Point_Cloud_Models_Under_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Zhao,  Yuwei Wu,  Caihua Chen,  Andrew Lim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0237</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Achieving Robustness in the Wild via Adversarial Mixing With Disentangled Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gowal_Achieving_Robustness_in_the_Wild_via_Adversarial_Mixing_With_Disentangled_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sven Gowal,  Chongli Qin,  Po-Sen Huang,  Taylan Cemgil,  Krishnamurthy Dvijotham,  Timothy Mann,  Pushmeet Kohli</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0070</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0188</td></tr>
<tr><td>QEBA: Query-Efficient Boundary-Based Blackbox Attack <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_QEBA_Query-Efficient_Boundary-Based_Blackbox_Attack_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huichen Li,  Xiaojun Xu,  Xiaolu Zhang,  Shuang Yang,  Bo Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0320</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1189</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Simulate Dynamic Environments With GameGAN <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Learning_to_Simulate_Dynamic_Environments_With_GameGAN_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Seung Wook Kim,  Yuhao Zhou,  Jonah Philion,  Antonio Torralba,  Sanja Fidler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1651</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learn2Perturb: An End-to-End Feature Perturbation Learning to Improve Adversarial Robustness <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jeddi_Learn2Perturb_An_End-to-End_Feature_Perturbation_Learning_to_Improve_Adversarial_Robustness_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ahmadreza Jeddi,  Mohammad Javad Shafiee,  Michelle Karg,  Christian Scharfenberger,  Alexander Wong</td> <td>0.0000</td> <td>0.0000</td> <td>0.1249</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0335</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td></tr>
<tr><td>SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_SDFDiff_Differentiable_Rendering_of_Signed_Distance_Fields_for_3D_Shape_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Jiang,  Dantong Ji,  Zhizhong Han,  Matthias Zwicker</td> <td>0.0581</td> <td>0.0000</td> <td>0.0004</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Through_the_Looking_Glass_Neural_3D_Reconstruction_of_Transparent_Shapes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengqin Li,  Yu-Ying Yeh,  Manmohan Chandraker</td> <td>0.0000</td> <td>0.0290</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>TextureFusion: High-Quality Texture Acquisition for Real-Time RGB-D Scanning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_TextureFusion_High-Quality_Texture_Acquisition_for_Real-Time_RGB-D_Scanning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Joo Ho Lee,  Hyunho Ha,  Yue Dong,  Xin Tong,  Min H. Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td></tr>
<tr><td>D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_D3VO_Deep_Depth_Deep_Pose_and_Deep_Uncertainty_for_Monocular_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nan Yang,  Lukas von Stumberg,  Rui Wang,  Daniel Cremers</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0636</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0211</td></tr>
<tr><td>Deep Implicit Volume Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Deep_Implicit_Volume_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Danhang Tang,  Saurabh Singh,  Philip A. Chou,  Christian Hane,  Mingsong Dou,  Sean Fanello,  Jonathan Taylor,  Philip Davidson,  Onur G. Guleryuz,  Yinda Zhang,  Shahram Izadi,  Andrea Tagliasacchi,  Sofien Bouaziz,  Cem Keskin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0206</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MAGSAC++, a Fast, Reliable and Accurate Robust Estimator <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Barath_MAGSAC_a_Fast_Reliable_and_Accurate_Robust_Estimator_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daniel Barath,  Jana Noskova,  Maksym Ivashechkin,  Jiri Matas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_OctSqueeze_Octree-Structured_Entropy_Model_for_LiDAR_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lila Huang,  Shenlong Wang,  Kelvin Wong,  Jerry Liu,  Raquel Urtasun</td> <td>0.0000</td> <td>0.1479</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>4D Association Graph for Realtime Multi-Person Motion Capture Using Multiple Video Cameras <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_4D_Association_Graph_for_Realtime_Multi-Person_Motion_Capture_Using_Multiple_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuxiang Zhang,  Liang An,  Tao Yu,  Xiu Li,  Kun Li,  Yebin Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0940</td></tr>
<tr><td>Upgrading Optical Flow to 3D Scene Flow Through Optical Expansion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Upgrading_Optical_Flow_to_3D_Scene_Flow_Through_Optical_Expansion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gengshan Yang,  Deva Ramanan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1277</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Robust 3D Self-Portraits in Seconds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Robust_3D_Self-Portraits_in_Seconds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhe Li,  Tao Yu,  Chuanyu Pan,  Zerong Zheng,  Yebin Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0633</td></tr>
<tr><td>FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tassano_FastDVDnet_Towards_Real-Time_Deep_Video_Denoising_Without_Flow_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matias Tassano,  Julie Delon,  Thomas Veit</td> <td>0.0000</td> <td>0.0000</td> <td>0.0741</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0031</td></tr>
<tr><td>Learning to Have an Ear for Face Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Givi Meishvili,  Simon Jenni,  Paolo Favaro</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Optics for Single-Shot High-Dynamic-Range Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christopher A. Metzler,  Hayato Ikoma,  Yifan Peng,  Gordon Wetzstein</td> <td>0.0000</td> <td>0.0000</td> <td>0.0435</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Rank-1 Diffractive Optics for Single-Shot High Dynamic Range Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Learning_Rank-1_Diffractive_Optics_for_Single-Shot_High_Dynamic_Range_Imaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qilin Sun,  Ethan Tseng,  Qiang Fu,  Wolfgang Heidrich,  Felix Heide</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep White-Balance Editing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Afifi_Deep_White-Balance_Editing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mahmoud Afifi,  Michael S. Brown</td> <td>0.0000</td> <td>0.0000</td> <td>0.0226</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Non-Line-of-Sight Surface Reconstruction Using the Directional Light-Cone Transform <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Young_Non-Line-of-Sight_Surface_Reconstruction_Using_the_Directional_Light-Cone_Transform_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sean I. Young,  David B. Lindell,  Bernd Girod,  David Taubman,  Gordon Wetzstein</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Seeing the World in a Bag of Chips <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Park_Seeing_the_World_in_a_Bag_of_Chips_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jeong Joon Park,  Aleksander Holynski,  Steven M. Seitz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abu_Hussein_Correction_Filter_for_Single_Image_Super-Resolution_Robustifying_Off-the-Shelf_Deep_Super-Resolvers_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shady Abu Hussein,  Tom Tirer,  Raja Giryes</td> <td>0.0000</td> <td>0.0000</td> <td>0.0455</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0390</td> <td>0.0000</td></tr>
<tr><td>Retina-Like Visual Image Reconstruction via Spiking Neural Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Retina-Like_Visual_Image_Reconstruction_via_Spiking_Neural_Model_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lin Zhu,  Siwei Dong,  Jianing Li,  Tiejun Huang,  Yonghong Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0092</td></tr>
<tr><td>Plug-and-Play Algorithms for Large-Scale Snapshot Compressive Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Plug-and-Play_Algorithms_for_Large-Scale_Snapshot_Compressive_Imaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Yuan,  Yang Liu,  Jinli Suo,  Qionghai Dai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Neural Network Pruning With Residual-Connections and Limited-Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Neural_Network_Pruning_With_Residual-Connections_and_Limited-Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jian-Hao Luo,  Jianxin Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0031</td></tr>
<tr><td>AdderNet: Do We Really Need Multiplications in Deep Learning? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_AdderNet_Do_We_Really_Need_Multiplications_in_Deep_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hanting Chen,  Yunhe Wang,  Chunjing Xu,  Boxin Shi,  Chao Xu,  Qi Tian,  Chang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.1435</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0112</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_NeuralScale_Efficient_Scaling_of_Neurons_for_Resource-Constrained_Deep_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eugene Lee,  Chen-Yi Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0542</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0294</td></tr>
<tr><td>Training Quantized Neural Networks With a Full-Precision Auxiliary Module <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhuang_Training_Quantized_Neural_Networks_With_a_Full-Precision_Auxiliary_Module_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bohan Zhuang,  Lingqiao Liu,  Mingkui Tan,  Chunhua Shen,  Ian Reid</td> <td>0.0091</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Networks_Are_More_Productive_Teachers_Than_Human_Raters_Active_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongdong Wang,  Yandong Li,  Liqiang Wang,  Boqing Gong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0505</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0030</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-Dimensional Pruning: A Unified Framework for Model Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Multi-Dimensional_Pruning_A_Unified_Framework_for_Model_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinyang Guo,  Wanli Ouyang,  Dong Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0339</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td></tr>
<tr><td>Towards Efficient Model Compression via Learned Global Ranking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chin_Towards_Efficient_Model_Compression_via_Learned_Global_Ranking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ting-Wu Chin,  Ruizhou Ding,  Cha Zhang,  Diana Marculescu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0481</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HRank: Filter Pruning Using High-Rank Feature Map <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_HRank_Filter_Pruning_Using_High-Rank_Feature_Map_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mingbao Lin,  Rongrong Ji,  Yan Wang,  Yichen Zhang,  Baochang Zhang,  Yonghong Tian,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0618</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0688</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0136</td></tr>
<tr><td>DMCP: Differentiable Markov Channel Pruning for Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_DMCP_Differentiable_Markov_Channel_Pruning_for_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaopeng Guo,  Yujie Wang,  Quanquan Li,  Junjie Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td></tr>
<tr><td>ReSprop: Reuse Sparsified Backpropagation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Goli_ReSprop_Reuse_Sparsified_Backpropagation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Negar Goli,  Tor M. Aamodt</td> <td>0.0000</td> <td>0.0000</td> <td>0.0707</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adversarial Texture Optimization From RGB-D Scans <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Adversarial_Texture_Optimization_From_RGB-D_Scans_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingwei Huang,  Justus Thies,  Angela Dai,  Abhijit Kundu,  Chiyu "Max" Jiang,  Leonidas J. Guibas,  Matthias Niessner,  Thomas Funkhouser</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Synchronizing Probability Measures on Rotations via Optimal Transport <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Birdal_Synchronizing_Probability_Measures_on_Rotations_via_Optimal_Transport_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tolga Birdal,  Michael Arbel,  Umut Simsekli,  Leonidas J. Guibas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0407</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0482</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GhostNet: More Features From Cheap Operations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_GhostNet_More_Features_From_Cheap_Operations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Han,  Yunhe Wang,  Qi Tian,  Jianyuan Guo,  Chunjing Xu,  Chang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0576</td> <td>0.0129</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0448</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Attention-Aware Multi-View Stereo <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Attention-Aware_Multi-View_Stereo_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Keyang Luo,  Tao Guan,  Lili Ju,  Yuesong Wang,  Zhuo Chen,  Yawei Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0252</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0153</td> <td>0.0000</td> <td>0.0292</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td></tr>
<tr><td>Bi3D: Stereo Depth Estimation via Binary Classifications <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Badki_Bi3D_Stereo_Depth_Estimation_via_Binary_Classifications_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abhishek Badki,  Alejandro Troccoli,  Kihwan Kim,  Jan Kautz,  Pradeep Sen,  Orazio Gallo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0140</td> <td>0.0000</td> <td>0.0416</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td></tr>
<tr><td>Joint Filtering of Intensity Images and Neuromorphic Events for High-Resolution Noise-Robust Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Joint_Filtering_of_Intensity_Images_and_Neuromorphic_Events_for_High-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zihao W. Wang,  Peiqi Duan,  Oliver Cossairt,  Aggelos Katsaggelos,  Tiejun Huang,  Boxin Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>SGAS: Sequential Greedy Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_SGAS_Sequential_Greedy_Architecture_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guohao Li,  Guocheng Qian,  Itzel C. Delgadillo,  Matthias Muller,  Ali Thabet,  Bernard Ghanem</td> <td>0.0000</td> <td>0.0339</td> <td>0.0362</td> <td>0.3243</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_HVNet_Hybrid_Voxel_Network_for_LiDAR_Based_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maosheng Ye,  Shuangjie Xu,  Tongyi Cao</td> <td>0.0857</td> <td>0.0309</td> <td>0.0034</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Frequency Domain Compact 3D Convolutional Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Frequency_Domain_Compact_3D_Convolutional_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hanting Chen,  Yunhe Wang,  Han Shu,  Yehui Tang,  Chunjing Xu,  Boxin Shi,  Chao Xu,  Qi Tian,  Chang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0712</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0167</td></tr>
<tr><td>Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Single-Image_HDR_Reconstruction_by_Learning_to_Reverse_the_Camera_Pipeline_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu-Lun Liu,  Wei-Sheng Lai,  Yu-Sheng Chen,  Yi-Lung Kao,  Ming-Hsuan Yang,  Yung-Yu Chuang,  Jia-Bin Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0374</td></tr>
<tr><td>DNU: Deep Non-Local Unrolling for Computational Spectral Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_DNU_Deep_Non-Local_Unrolling_for_Computational_Spectral_Imaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lizhi Wang,  Chen Sun,  Maoqing Zhang,  Ying Fu,  Hua Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0747</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0190</td></tr>
<tr><td>Single Image Optical Flow Estimation With an Event Camera <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Single_Image_Optical_Flow_Estimation_With_an_Event_Camera_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Liyuan Pan,  Miaomiao Liu,  Richard Hartley</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0982</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0076</td></tr>
<tr><td>Multi-View Neural Human Rendering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Multi-View_Neural_Human_Rendering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Minye Wu,  Yuehao Wang,  Qiang Hu,  Jingyi Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0149</td></tr>
<tr><td>Depth Sensing Beyond LiDAR Range <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Depth_Sensing_Beyond_LiDAR_Range_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Zhang,  Jiaxin Xie,  Noah Snavely,  Qifeng Chen</td> <td>0.0049</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1623</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0017</td></tr>
<tr><td>Event Probability Mask (EPM) and Event Denoising Convolutional Neural Network (EDnCNN) for Neuromorphic Cameras <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Baldwin_Event_Probability_Mask_EPM_and_Event_Denoising_Convolutional_Neural_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>R. Wes Baldwin,  Mohammed Almatrafi,  Vijayan Asari,  Keigo Hirakawa</td> <td>0.0000</td> <td>0.0000</td> <td>0.0725</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Point-GNN_Graph_Neural_Network_for_3D_Object_Detection_in_a_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weijing Shi,  Raj Rajkumar</td> <td>0.0719</td> <td>0.0966</td> <td>0.0374</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0456</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Learning Video Rain Streak Removal: When Cyclic Consistency Meets Temporal Correspondence <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Self-Learning_Video_Rain_Streak_Removal_When_Cyclic_Consistency_Meets_Temporal_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenhan Yang,  Robby T. Tan,  Shiqi Wang,  Jiaying Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0065</td></tr>
<tr><td>Neuromorphic Camera Guided High Dynamic Range Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_Neuromorphic_Camera_Guided_High_Dynamic_Range_Imaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jin Han,  Chu Zhou,  Peiqi Duan,  Yehui Tang,  Chang Xu,  Chao Xu,  Tiejun Huang,  Boxin Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td></tr>
<tr><td>Learning in the Frequency Domain <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Learning_in_the_Frequency_Domain_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Xu,  Minghai Qin,  Fei Sun,  Yuhao Wang,  Yen-Kuang Chen,  Fengbo Ren</td> <td>0.0000</td> <td>0.0000</td> <td>0.0582</td> <td>0.0000</td> <td>0.0348</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0396</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0492</td></tr>
<tr><td>Polarized Reflection Removal With Perfect Alignment in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lei_Polarized_Reflection_Removal_With_Perfect_Alignment_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenyang Lei,  Xuhua Huang,  Mengdi Zhang,  Qiong Yan,  Wenxiu Sun,  Qifeng Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0250</td></tr>
<tr><td>Learning Multiview 3D Point Cloud Registration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gojcic_Learning_Multiview_3D_Point_Cloud_Registration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zan Gojcic,  Caifa Zhou,  Jan D. Wegner,  Leonidas J. Guibas,  Tolga Birdal</td> <td>0.0000</td> <td>0.1153</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0210</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0229</td></tr>
<tr><td>A Sparse Resultant Based Method for Efficient Minimal Solvers <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bhayani_A_Sparse_Resultant_Based_Method_for_Efficient_Minimal_Solvers_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Snehal Bhayani,  Zuzana Kukelova,  Janne Heikkila</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0734</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chunle Guo,  Chongyi Li,  Jichang Guo,  Chen Change Loy,  Junhui Hou,  Sam Kwong,  Runmin Cong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0079</td></tr>
<tr><td>BlendedMVS: A Large-Scale Dataset for Generalized Multi-View Stereo Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yao_BlendedMVS_A_Large-Scale_Dataset_for_Generalized_Multi-View_Stereo_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yao Yao,  Zixin Luo,  Shiwei Li,  Jingyang Zhang,  Yufan Ren,  Lei Zhou,  Tian Fang,  Long Quan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0104</td> <td>0.0000</td> <td>0.0000</td> <td>0.0653</td> <td>0.0000</td> <td>0.0493</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0280</td> <td>0.0002</td></tr>
<tr><td>Convolution in the Cloud: Learning Deformable Kernels in 3D Graph Convolution Networks for Point Cloud Analysis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Convolution_in_the_Cloud_Learning_Deformable_Kernels_in_3D_Graph_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhi-Hao Lin,  Sheng-Yu Huang,  Yu-Chiang Frank Wang</td> <td>0.0000</td> <td>0.1538</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td></tr>
<tr><td>A Semi-Supervised Assessor of Neural Architectures <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_A_Semi-Supervised_Assessor_of_Neural_Architectures_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yehui Tang,  Yunhe Wang,  Yixing Xu,  Hanting Chen,  Boxin Shi,  Chao Xu,  Chunjing Xu,  Qi Tian,  Chang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0525</td> <td>0.3084</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning a Reinforced Agent for Flexible Exposure Bracketing Selection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Learning_a_Reinforced_Agent_for_Flexible_Exposure_Bracketing_Selection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhouxia Wang,  Jiawei Zhang,  Mude Lin,  Jiong Wang,  Ping Luo,  Jimmy Ren</td> <td>0.0000</td> <td>0.0000</td> <td>0.0323</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CARS: Continuous Evolution for Efficient Neural Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_CARS_Continuous_Evolution_for_Efficient_Neural_Architecture_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhaohui Yang,  Yunhe Wang,  Xinghao Chen,  Boxin Shi,  Chao Xu,  Chunjing Xu,  Qi Tian,  Chang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0434</td> <td>0.2263</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0025</td></tr>
<tr><td>Joint 3D Instance Segmentation and Object Detection for Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Joint_3D_Instance_Segmentation_and_Object_Detection_for_Autonomous_Driving_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dingfu Zhou,  Jin Fang,  Xibin Song,  Liu Liu,  Junbo Yin,  Yuchao Dai,  Hongdong Li,  Ruigang Yang</td> <td>0.1424</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2047</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0118</td></tr>
<tr><td>View-GCN: View-Based Graph Convolutional Network for 3D Shape Analysis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_View-GCN_View-Based_Graph_Convolutional_Network_for_3D_Shape_Analysis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Wei,  Ruixuan Yu,  Jian Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0533</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0125</td></tr>
<tr><td>Collaborative Distillation for Ultra-Resolution Universal Style Transfer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Collaborative_Distillation_for_Ultra-Resolution_Universal_Style_Transfer_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huan Wang,  Yijun Li,  Yuehai Wang,  Haoji Hu,  Ming-Hsuan Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0280</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0081</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>TomoFluid: Reconstructing Dynamic Fluid From Sparse View Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zang_TomoFluid_Reconstructing_Dynamic_Fluid_From_Sparse_View_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guangming Zang,  Ramzi Idoughi,  Congli Wang,  Anthony Bennett,  Jianguo Du,  Scott Skeen,  William L. Roberts,  Peter Wonka,  Wolfgang Heidrich</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0057</td></tr>
<tr><td>Instance Shadow Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianyu Wang,  Xiaowei Hu,  Qiong Wang,  Pheng-Ann Heng,  Chi-Wing Fu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0181</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Quan_Self2Self_With_Dropout_Learning_Self-Supervised_Denoising_From_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuhui Quan,  Mingqin Chen,  Tongyao Pang,  Hui Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0430</td></tr>
<tr><td>Discrete Model Compression With Resource Constraint for Deep Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Discrete_Model_Compression_With_Resource_Constraint_for_Deep_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shangqian Gao,  Feihu Huang,  Jian Pei,  Heng Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0486</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0337</td></tr>
<tr><td>Structured Compression by Weight Encryption for Unstructured Pruning and Quantization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kwon_Structured_Compression_by_Weight_Encryption_for_Unstructured_Pruning_and_Quantization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Se Jung Kwon,  Dongsoo Lee,  Byeongwook Kim,  Parichay Kapoor,  Baeseong Park,  Gu-Yeon Wei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0205</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>End-to-End Learning Local Multi-View Descriptors for 3D Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_End-to-End_Learning_Local_Multi-View_Descriptors_for_3D_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Li,  Siyu Zhu,  Hongbo Fu,  Ping Tan,  Chiew-Lan Tai</td> <td>0.0000</td> <td>0.0850</td> <td>0.0191</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0104</td></tr>
<tr><td>Minimal Solutions for Relative Pose With a Single Affine Correspondence <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guan_Minimal_Solutions_for_Relative_Pose_With_a_Single_Affine_Correspondence_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Banglei Guan,  Ji Zhao,  Zhang Li,  Fang Sun,  Friedrich Fraundorfer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0316</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0426</td></tr>
<tr><td>Point Cloud Completion by Skip-Attention Network With Hierarchical Folding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wen_Point_Cloud_Completion_by_Skip-Attention_Network_With_Hierarchical_Folding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Wen,  Tianyang Li,  Zhizhong Han,  Yu-Shen Liu</td> <td>0.0057</td> <td>0.1888</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td></tr>
<tr><td>Fast-MVSNet: Sparse-to-Dense Multi-View Stereo With Learned Propagation and Gauss-Newton Refinement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Fast-MVSNet_Sparse-to-Dense_Multi-View_Stereo_With_Learned_Propagation_and_Gauss-Newton_Refinement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zehao Yu,  Shenghua Gao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0242</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0413</td> <td>0.0000</td> <td>0.0000</td> <td>0.3420</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AANet: Adaptive Aggregation Network for Efficient Stereo Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_AANet_Adaptive_Aggregation_Network_for_Efficient_Stereo_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haofei Xu,  Juyong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0108</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0108</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0138</td></tr>
<tr><td>Towards Unified INT8 Training for Convolutional Neural Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Towards_Unified_INT8_Training_for_Convolutional_Neural_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Feng Zhu,  Ruihao Gong,  Fengwei Yu,  Xianglong Liu,  Yanfei Wang,  Zhelong Li,  Xiuqi Yang,  Junjie Yan</td> <td>0.0143</td> <td>0.0000</td> <td>0.0504</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Active 3D Motion Visualization Based on Spatiotemporal Light-Ray Integration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sakaue_Active_3D_Motion_Visualization_Based_on_Spatiotemporal_Light-Ray_Integration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fumihiko Sakaue,  Jun Sato</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0096</td></tr>
<tr><td>Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Block-Wisely_Supervised_Neural_Architecture_Search_With_Knowledge_Distillation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Changlin Li,  Jiefeng Peng,  Liuchun Yuan,  Guangrun Wang,  Xiaodan Liang,  Liang Lin,  Xiaojun Chang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2612</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0359</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GreedyNAS: Towards Fast One-Shot NAS With Greedy Supernet <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/You_GreedyNAS_Towards_Fast_One-Shot_NAS_With_Greedy_Supernet_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shan You,  Tao Huang,  Mingmin Yang,  Fei Wang,  Chen Qian,  Changshui Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3391</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0359</td></tr>
<tr><td>Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Learning_Filter_Pruning_Criteria_for_Deep_Convolutional_Neural_Networks_Acceleration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang He,  Yuhang Ding,  Ping Liu,  Linchao Zhu,  Hanwang Zhang,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0094</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0052</td></tr>
<tr><td>DIST: Rendering Deep Implicit Signed Distance Function With Differentiable Sphere Tracing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_DIST_Rendering_Deep_Implicit_Signed_Distance_Function_With_Differentiable_Sphere_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaohui Liu,  Yinda Zhang,  Songyou Peng,  Boxin Shi,  Marc Pollefeys,  Zhaopeng Cui</td> <td>0.0000</td> <td>0.0000</td> <td>0.0192</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Visually Imbalanced Stereo Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Visually_Imbalanced_Stereo_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yicun Liu,  Jimmy Ren,  Jiawei Zhang,  Jianbo Liu,  Mude Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0425</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Mesh-Guided Multi-View Stereo With Pyramid Architecture <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Mesh-Guided_Multi-View_Stereo_With_Pyramid_Architecture_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuesong Wang,  Tao Guan,  Zhuo Chen,  Yawei Luo,  Keyang Luo,  Lili Ju</td> <td>0.0000</td> <td>0.0000</td> <td>0.0293</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2622</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0214</td></tr>
<tr><td>BiDet: An Efficient Binarized Object Detector <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_BiDet_An_Efficient_Binarized_Object_Detector_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziwei Wang,  Ziyi Wu,  Jiwen Lu,  Jie Zhou</td> <td>0.0829</td> <td>0.0000</td> <td>0.0366</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0228</td></tr>
<tr><td>Local Non-Rigid Structure-From-Motion From Diffeomorphic Mappings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Parashar_Local_Non-Rigid_Structure-From-Motion_From_Diffeomorphic_Mappings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaifali Parashar,  Mathieu Salzmann,  Pascal Fua</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1590</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Seeing Around Street Corners: Non-Line-of-Sight Detection and Tracking In-the-Wild Using Doppler Radar <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Scheiner_Seeing_Around_Street_Corners_Non-Line-of-Sight_Detection_and_Tracking_In-the-Wild_Using_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nicolas Scheiner,  Florian Kraus,  Fangyin Wei,  Buu Phan,  Fahim Mannan,  Nils Appenrodt,  Werner Ritter,  Jurgen Dickmann,  Klaus Dietmayer,  Bernhard Sick,  Felix Heide</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>APQ: Joint Search for Network Architecture, Pruning and Quantization Policy <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_APQ_Joint_Search_for_Network_Architecture_Pruning_and_Quantization_Policy_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianzhe Wang,  Kuan Wang,  Han Cai,  Ji Lin,  Zhijian Liu,  Hanrui Wang,  Yujun Lin,  Song Han</td> <td>0.0000</td> <td>0.0000</td> <td>0.0339</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On the Acceleration of Deep Learning Model Parallelism With Staleness <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_On_the_Acceleration_of_Deep_Learning_Model_Parallelism_With_Staleness_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>An Xu,  Zhouyuan Huo,  Heng Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0458</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0455</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0274</td></tr>
<tr><td>RevealNet: Seeing Behind Objects in RGB-D Scans <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_RevealNet_Seeing_Behind_Objects_in_RGB-D_Scans_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ji Hou,  Angela Dai,  Matthias Niessner</td> <td>0.0229</td> <td>0.0000</td> <td>0.0145</td> <td>0.0000</td> <td>0.0801</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MemNAS: Memory-Efficient Neural Architecture Search With Grow-Trim Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_MemNAS_Memory-Efficient_Neural_Architecture_Search_With_Grow-Trim_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peiye Liu,  Bo Wu,  Huadong Ma,  Mingoo Seok</td> <td>0.0000</td> <td>0.0000</td> <td>0.0117</td> <td>0.1980</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>StegaStamp: Invisible Hyperlinks in Physical Photographs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tancik_StegaStamp_Invisible_Hyperlinks_in_Physical_Photographs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matthew Tancik,  Ben Mildenhall,  Ren Ng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0906</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>L2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/You_L2-GCN_Layer-Wise_and_Learned_Efficient_Training_of_Graph_Convolutional_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuning You,  Tianlong Chen,  Zhangyang Wang,  Yang Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0705</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0158</td></tr>
<tr><td>Polarized Non-Line-of-Sight Imaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tanaka_Polarized_Non-Line-of-Sight_Imaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kenichiro Tanaka,  Yasuhiro Mukaigawa,  Achuta Kadambi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AdaBits: Neural Network Quantization With Adaptive Bit-Widths <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_AdaBits_Neural_Network_Quantization_With_Adaptive_Bit-Widths_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qing Jin,  Linjie Yang,  Zhenyu Liao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0938</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-Scale Boosted Dehazing Network With Dense Feature Fusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Multi-Scale_Boosted_Dehazing_Network_With_Dense_Feature_Fusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hang Dong,  Jinshan Pan,  Lei Xiang,  Zhe Hu,  Xinyi Zhang,  Fei Wang,  Ming-Hsuan Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0361</td></tr>
<tr><td>ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_ClusterVO_Clustering_Moving_Instances_and_Estimating_Visual_Odometry_for_Self_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiahui Huang,  Sheng Yang,  Tai-Jiang Mu,  Shi-Min Hu</td> <td>0.0026</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Automatic_Neural_Network_Compression_by_Sparsity-Quantization_Joint_Learning_A_Constrained_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haichuan Yang,  Shupeng Gui,  Yuhao Zhu,  Ji Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0410</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0019</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Normal Assisted Stereo Depth Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kusupati_Normal_Assisted_Stereo_Depth_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Uday Kusupati,  Shuo Cheng,  Rui Chen,  Hao Su</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0571</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0042</td></tr>
<tr><td>Fusing Wearable IMUs With Multi-View Images for Human Pose Estimation: A Geometric Approach <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Fusing_Wearable_IMUs_With_Multi-View_Images_for_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhe Zhang,  Chunyu Wang,  Wenhu Qin,  Wenjun Zeng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1285</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity Priors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fragoso_gDLS_Generalized_Pose-and-Scale_Estimation_Given_Scale_and_Gravity_Priors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Victor Fragoso,  Joseph DeGol,  Gang Hua</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Embodied Language Grounding With 3D Visual Feature Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Prabhudesai_Embodied_Language_Grounding_With_3D_Visual_Feature_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mihir Prabhudesai,  Hsiao-Yu Fish Tung,  Syed Ashar Javed,  Maximilian Sieb,  Adam W. Harley,  Katerina Fragkiadaki</td> <td>0.0000</td> <td>0.0014</td> <td>0.0031</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0046</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Autofocus <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Herrmann_Learning_to_Autofocus_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Charles Herrmann,  Richard Strong Bowen,  Neal Wadhwa,  Rahul Garg,  Qiurui He,  Jonathan T. Barron,  Ramin Zabih</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Joint Demosaicing and Denoising With Self Guidance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Joint_Demosaicing_and_Denoising_With_Self_Guidance_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lin Liu,  Xu Jia,  Jianzhuang Liu,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0162</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>Forward and Backward Information Retention for Accurate Binary Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qin_Forward_and_Backward_Information_Retention_for_Accurate_Binary_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haotong Qin,  Ruihao Gong,  Xianglong Liu,  Mingzhu Shen,  Ziran Wei,  Fengwei Yu,  Jingkuan Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0434</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Light Field Spatial Super-Resolution via Deep Combinatorial Geometry Embedding and Structural Consistency Regularization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_Light_Field_Spatial_Super-Resolution_via_Deep_Combinatorial_Geometry_Embedding_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jing Jin,  Junhui Hou,  Jie Chen,  Sam Kwong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0738</td></tr>
<tr><td>A Multi-Hypothesis Approach to Color Constancy <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hernandez-Juarez_A_Multi-Hypothesis_Approach_to_Color_Constancy_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daniel Hernandez-Juarez,  Sarah Parisot,  Benjamin Busam,  Ales Leonardis,  Gregory Slabaugh,  Steven McDonagh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Restore Low-Light Images via Decomposition-and-Enhancement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Learning_to_Restore_Low-Light_Images_via_Decomposition-and-Enhancement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ke Xu,  Xin Yang,  Baocai Yin,  Rynson W.H. Lau</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0351</td></tr>
<tr><td>Background Matting: The World Is Your Green Screen <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sengupta_Background_Matting_The_World_Is_Your_Green_Screen_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Soumyadip Sengupta,  Vivek Jayaram,  Brian Curless,  Steven M. Seitz,  Ira Kemelmacher-Shlizerman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yue_Supervised_Raw_Video_Denoising_With_a_Benchmark_Dataset_on_Dynamic_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huanjing Yue,  Cong Cao,  Lei Liao,  Ronghe Chu,  Jingyu Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0477</td></tr>
<tr><td>Photometric Stereo via Discrete Hypothesis-and-Test Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Enomoto_Photometric_Stereo_via_Discrete_Hypothesis-and-Test_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kenji Enomoto,  Michael Waechter,  Kiriakos N. Kutulakos,  Yasuyuki Matsushita</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Verelst_Dynamic_Convolutions_Exploiting_Spatial_Sparsity_for_Faster_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thomas Verelst,  Tinne Tuytelaars</td> <td>0.0000</td> <td>0.0000</td> <td>0.0390</td> <td>0.0000</td> <td>0.0000</td> <td>0.1228</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fixed-Point Back-Propagation Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Fixed-Point_Back-Propagation_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xishan Zhang,  Shaoli Liu,  Rui Zhang,  Chang Liu,  Di Huang,  Shiyi Zhou,  Jiaming Guo,  Qi Guo,  Zidong Du,  Tian Zhi,  Yunji Chen</td> <td>0.0132</td> <td>0.0000</td> <td>0.1042</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0137</td></tr>
<tr><td>Heterogeneous Knowledge Distillation Using Information Flow Modeling <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Passalis_Heterogeneous_Knowledge_Distillation_Using_Information_Flow_Modeling_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nikolaos Passalis,  Maria Tzelepi,  Anastasios Tefas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0067</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0482</td></tr>
<tr><td>Rethinking Differentiable Search for Mixed-Precision Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Rethinking_Differentiable_Search_for_Mixed-Precision_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhaowei Cai,  Nuno Vasconcelos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0138</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Residual Feature Aggregation Network for Image Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Residual_Feature_Aggregation_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Liu,  Wenjie Zhang,  Yuting Tang,  Jie Tang,  Gangshan Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0210</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Resolution Adaptive Networks for Efficient Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Resolution_Adaptive_Networks_for_Efficient_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Le Yang,  Yizeng Han,  Xi Chen,  Shiji Song,  Jifeng Dai,  Gao Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0196</td></tr>
<tr><td>Learning to Forget for Meta-Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Baik_Learning_to_Forget_for_Meta-Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sungyong Baik,  Seokil Hong,  Kyoung Mu Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0183</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1426</td></tr>
<tr><td>Deep Learning for Handling Kernel/model Uncertainty in Image Deconvolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nan_Deep_Learning_for_Handling_Kernelmodel_Uncertainty_in_Image_Deconvolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuesong Nan,  Hui Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0164</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0244</td></tr>
<tr><td>Reflection Scene Separation From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_Reflection_Scene_Separation_From_a_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Renjie Wan,  Boxin Shi,  Haoliang Li,  Ling-Yu Duan,  Alex C. Kot</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0826</td></tr>
<tr><td>Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre Bokeh Effect on Smartphones <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Wavelet_Synthesis_Net_for_Disparity_Estimation_to_Synthesize_DSLR_Calibre_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenchi Luo,  Yingmao Li,  Kaimo Lin,  George Chen,  Seok-Jun Lee,  Jihwan Choi,  Youngjun Francis Yoo,  Michael O. Polley</td> <td>0.0000</td> <td>0.0000</td> <td>0.0169</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1892</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0156</td> <td>0.0000</td></tr>
<tr><td>Bundle Adjustment on a Graph Processor <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ortiz_Bundle_Adjustment_on_a_Graph_Processor_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Joseph Ortiz,  Mark Pupilli,  Stefan Leutenegger,  Andrew J. Davison</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0767</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pedersen_3D-ZeF_A_3D_Zebrafish_Tracking_Benchmark_Dataset_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Malte Pedersen,  Joakim Bruslund Haurum,  Stefan Hein Bengtson,  Thomas B. Moeslund</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Menon_PULSE_Self-Supervised_Photo_Upsampling_via_Latent_Space_Exploration_of_Generative_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sachit Menon,  Alexandru Damian,  Shijia Hu,  Nikhil Ravi,  Cynthia Rudin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0160</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0515</td></tr>
<tr><td>Scalability in Perception for Autonomous Driving: Waymo Open Dataset <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pei Sun,  Henrik Kretzschmar,  Xerxes Dotiwalla,  Aurelien Chouard,  Vijaysai Patnaik,  Paul Tsui,  James Guo,  Yin Zhou,  Yuning Chai,  Benjamin Caine,  Vijay Vasudevan,  Wei Han,  Jiquan Ngiam,  Hang Zhao,  Aleksei Timofeev,  Scott Ettinger,  Maxim Krivokon,  Amy Gao,  Aditya Joshi,  Yu Zhang,  Jonathon Shlens,  Zhifeng Chen,  Dragomir Anguelov</td> <td>0.0190</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Extreme Relative Pose Network Under Hybrid Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Extreme_Relative_Pose_Network_Under_Hybrid_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenpei Yang,  Siming Yan,  Qixing Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0655</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>Single-Shot Monocular RGB-D Imaging Using Uneven Double Refraction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Meuleman_Single-Shot_Monocular_RGB-D_Imaging_Using_Uneven_Double_Refraction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Andreas Meuleman,  Seung-Hwan Baek,  Felix Heide,  Min H. Kim</td> <td>0.1205</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0218</td></tr>
<tr><td>Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying Lighting and SVBRDF From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Inverse_Rendering_for_Complex_Indoor_Scenes_Shape_Spatially-Varying_Lighting_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengqin Li,  Mohammad Shafiei,  Ravi Ramamoorthi,  Kalyan Sunkavalli,  Manmohan Chandraker</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>3D Packing for Self-Supervised Monocular Depth Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guizilini_3D_Packing_for_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vitor Guizilini,  Rares Ambrus,  Sudeep Pillai,  Allan Raventos,  Adrien Gaidon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0421</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0059</td></tr>
<tr><td>Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaodong Gu,  Zhiwen Fan,  Siyu Zhu,  Zuozhuo Dai,  Feitong Tan,  Ping Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0373</td> <td>0.0000</td> <td>0.0000</td> <td>0.0489</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0172</td></tr>
<tr><td>From Two Rolling Shutters to One Global Shutter <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Albl_From_Two_Rolling_Shutters_to_One_Global_Shutter_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cenek Albl,  Zuzana Kukelova,  Viktor Larsson,  Michal Polic,  Tomas Pajdla,  Konrad Schindler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Global Registration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choy_Deep_Global_Registration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christopher Choy,  Wei Dong,  Vladlen Koltun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0460</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0518</td></tr>
<tr><td>Deep Stereo Using Adaptive Thin Volume Representation With Uncertainty Awareness <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuo Cheng,  Zexiang Xu,  Shilin Zhu,  Zhuwen Li,  Li Erran Li,  Ravi Ramamoorthi,  Hao Su</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0135</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Why Having 10,000 Parameters in Your Camera Model Is Better Than Twelve <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Schops_Why_Having_10000_Parameters_in_Your_Camera_Model_Is_Better_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thomas Schops,  Viktor Larsson,  Marc Pollefeys,  Torsten Sattler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0280</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0111</td> <td>0.0000</td> <td>0.0376</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Blur Aware Calibration of Multi-Focus Plenoptic Camera <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Labussiere_Blur_Aware_Calibration_of_Multi-Focus_Plenoptic_Camera_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mathieu Labussiere,  Celine Teuliere,  Frederic Bernardin,  Omar Ait-Aider</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Fused Pixel and Feature-Based View Reconstructions for Light Fields <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Learning_Fused_Pixel_and_Feature-Based_View_Reconstructions_for_Light_Fields_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinglei Shi,  Xiaoran Jiang,  Christine Guillemot</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0763</td> <td>0.0000</td> <td>0.0000</td> <td>0.0825</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0251</td></tr>
<tr><td>SAL: Sign Agnostic Learning of Shapes From Raw Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Atzmon_SAL_Sign_Agnostic_Learning_of_Shapes_From_Raw_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matan Atzmon,  Yaron Lipman</td> <td>0.0000</td> <td>0.0700</td> <td>0.0289</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0304</td> <td>0.0000</td></tr>
<tr><td>Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Weyand_Google_Landmarks_Dataset_v2_-_A_Large-Scale_Benchmark_for_Instance-Level_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tobias Weyand,  Andre Araujo,  Bingyi Cao,  Jack Sim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0525</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0030</td></tr>
<tr><td>Instance Guided Proposal Network for Person Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Instance_Guided_Proposal_Network_for_Person_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenkai Dong,  Zhaoxiang Zhang,  Chunfeng Song,  Tieniu Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>Which Is Plagiarism: Fashion Image Retrieval Based on Regional Representation for Design Protection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_Which_Is_Plagiarism_Fashion_Image_Retrieval_Based_on_Regional_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yining Lang,  Yuan He,  Fan Yang,  Jianfeng Dong,  Hui Xue</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0243</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0024</td></tr>
<tr><td>Inter-Task Association Critic for Cross-Resolution Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Inter-Task_Association_Critic_for_Cross-Resolution_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiyi Cheng,  Qi Dong,  Shaogang Gong,  Xiatian Zhu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0246</td> <td>0.0000</td></tr>
<tr><td>FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_FineGym_A_Hierarchical_Video_Dataset_for_Fine-Grained_Action_Understanding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dian Shao,  Yue Zhao,  Bo Dai,  Dahua Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.4434</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Warburg_Mapillary_Street-Level_Sequences_A_Dataset_for_Lifelong_Place_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Frederik Warburg,  Soren Hauberg,  Manuel Lopez-Antequera,  Pau Gargallo,  Yubin Kuang,  Javier Civera</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0596</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0126</td></tr>
<tr><td>BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_BDD100K_A_Diverse_Driving_Dataset_for_Heterogeneous_Multitask_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fisher Yu,  Haofeng Chen,  Xin Wang,  Wenqi Xian,  Yingying Chen,  Fangchen Liu,  Vashisht Madhavan,  Trevor Darrell</td> <td>0.0100</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0449</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Rethinking Computer-Aided Tuberculosis Diagnosis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Rethinking_Computer-Aided_Tuberculosis_Diagnosis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yun Liu,  Yu-Huan Wu,  Yunfeng Ban,  Huifang Wang,  Ming-Ming Cheng</td> <td>0.0046</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0411</td> <td>0.0000</td></tr>
<tr><td>IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_IntrA_3D_Intracranial_Aneurysm_Dataset_for_Deep_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xi Yang,  Ding Xia,  Taichi Kin,  Takeo Igarashi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0022</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Revisiting Saliency Metrics: Farthest-Neighbor Area Under Curve <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jia_Revisiting_Saliency_Metrics_Farthest-Neighbor_Area_Under_Curve_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sen Jia,  Neil D. B. Bruce</td> <td>0.0000</td> <td>0.0000</td> <td>0.0623</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td></tr>
<tr><td>Computing the Testing Error Without a Testing Set <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Corneanu_Computing_the_Testing_Error_Without_a_Testing_Set_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ciprian A. Corneanu,  Sergio Escalera,  Aleix M. Martinez</td> <td>0.0000</td> <td>0.0000</td> <td>0.0406</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0431</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0358</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Improving Confidence Estimates for Unfamiliar Examples <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Improving_Confidence_Estimates_for_Unfamiliar_Examples_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhizhong Li,  Derek Hoiem</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CycleISP: Real Image Restoration via Improved Data Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zamir_CycleISP_Real_Image_Restoration_via_Improved_Data_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Syed Waqas Zamir,  Aditya Arora,  Salman Khan,  Munawar Hayat,  Fahad Shahbaz Khan,  Ming-Hsuan Yang,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0379</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0169</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0076</td></tr>
<tr><td>Enhanced Blind Face Restoration With Multi-Exemplar Images and Adaptive Spatial Feature Fusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Enhanced_Blind_Face_Restoration_With_Multi-Exemplar_Images_and_Adaptive_Spatial_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoming Li,  Wenyu Li,  Dongwei Ren,  Hongzhi Zhang,  Meng Wang,  Wangmeng Zuo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0195</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Explorable Super Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bahat_Explorable_Super_Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuval Bahat,  Tomer Michaeli</td> <td>0.0000</td> <td>0.0000</td> <td>0.0102</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Syn2Real Transfer Learning for Image Deraining Using Gaussian Processes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yasarla_Syn2Real_Transfer_Learning_for_Image_Deraining_Using_Gaussian_Processes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rajeev Yasarla,  Vishwanath A. Sindagi,  Vishal M. Patel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0231</td></tr>
<tr><td>Deblurring by Realistic Blurring <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deblurring_by_Realistic_Blurring_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaihao Zhang,  Wenhan Luo,  Yiran Zhong,  Lin Ma,  Bjorn Stenger,  Wei Liu,  Hongdong Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0354</td></tr>
<tr><td>Bringing Old Photos Back to Life <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_Bringing_Old_Photos_Back_to_Life_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziyu Wan,  Bo Zhang,  Dongdong Chen,  Pan Zhang,  Dong Chen,  Jing Liao,  Fang Wen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0429</td></tr>
<tr><td>A Physics-Based Noise Formation Model for Extreme Low-Light Raw Denoising <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_A_Physics-Based_Noise_Formation_Model_for_Extreme_Low-Light_Raw_Denoising_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaixuan Wei,  Ying Fu,  Jiaolong Yang,  Hua Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0080</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Camouflaged Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Deng-Ping Fan,  Ge-Peng Ji,  Guolei Sun,  Ming-Ming Cheng,  Jianbing Shen,  Ling Shao</td> <td>0.0890</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0576</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0059</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Holistically-Attracted Wireframe Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xue_Holistically-Attracted_Wireframe_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nan Xue,  Tianfu Wu,  Song Bai,  Fudong Wang,  Gui-Song Xia,  Liangpei Zhang,  Philip H.S. Torr</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0888</td></tr>
<tr><td>Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Conv-MPN_Convolutional_Message_Passing_Neural_Network_for_Structured_Outdoor_Architecture_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fuyang Zhang,  Nelson Nauata,  Yasutaka Furukawa</td> <td>0.0000</td> <td>0.0000</td> <td>0.0154</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Domain Adaptation for Image Dehazing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_Domain_Adaptation_for_Image_Dehazing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuanjie Shao,  Lerenhan Li,  Wenqi Ren,  Changxin Gao,  Nong Sang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0162</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td></tr>
<tr><td>Auto-Encoding Twin-Bottleneck Hashing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Auto-Encoding_Twin-Bottleneck_Hashing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuming Shen,  Jie Qin,  Jiaxin Chen,  Mengyang Yu,  Li Liu,  Fan Zhu,  Fumin Shen,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0090</td></tr>
<tr><td>Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chiu_Agriculture-Vision_A_Large_Aerial_Image_Database_for_Agricultural_Pattern_Analysis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mang Tik Chiu,  Xingqian Xu,  Yunchao Wei,  Zilong Huang,  Alexander G. Schwing,  Robert Brunner,  Hrant Khachatrian,  Hovnatan Karapetyan,  Ivan Dozier,  Greg Rose,  David Wilson,  Adrian Tudor,  Naira Hovakimyan,  Thomas S. Huang,  Honghui Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1577</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1338</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Bi-Directional Interaction Network for Person Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Bi-Directional_Interaction_Network_for_Person_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenkai Dong,  Zhaoxiang Zhang,  Chunfeng Song,  Tieniu Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td></tr>
<tr><td>Meshlet Priors for 3D Mesh Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Badki_Meshlet_Priors_for_3D_Mesh_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abhishek Badki,  Orazio Gallo,  Jan Kautz,  Pradeep Sen</td> <td>0.0000</td> <td>0.0130</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Space-Time-Aware Multi-Resolution Video Enhancement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Haris_Space-Time-Aware_Multi-Resolution_Video_Enhancement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muhammad Haris,  Greg Shakhnarovich,  Norimichi Ukita</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0349</td></tr>
<tr><td>FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_FSS-1000_A_1000-Class_Dataset_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiang Li,  Tianhan Wei,  Yau Pun Chen,  Yu-Wing Tai,  Chi-Keung Tang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0095</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lambert_MSeg_A_Composite_Dataset_for_Multi-Domain_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>John Lambert,  Zhuang Liu,  Ozan Sener,  James Hays,  Vladlen Koltun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1935</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Learning_Multi-Granular_Hypergraphs_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yichao Yan,  Jie Qin,  Jiaxin Chen,  Li Liu,  Fan Zhu,  Ying Tai,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0312</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0207</td></tr>
<tr><td>Online Joint Multi-Metric Adaptation From Frequent Sharing-Subset Mining for Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Online_Joint_Multi-Metric_Adaptation_From_Frequent_Sharing-Subset_Mining_for_Person_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiahuan Zhou,  Bing Su,  Ying Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0274</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0280</td> <td>0.0000</td></tr>
<tr><td>Taking a Deeper Look at Co-Salient Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Taking_a_Deeper_Look_at_Co-Salient_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Deng-Ping Fan,  Zheng Lin,  Ge-Peng Ji,  Dingwen Zhang,  Huazhu Fu,  Ming-Ming Cheng</td> <td>0.0553</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0037</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0060</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Single-Stage 6D Object Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Single-Stage_6D_Object_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yinlin Hu,  Pascal Fua,  Wei Wang,  Mathieu Salzmann</td> <td>0.0155</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0776</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>OccuSeg: Occupancy-Aware 3D Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_OccuSeg_Occupancy-Aware_3D_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Han,  Tian Zheng,  Lan Xu,  Lu Fang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1575</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Camera Trace Erasing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Camera_Trace_Erasing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chang Chen,  Zhiwei Xiong,  Xiaoming Liu,  Feng Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Metric Learning via Adaptive Learnable Assessment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Deep_Metric_Learning_via_Adaptive_Learnable_Assessment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenzhao Zheng,  Jiwen Lu,  Jie Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0337</td></tr>
<tr><td>Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Deep_Representation_Learning_on_Long-Tailed_Data_A_Learnable_Embedding_Augmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jialun Liu,  Yifan Sun,  Chuchu Han,  Zhaopeng Dou,  Wenhui Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Fantastic_Answers_and_Where_to_Find_Them_Immersive_Question-Directed_Visual_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ming Jiang,  Shi Chen,  Jinhui Yang,  Qi Zhao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0075</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HUMBI: A Large Multiview Dataset of Human Body Expressions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_HUMBI_A_Large_Multiview_Dataset_of_Human_Body_Expressions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhixuan Yu,  Jae Shin Yoon,  In Kyu Lee,  Prashanth Venkatesh,  Jaesik Park,  Jihun Yu,  Hyun Soo Park</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Image Search With Text Feedback by Visiolinguistic Attention Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Image_Search_With_Text_Feedback_by_Visiolinguistic_Attention_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanbei Chen,  Shaogang Gong,  Loris Bazzani</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0037</td></tr>
<tr><td>Image Processing Using Multi-Code GAN Prior <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Image_Processing_Using_Multi-Code_GAN_Prior_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinjin Gu,  Yujun Shen,  Bolei Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2012</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>What Does Plate Glass Reveal About Camera Calibration? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_What_Does_Plate_Glass_Reveal_About_Camera_Calibration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qian Zheng,  Jinnan Chen,  Zhan Lu,  Boxin Shi,  Xudong Jiang,  Kim-Hui Yap,  Ling-Yu Duan,  Alex C. Kot</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td></tr>
<tr><td>Zero-Assignment Constraint for Graph Matching With Outliers <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Zero-Assignment_Constraint_for_Graph_Matching_With_Outliers_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fudong Wang,  Nan Xue,  Jin-Gang Yu,  Gui-Song Xia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0416</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0097</td></tr>
<tr><td>Cascaded Deep Video Deblurring Using Temporal Sharpness Prior <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinshan Pan,  Haoran Bai,  Jinhui Tang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0183</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1057</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0058</td></tr>
<tr><td>JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Keren Fu,  Deng-Ping Fan,  Ge-Peng Ji,  Qijun Zhao</td> <td>0.0206</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0552</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0367</td> <td>0.0000</td></tr>
<tr><td>From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_From_Fidelity_to_Perceptual_Quality_A_Semi-Supervised_Approach_for_Low-Light_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenhan Yang,  Shiqi Wang,  Yuming Fang,  Yue Wang,  Jiaying Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Adaptation Learning for Hyperspectral Imagery Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Unsupervised_Adaptation_Learning_for_Hyperspectral_Imagery_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Zhang,  Jiangtao Nie,  Wei Wei,  Yanning Zhang,  Shengcai Liao,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Central Similarity Quantization for Efficient Image and Video Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Central_Similarity_Quantization_for_Efficient_Image_and_Video_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li Yuan,  Tao Wang,  Xiaopeng Zhang,  Francis EH Tay,  Zequn Jie,  Wei Liu,  Jiashi Feng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ARCH: Animatable Reconstruction of Clothed Humans <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_ARCH_Animatable_Reconstruction_of_Clothed_Humans_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zeng Huang,  Yuanlu Xu,  Christoph Lassner,  Hao Li,  Tony Tung</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0142</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0325</td> <td>0.0154</td></tr>
<tr><td>A Model-Driven Deep Neural Network for Single Image Rain Removal <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_A_Model-Driven_Deep_Neural_Network_for_Single_Image_Rain_Removal_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hong Wang,  Qi Xie,  Qian Zhao,  Deyu Meng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0402</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0101</td></tr>
<tr><td>Novel Object Viewpoint Estimation Through Reconstruction Alignment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Banani_Novel_Object_Viewpoint_Estimation_Through_Reconstruction_Alignment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohamed El Banani,  Jason J. Corso,  David F. Fouhey</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0239</td> <td>0.0000</td></tr>
<tr><td>Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Creating_Something_From_Nothing_Unsupervised_Knowledge_Distillation_for_Cross-Modal_Hashing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hengtong Hu,  Lingxi Xie,  Richang Hong,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0178</td></tr>
<tr><td>Evaluating Weakly Supervised Object Localization Methods Right <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choe_Evaluating_Weakly_Supervised_Object_Localization_Methods_Right_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junsuk Choe,  Seong Joon Oh,  Seungho Lee,  Sanghyuk Chun,  Zeynep Akata,  Hyunjung Shim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Style Normalization and Restitution for Generalizable Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_Style_Normalization_and_Restitution_for_Generalizable_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Jin,  Cuiling Lan,  Wenjun Zeng,  Zhibo Chen,  Li Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1321</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Reconstruct Locally, Localize Globally: A Model Free Method for Object Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ming Cai,  Ian Reid</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0178</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0252</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RoboTHOR: An Open Simulation-to-Real Embodied AI Platform <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Deitke_RoboTHOR_An_Open_Simulation-to-Real_Embodied_AI_Platform_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matt Deitke,  Winson Han,  Alvaro Herrasti,  Aniruddha Kembhavi,  Eric Kolve,  Roozbeh Mottaghi,  Jordi Salvador,  Dustin Schwenk,  Eli VanderBilt,  Matthew Wallingford,  Luca Weihs,  Mark Yatskar,  Ali Farhadi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0072</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1498</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>All in One Bad Weather Removal Using Architectural Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_All_in_One_Bad_Weather_Removal_Using_Architectural_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruoteng Li,  Robby T. Tan,  Loong-Fah Cheong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0735</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0121</td></tr>
<tr><td>Relation-Aware Global Attention for Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Relation-Aware_Global_Attention_for_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhizheng Zhang,  Cuiling Lan,  Wenjun Zeng,  Xin Jin,  Zhibo Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0204</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0588</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HOnnotate: A Method for 3D Annotation of Hand and Object Poses <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shreyas Hampali,  Mahdi Rad,  Markus Oberweger,  Vincent Lepetit</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0191</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Celeb-DF_A_Large-Scale_Challenging_Dataset_for_DeepFake_Forensics_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuezun Li,  Xin Yang,  Pu Sun,  Honggang Qi,  Siwei Lyu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Unfolding Network for Image Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deep_Unfolding_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Zhang,  Luc Van Gool,  Radu Timofte</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On the Uncertainty of Self-Supervised Monocular Depth Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Poggi_On_the_Uncertainty_of_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matteo Poggi,  Filippo Aleotti,  Fabio Tosi,  Stefano Mattoccia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1593</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Proxy Anchor Loss for Deep Metric Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Proxy_Anchor_Loss_for_Deep_Metric_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sungyeon Kim,  Dongwon Kim,  Minsu Cho,  Suha Kwak</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0062</td></tr>
<tr><td>Unsupervised Learning for Intrinsic Image Decomposition From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unsupervised_Learning_for_Intrinsic_Image_Decomposition_From_a_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunfei Liu,  Yu Li,  Shaodi You,  Feng Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0312</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0241</td> <td>0.0382</td></tr>
<tr><td>Multi-Domain Learning for Accurate and Few-Shot Color Constancy <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiao_Multi-Domain_Learning_for_Accurate_and_Few-Shot_Color_Constancy_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jin Xiao,  Shuhang Gu,  Lei Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0183</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0146</td> <td>0.0067</td></tr>
<tr><td>PANDA: A Gigapixel-Level Human-Centric Video Dataset <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_PANDA_A_Gigapixel-Level_Human-Centric_Video_Dataset_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xueyang Wang,  Xiya Zhang,  Yinheng Zhu,  Yuchen Guo,  Xiaoyun Yuan,  Liuyu Xiang,  Zerun Wang,  Guiguang Ding,  David Brady,  Qionghai Dai,  Lu Fang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cross-View Tracking for Multi-Human 3D Pose Estimation at Over 100 FPS <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Cross-View_Tracking_for_Multi-Human_3D_Pose_Estimation_at_Over_100_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Long Chen,  Haizhou Ai,  Rui Chen,  Zijie Zhuang,  Shuang Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0907</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0264</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Spatial-Temporal Graph Convolutional Network for Video-Based Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinrui Yang,  Wei-Shi Zheng,  Qize Yang,  Ying-Cong Chen,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0055</td></tr>
<tr><td>Salience-Guided Cascaded Suppression Network for Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Salience-Guided_Cascaded_Suppression_Network_for_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuesong Chen,  Canmiao Fu,  Yong Zhao,  Feng Zheng,  Jingkuan Song,  Rongrong Ji,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0713</td></tr>
<tr><td>Fashion Outfit Complementary Item Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Fashion_Outfit_Complementary_Item_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yen-Liang Lin,  Son Tran,  Larry S. Davis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0648</td></tr>
<tr><td>Learning Event-Based Motion Deblurring <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Learning_Event-Based_Motion_Deblurring_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhe Jiang,  Yu Zhang,  Dongqing Zou,  Jimmy Ren,  Jiancheng Lv,  Yebin Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0127</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Domain Decluttering: Simplifying Images to Mitigate Synthetic-Real Domain Shift and Improve Depth Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Domain_Decluttering_Simplifying_Images_to_Mitigate_Synthetic-Real_Domain_Shift_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunhan Zhao,  Shu Kong,  Daeyun Shin,  Charless Fowlkes</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0048</td> <td>0.1123</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0241</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0222</td> <td>0.0029</td></tr>
<tr><td>Neural Blind Deconvolution Using Deep Priors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ren_Neural_Blind_Deconvolution_Using_Deep_Priors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongwei Ren,  Kai Zhang,  Qilong Wang,  Qinghua Hu,  Wangmeng Zuo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0065</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0131</td></tr>
<tr><td>Anisotropic Convolutional Networks for 3D Semantic Scene Completion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Anisotropic_Convolutional_Networks_for_3D_Semantic_Scene_Completion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Li,  Kai Han,  Peng Wang,  Yu Liu,  Xia Yuan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0275</td></tr>
<tr><td>TDAN: Temporally-Deformable Alignment Network for Video Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tian_TDAN_Temporally-Deformable_Alignment_Network_for_Video_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yapeng Tian,  Yulun Zhang,  Yun Fu,  Chenliang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0396</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0024</td></tr>
<tr><td>Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiang_Zooming_Slow-Mo_Fast_and_Accurate_One-Stage_Space-Time_Video_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoyu Xiang,  Yapeng Tian,  Yulun Zhang,  Yun Fu,  Jan P. Allebach,  Chenliang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0556</td></tr>
<tr><td>Fast MSER <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Fast_MSER_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hailiang Xu,  Siqi Xie,  Fan Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0103</td></tr>
<tr><td>Unsupervised Person Re-Identification via Softened Similarity Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Unsupervised_Person_Re-Identification_via_Softened_Similarity_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yutian Lin,  Lingxi Xie,  Yu Wu,  Chenggang Yan,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0451</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>COCAS: A Large-Scale Clothes Changing Person Dataset for Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_COCAS_A_Large-Scale_Clothes_Changing_Person_Dataset_for_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shijie Yu,  Shihua Li,  Dapeng Chen,  Rui Zhao,  Junjie Yan,  Yu Qiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Formation of Physically-Based Face Attributes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Learning_Formation_of_Physically-Based_Face_Attributes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruilong Li,  Karl Bladin,  Yajie Zhao,  Chinmay Chinara,  Owen Ingraham,  Pengda Xiang,  Xinglei Ren,  Pratusha Prasad,  Bipin Kishore,  Jun Xing,  Hao Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Generalized Product Quantization Network for Semi-Supervised Image Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jang_Generalized_Product_Quantization_Network_for_Semi-Supervised_Image_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Young Kyun Jang,  Nam Ik Cho</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0349</td></tr>
<tr><td>Stereoscopic Flash and No-Flash Photography for Shape and Albedo Recovery <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_Stereoscopic_Flash_and_No-Flash_Photography_for_Shape_and_Albedo_Recovery_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xu Cao,  Michael Waechter,  Boxin Shi,  Ye Gao,  Bo Zheng,  Yasuyuki Matsushita</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Context-Aware Group Captioning via Self-Attention and Contrastive Features <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Context-Aware_Group_Captioning_via_Self-Attention_and_Contrastive_Features_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhuowan Li,  Quan Tran,  Long Mai,  Zhe Lin,  Alan L. Yuille</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0076</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MEBOW: Monocular Estimation of Body Orientation in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_MEBOW_Monocular_Estimation_of_Body_Orientation_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenyan Wu,  Yukun Chen,  Jiajia Luo,  Che-Chun Su,  Anuja Dawane,  Bikramjot Hanzra,  Zhuo Deng,  Bilan Liu,  James Z. Wang,  Cheng-hao Kuo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2254</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0171</td> <td>0.0000</td></tr>
<tr><td>Distilling Image Dehazing With Heterogeneous Task Imitation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hong_Distilling_Image_Dehazing_With_Heterogeneous_Task_Imitation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ming Hong,  Yuan Xie,  Cuihua Li,  Yanyun Qu</td> <td>0.0118</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0339</td></tr>
<tr><td>Select, Supplement and Focus for RGB-D Saliency Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Select_Supplement_and_Focus_for_RGB-D_Saliency_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Miao Zhang,  Weisong Ren,  Yongri Piao,  Zhengkun Rong,  Huchuan Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0531</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Transfer Learning From Synthetic to Real-Noise Denoising With Adaptive Instance Normalization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Transfer_Learning_From_Synthetic_to_Real-Noise_Denoising_With_Adaptive_Instance_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yoonsik Kim,  Jae Woong Soh,  Gu Yong Park,  Nam Ik Cho</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On Joint Estimation of Pose, Geometry and svBRDF From a Handheld Scanner <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Schmitt_On_Joint_Estimation_of_Pose_Geometry_and_svBRDF_From_a_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Carolin Schmitt,  Simon Donne,  Gernot Riegler,  Vladlen Koltun,  Andreas Geiger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0008</td></tr>
<tr><td>Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Niemeyer_Differentiable_Volumetric_Rendering_Learning_Implicit_3D_Representations_Without_3D_Supervision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michael Niemeyer,  Lars Mescheder,  Michael Oechsle,  Andreas Geiger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Meta-Transfer Learning for Zero-Shot Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Soh_Meta-Transfer_Learning_for_Zero-Shot_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jae Woong Soh,  Sunwoo Cho,  Nam Ik Cho</td> <td>0.0000</td> <td>0.0000</td> <td>0.0401</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Solving Jigsaw Puzzles With Eroded Boundaries <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bridger_Solving_Jigsaw_Puzzles_With_Eroded_Boundaries_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dov Bridger,  Dov Danon,  Ayellet Tal</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0681</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Context-Aware Attention Network for Image-Text Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Context-Aware_Attention_Network_for_Image-Text_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Zhang,  Zhen Lei,  Zhaoxiang Zhang,  Stan Z. Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0008</td></tr>
<tr><td>M-LVC: Multiple Frames Prediction for Learned Video Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_M-LVC_Multiple_Frames_Prediction_for_Learned_Video_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianping Lin,  Dong Liu,  Houqiang Li,  Feng Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0288</td></tr>
<tr><td>Efficient Dynamic Scene Deblurring Using Spatially Variant Deconvolution Network With Optical Flow Guided Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Efficient_Dynamic_Scene_Deblurring_Using_Spatially_Variant_Deconvolution_Network_With_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuan Yuan,  Wei Su,  Dandan Ma</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1393</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0686</td></tr>
<tr><td>Single Image Reflection Removal Through Cascaded Refinement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Single_Image_Reflection_Removal_Through_Cascaded_Refinement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chao Li,  Yixiao Yang,  Kun He,  Stephen Lin,  John E. Hopcroft</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0527</td></tr>
<tr><td>From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ying_From_Patches_to_Pictures_PaQ-2-PiQ_Mapping_the_Perceptual_Space_of_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenqiang Ying,  Haoran Niu,  Praful Gupta,  Dhruv Mahajan,  Deepti Ghadiyaram,  Alan Bovik</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0368</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Video to Events: Recycling Video Datasets for Event Cameras <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gehrig_Video_to_Events_Recycling_Video_Datasets_for_Event_Cameras_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daniel Gehrig,  Mathias Gehrig,  Javier Hidalgo-Carrio,  Davide Scaramuzza</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0646</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0253</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Composed Query Image Retrieval Using Locally Bounded Features <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hosseinzadeh_Composed_Query_Image_Retrieval_Using_Locally_Bounded_Features_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mehrdad Hosseinzadeh,  Yang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0686</td></tr>
<tr><td>Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Suin_Spatially-Attentive_Patch-Hierarchical_Network_for_Adaptive_Motion_Deblurring_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maitreya Suin,  Kuldeep Purohit,  A. N. Rajagopalan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>End-to-End Illuminant Estimation Based on Deep Metric Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_End-to-End_Illuminant_Estimation_Based_on_Deep_Metric_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bolei Xu,  Jingxin Liu,  Xianxu Hou,  Bozhi Liu,  Guoping Qiu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0305</td></tr>
<tr><td>Variational-EM-Based Deep Learning for Noise-Blind Image Deblurring <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nan_Variational-EM-Based_Deep_Learning_for_Noise-Blind_Image_Deblurring_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuesong Nan,  Yuhui Quan,  Hui Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0101</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0264</td></tr>
<tr><td>Image Demoireing with Learnable Bandpass Filters <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Image_Demoireing_with_Learnable_Bandpass_Filters_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bolun Zheng,  Shanxin Yuan,  Gregory Slabaugh,  Ales Leonardis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0414</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1319</td></tr>
<tr><td>Assessing Image Quality Issues for Real-World Problems <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chiu_Assessing_Image_Quality_Issues_for_Real-World_Problems_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tai-Yin Chiu,  Yinan Zhao,  Danna Gurari</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Memory-Efficient Hierarchical Neural Architecture Search for Image Denoising <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Memory-Efficient_Hierarchical_Neural_Architecture_Search_for_Image_Denoising_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haokui Zhang,  Ying Li,  Hao Chen,  Chunhua Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0137</td> <td>0.2052</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0294</td></tr>
<tr><td>Blindly Assess Image Quality in the Wild Guided by a Self-Adaptive Hyper Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaolin Su,  Qingsen Yan,  Yu Zhu,  Cheng Zhang,  Xin Ge,  Jinqiu Sun,  Yanning Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0345</td></tr>
<tr><td>Perceptual Quality Assessment of Smartphone Photography <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_Perceptual_Quality_Assessment_of_Smartphone_Photography_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuming Fang,  Hanwei Zhu,  Yan Zeng,  Kede Ma,  Zhou Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0293</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0089</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Don't Hit Me! Glass Detection in Real-World Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mei_Dont_Hit_Me_Glass_Detection_in_Real-World_Scenes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haiyang Mei,  Xin Yang,  Yang Wang,  Yuanyuan Liu,  Shengfeng He,  Qiang Zhang,  Xiaopeng Wei,  Rynson W.H. Lau</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0204</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0569</td></tr>
<tr><td>Progressive Mirror Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Progressive_Mirror_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiaying Lin,  Guodong Wang,  Rynson W.H. Lau</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0136</td></tr>
<tr><td>Category-Level Articulated Object Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Category-Level_Articulated_Object_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaolong Li,  He Wang,  Li Yi,  Leonidas J. Guibas,  A. Lynn Abbott,  Shuran Song</td> <td>0.0000</td> <td>0.0266</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0170</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unbiased Scene Graph Generation From Biased Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Unbiased_Scene_Graph_Generation_From_Biased_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaihua Tang,  Yulei Niu,  Jianqiang Huang,  Jiaxin Shi,  Hanwang Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0582</td> <td>0.0000</td> <td>0.0098</td></tr>
<tr><td>Dynamic Graph Message Passing Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Dynamic_Graph_Message_Passing_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li Zhang,  Dan Xu,  Anurag Arnab,  Philip H.S. Torr</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0557</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly Supervised Visual Semantic Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zareian_Weakly_Supervised_Visual_Semantic_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alireza Zareian,  Svebor Karaman,  Shih-Fu Chang</td> <td>0.0033</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0032</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GPS-Net: Graph Property Sensing Network for Scene Graph Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_GPS-Net_Graph_Property_Sensing_Network_for_Scene_Graph_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Lin,  Changxing Ding,  Jinquan Zeng,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0135</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0359</td> <td>0.0000</td> <td>0.0340</td></tr>
<tr><td>End-to-End Optimization of Scene Layout <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_End-to-End_Optimization_of_Scene_Layout_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Andrew Luo,  Zhoutong Zhang,  Jiajun Wu,  Joshua B. Tenenbaum</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0435</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Intra-Domain Adaptation for Semantic Segmentation Through Self-Supervision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Unsupervised_Intra-Domain_Adaptation_for_Semantic_Segmentation_Through_Self-Supervision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fei Pan,  Inkyu Shin,  Francois Rameau,  Seokju Lee,  In So Kweon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0034</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0506</td> <td>0.1135</td> <td>0.0000</td> <td>0.0550</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0014</td></tr>
<tr><td>Dual Super-Resolution Learning for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li Wang,  Dong Li,  Yousong Zhu,  Lu Tian,  Yi Shan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2360</td> <td>0.1466</td> <td>0.0000</td> <td>0.0000</td> <td>0.0071</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0136</td></tr>
<tr><td>Self-Supervised Scene De-Occlusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Self-Supervised_Scene_De-Occlusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaohang Zhan,  Xingang Pan,  Bo Dai,  Ziwei Liu,  Dahua Lin,  Chen Change Loy</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0013</td></tr>
<tr><td>BANet: Bidirectional Aggregation Network With Occlusion Handling for Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_BANet_Bidirectional_Aggregation_Network_With_Occlusion_Handling_for_Panoptic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yifeng Chen,  Guangchen Lin,  Songyuan Li,  Omar Bourahla,  Yiming Wu,  Fangfang Wang,  Junyi Feng,  Mingliang Xu,  Xi Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2020</td> <td>0.0000</td> <td>0.1455</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0355</td></tr>
<tr><td>CPR-GCN: Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_CPR-GCN_Conditional_Partial-Residual_Graph_Convolutional_Network_in_Automated_Anatomical_Labeling_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Han Yang,  Xingjian Zhen,  Ying Chi,  Lei Zhang,  Xian-Sheng Hua</td> <td>0.0000</td> <td>0.0000</td> <td>0.0180</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0402</td></tr>
<tr><td>Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Cross-View_Correspondence_Reasoning_Based_on_Bipartite_Graph_Convolutional_Network_for_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuhang Liu,  Fandong Zhang,  Qianyi Zhang,  Siwen Wang,  Yizhou Wang,  Yizhou Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0360</td></tr>
<tr><td>MPM: Joint Representation of Motion and Position Map for Cell Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hayashida_MPM_Joint_Representation_of_Motion_and_Position_Map_for_Cell_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junya Hayashida,  Kazuya Nishimura,  Ryoma Bise</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0302</td></tr>
<tr><td>Deep Distance Transform for Tubular Structure Segmentation in CT Scans <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Deep_Distance_Transform_for_Tubular_Structure_Segmentation_in_CT_Scans_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yan Wang,  Xu Wei,  Fengze Liu,  Jieneng Chen,  Yuyin Zhou,  Wei Shen,  Elliot K. Fishman,  Alan L. Yuille</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Instance Segmentation of Biological Images Using Harmonic Embeddings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kulikov_Instance_Segmentation_of_Biological_Images_Using_Harmonic_Embeddings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Victor Kulikov,  Victor Lempitsky</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2269</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0155</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-scale Domain-adversarial Multiple-instance CNN for Cancer Subtype Classification with Unannotated Histopathological Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hashimoto_Multi-scale_Domain-adversarial_Multiple-instance_CNN_for_Cancer_Subtype_Classification_with_Unannotated_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Noriaki Hashimoto,  Daisuke Fukushima,  Ryoichi Koga,  Yusuke Takagi,  Kaho Ko,  Kei Kohno,  Masato Nakaguro,  Shigeo Nakamura,  Hidekata Hontani,  Ichiro Takeuchi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0265</td></tr>
<tr><td>SOS: Selective Objective Switch for Rapid Immunofluorescence Whole Slide Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Maksoud_SOS_Selective_Objective_Switch_for_Rapid_Immunofluorescence_Whole_Slide_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sam Maksoud,  Kun Zhao,  Peter Hobson,  Anthony Jennings,  Brian C. Lovell</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_Task_Agnostic_Robust_Learning_on_Corrupt_Outputs_by_Correlation-Guided_Mixture_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sungjoon Choi,  Sanghoon Hong,  Kyungjae Lee,  Sungbin Lim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0377</td> <td>0.1306</td></tr>
<tr><td>METAL: Minimum Effort Temporal Activity Localization in Untrimmed Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_METAL_Minimum_Effort_Temporal_Activity_Localization_in_Untrimmed_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Da Zhang,  Xiyang Dai,  Yuan-Fang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0064</td></tr>
<tr><td>Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Neural_Data_Server_A_Large-Scale_Search_Engine_for_Transfer_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xi Yan,  David Acuna,  Sanja Fidler</td> <td>0.0186</td> <td>0.0000</td> <td>0.0001</td> <td>0.0000</td> <td>0.0523</td> <td>0.0000</td> <td>0.0000</td> <td>0.1046</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0307</td> <td>0.0000</td></tr>
<tr><td>Revisiting Knowledge Distillation via Label Smoothing Regularization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li Yuan,  Francis EH Tay,  Guilin Li,  Tao Wang,  Jiashi Feng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0170</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>WCP: Worst-Case Perturbations for Semi-Supervised Deep Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_WCP_Worst-Case_Perturbations_for_Semi-Supervised_Deep_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Liheng Zhang,  Guo-Jun Qi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DEPARA: Deep Attribution Graph for Deep Knowledge Transferability <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Song_DEPARA_Deep_Attribution_Graph_for_Deep_Knowledge_Transferability_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Song,  Yixin Chen,  Jingwen Ye,  Xinchao Wang,  Chengchao Shen,  Feng Mao,  Mingli Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0433</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0173</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0419</td></tr>
<tr><td>Conditional Channel Gated Networks for Task-Aware Continual Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abati_Conditional_Channel_Gated_Networks_for_Task-Aware_Continual_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Davide Abati,  Jakub Tomczak,  Tijmen Blankevoort,  Simone Calderara,  Rita Cucchiara,  Babak Ehteshami Bejnordi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0318</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Discriminability and Diversity: Batch Nuclear-Norm Maximization Under Label Insufficient Situations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cui_Towards_Discriminability_and_Diversity_Batch_Nuclear-Norm_Maximization_Under_Label_Insufficient_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuhao Cui,  Shuhui Wang,  Junbao Zhuo,  Liang Li,  Qingming Huang,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0406</td> <td>0.0000</td> <td>0.0731</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0034</td></tr>
<tr><td>FocalMix: Semi-Supervised Learning for 3D Medical Image Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_FocalMix_Semi-Supervised_Learning_for_3D_Medical_Image_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dong Wang,  Yuan Zhang,  Kexin Zhang,  Liwei Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0266</td></tr>
<tr><td>Learning 3D Semantic Scene Graphs From 3D Indoor Reconstructions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wald_Learning_3D_Semantic_Scene_Graphs_From_3D_Indoor_Reconstructions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Johanna Wald,  Helisa Dhamo,  Nassir Navab,  Federico Tombari</td> <td>0.0000</td> <td>0.0263</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0303</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0242</td> <td>0.0000</td> <td>0.0000</td> <td>0.0984</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Viewpoint Learning From Image Collections <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mustikovela_Self-Supervised_Viewpoint_Learning_From_Image_Collections_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Siva Karthik Mustikovela,  Varun Jampani,  Shalini De Mello,  Sifei Liu,  Umar Iqbal,  Carsten Rother,  Jan Kautz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0301</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Two-Shot Spatially-Varying BRDF and Shape Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Boss_Two-Shot_Spatially-Varying_BRDF_and_Shape_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mark Boss,  Varun Jampani,  Kihwan Kim,  Hendrik P.A. Lensch,  Jan Kautz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0294</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0088</td></tr>
<tr><td>Variational Context-Deformable ConvNets for Indoor Scene Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiong_Variational_Context-Deformable_ConvNets_for_Indoor_Scene_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhitong Xiong,  Yuan Yuan,  Nianhui Guo,  Qi Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0280</td></tr>
<tr><td>Strip Pooling: Rethinking Spatial Pooling for Scene Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Strip_Pooling_Rethinking_Spatial_Pooling_for_Scene_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qibin Hou,  Li Zhang,  Ming-Ming Cheng,  Jiashi Feng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0262</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Few-Shot Object Detection With Attention-RPN and Multi-Relation Detector <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Few-Shot_Object_Detection_With_Attention-RPN_and_Multi-Relation_Detector_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Fan,  Wei Zhuo,  Chi-Keung Tang,  Yu-Wing Tai</td> <td>0.0779</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0678</td> <td>0.0148</td></tr>
<tr><td>What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_What_Can_Be_Transferred_Unsupervised_Domain_Adaptation_for_Endoscopic_Lesions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiahua Dong,  Yang Cong,  Gan Sun,  Bineng Zhong,  Xiaowei Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0481</td> <td>0.1038</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0021</td></tr>
<tr><td>ADINet: Attribute Driven Incremental Network for Retinal Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Meng_ADINet_Attribute_Driven_Incremental_Network_for_Retinal_Image_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qier Meng,  Satoh Shin'ichi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0651</td></tr>
<tr><td>Unsupervised Domain Adaptation With Hierarchical Gradient Synchronization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Unsupervised_Domain_Adaptation_With_Hierarchical_Gradient_Synchronization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lanqing Hu,  Meina Kan,  Shiguang Shan,  Xilin Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3309</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0387</td></tr>
<tr><td>Deep Grouping Model for Unified Perceptual Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deep_Grouping_Model_for_Unified_Perceptual_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiheng Li,  Wenxuan Bao,  Jiayang Zheng,  Chenliang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0110</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Where Am I Looking At? Joint Location and Orientation Estimation by Cross-View Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yujiao Shi,  Xin Yu,  Dylan Campbell,  Hongdong Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0005</td></tr>
<tr><td>Gum-Net: Unsupervised Geometric Matching for Fast and Accurate 3D Subtomogram Image Alignment and Averaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Gum-Net_Unsupervised_Geometric_Matching_for_Fast_and_Accurate_3D_Subtomogram_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiangrui Zeng,  Min Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FDA: Fourier Domain Adaptation for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanchao Yang,  Stefano Soatto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0136</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1270</td> <td>0.1116</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0103</td></tr>
<tr><td>Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Foreground-Aware_Relation_Network_for_Geospatial_Object_Segmentation_in_High_Spatial_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhuo Zheng,  Yanfei Zhong,  Junjue Wang,  Ailong Ma</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1415</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0308</td></tr>
<tr><td>When2com: Multi-Agent Perception via Communication Graph Grouping <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_When2com_Multi-Agent_Perception_via_Communication_Graph_Grouping_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yen-Cheng Liu,  Junjiao Tian,  Nathaniel Glaser,  Zsolt Kira</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Human-Object Interaction Detection Using Interaction Points <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Learning_Human-Object_Interaction_Detection_Using_Interaction_Points_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tiancai Wang,  Tong Yang,  Martin Danelljan,  Fahad Shahbaz Khan,  Xiangyu Zhang,  Jian Sun</td> <td>0.0129</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0383</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_C2FNAS_Coarse-to-Fine_Neural_Architecture_Search_for_3D_Medical_Image_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qihang Yu,  Dong Yang,  Holger Roth,  Yutong Bai,  Yixiao Zhang,  Alan L. Yuille,  Daguang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0132</td> <td>0.2669</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adaptive Subspaces for Few-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christian Simon,  Piotr Koniusz,  Richard Nock,  Mehrtash Harandi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1045</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Detect Important People in Unlabelled Images for Semi-Supervised Important People Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hong_Learning_to_Detect_Important_People_in_Unlabelled_Images_for_Semi-Supervised_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fa-Ting Hong,  Wei-Hong Li,  Wei-Shi Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Stochastic Sparse Subspace Clustering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Stochastic_Sparse_Subspace_Clustering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ying Chen,  Chun-Guang Li,  Chong You</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CRNet: Cross-Reference Networks for Few-Shot Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CRNet_Cross-Reference_Networks_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weide Liu,  Chi Zhang,  Guosheng Lin,  Fayao Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0230</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0098</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td></tr>
<tr><td>Shoestring: Graph-Based Semi-Supervised Classification With Severely Limited Labeled Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Shoestring_Graph-Based_Semi-Supervised_Classification_With_Severely_Limited_Labeled_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wanyu Lin,  Zhaolin Gao,  Baochun Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0086</td></tr>
<tr><td>Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bergmann_Uninformed_Students_Student-Teacher_Anomaly_Detection_With_Discriminative_Latent_Embeddings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Paul Bergmann,  Michael Fauser,  David Sattlegger,  Carsten Steger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0262</td> <td>0.0105</td></tr>
<tr><td>3D Sketch-Aware Semantic Scene Completion via Semi-Supervised Structure Prior <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_3D_Sketch-Aware_Semantic_Scene_Completion_via_Semi-Supervised_Structure_Prior_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaokang Chen,  Kwan-Yee Lin,  Chen Qian,  Gang Zeng,  Hongsheng Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0133</td></tr>
<tr><td>Graph-Guided Architecture Search for Real-Time Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Graph-Guided_Architecture_Search_for_Real-Time_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peiwen Lin,  Peng Sun,  Guangliang Cheng,  Sirui Xie,  Xi Li,  Jianping Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td> <td>0.0932</td> <td>0.0000</td> <td>0.0000</td> <td>0.0906</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Composing Good Shots by Exploiting Mutual Relations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Composing_Good_Shots_by_Exploiting_Mutual_Relations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Debang Li,  Junge Zhang,  Kaiqi Huang,  Ming-Hsuan Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0691</td></tr>
<tr><td>Organ at Risk Segmentation for Head and Neck Cancer Using Stratified Learning and Neural Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Organ_at_Risk_Segmentation_for_Head_and_Neck_Cancer_Using_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dazhou Guo,  Dakai Jin,  Zhuotun Zhu,  Tsung-Ying Ho,  Adam P. Harrison,  Chun-Hung Chao,  Jing Xiao,  Le Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2391</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>G2L-Net: Global to Local Network for Real-Time 6D Pose Estimation With Embedding Vector Features <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_G2L-Net_Global_to_Local_Network_for_Real-Time_6D_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wei Chen,  Xi Jia,  Hyung Jin Chang,  Jinming Duan,  Ales Leonardis</td> <td>0.0000</td> <td>0.1194</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0465</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0259</td></tr>
<tr><td>Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-Weighting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unsupervised_Instance_Segmentation_in_Microscopy_Images_via_Panoptic_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongnan Liu,  Donghao Zhang,  Yang Song,  Fan Zhang,  Lauren O'Donnell,  Heng Huang,  Mei Chen,  Weidong Cai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0602</td> <td>0.0000</td> <td>0.0000</td> <td>0.1874</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0740</td></tr>
<tr><td>Single-Stage Semantic Segmentation From Image Labels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Araslanov_Single-Stage_Semantic_Segmentation_From_Image_Labels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nikita Araslanov,  Stefan Roth</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0459</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cascaded Human-Object Interaction Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Cascaded_Human-Object_Interaction_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianfei Zhou,  Wenguan Wang,  Siyuan Qi,  Haibin Ling,  Jianbing Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DuDoRNet: Learning a Dual-Domain Recurrent Network for Fast MRI Reconstruction With Deep T1 Prior <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_DuDoRNet_Learning_a_Dual-Domain_Recurrent_Network_for_Fast_MRI_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Zhou,  S. Kevin Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0439</td></tr>
<tr><td>Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Learning_Integral_Objects_With_Intra-Class_Discriminator_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junsong Fan,  Zhaoxiang Zhang,  Chunfeng Song,  Tieniu Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1507</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>FPConv: Learning Local Flattening for Point Convolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_FPConv_Learning_Local_Flattening_for_Point_Convolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiqun Lin,  Zizheng Yan,  Haibin Huang,  Dong Du,  Ligang Liu,  Shuguang Cui,  Xiaoguang Han</td> <td>0.0274</td> <td>0.1077</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Rotation Equivariant Graph Convolutional Network for Spherical Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Rotation_Equivariant_Graph_Convolutional_Network_for_Spherical_Image_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qin Yang,  Chenglin Li,  Wenrui Dai,  Junni Zou,  Guo-Jun Qi,  Hongkai Xiong</td> <td>0.0132</td> <td>0.0000</td> <td>0.0289</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FOAL: Fast Online Adaptive Learning for Cardiac Motion Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_FOAL_Fast_Online_Adaptive_Learning_for_Cardiac_Motion_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hanchao Yu,  Shanhui Sun,  Haichao Yu,  Xiao Chen,  Honghui Shi,  Thomas S. Huang,  Terrence Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fogel_ScrabbleGAN_Semi-Supervised_Varying_Length_Handwritten_Text_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sharon Fogel,  Hadar Averbuch-Elor,  Sarel Cohen,  Shai Mazor,  Roee Litman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cross-Domain Semantic Segmentation via Domain-Invariant Interactive Relation Transfer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lv_Cross-Domain_Semantic_Segmentation_via_Domain-Invariant_Interactive_Relation_Transfer_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fengmao Lv,  Tao Liang,  Xiang Chen,  Guosheng Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1463</td> <td>0.0258</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>Inflated Episodic Memory With Region Self-Attention for Long-Tailed Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Inflated_Episodic_Memory_With_Region_Self-Attention_for_Long-Tailed_Visual_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Linchao Zhu,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0289</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Makansi_Multimodal_Future_Localization_and_Emergence_Prediction_for_Objects_in_Egocentric_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Osama Makansi,  Ozgun Cicek,  Kevin Buchicchio,  Thomas Brox</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Structure Preserving Generative Cross-Domain Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xia_Structure_Preserving_Generative_Cross-Domain_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haifeng Xia,  Zhengming Ding</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2647</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Reverse Perspective Network for Perspective-Aware Object Counting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Reverse_Perspective_Network_for_Perspective-Aware_Object_Counting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yifan Yang,  Guorong Li,  Zhe Wu,  Li Su,  Qingming Huang,  Nicu Sebe</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0215</td></tr>
<tr><td>Multi-Path Region Mining for Weakly Supervised 3D Semantic Segmentation on Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Path_Region_Mining_for_Weakly_Supervised_3D_Semantic_Segmentation_on_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiacheng Wei,  Guosheng Lin,  Kim-Hui Yap,  Tzu-Yi Hung,  Lihua Xie</td> <td>0.0000</td> <td>0.1637</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0250</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Reliable Weighted Optimal Transport for Unsupervised Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Reliable_Weighted_Optimal_Transport_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Renjun Xu,  Pelen Liu,  Liyan Wang,  Chao Chen,  Jindong Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2695</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ImVoteNet: Boosting 3D Object Detection in Point Clouds With Image Votes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_ImVoteNet_Boosting_3D_Object_Detection_in_Point_Clouds_With_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Charles R. Qi,  Xinlei Chen,  Or Litany,  Leonidas J. Guibas</td> <td>0.0667</td> <td>0.1846</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Understanding Road Layout From Videos as a Whole <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Understanding_Road_Layout_From_Videos_as_a_Whole_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Buyu Liu,  Bingbing Zhuang,  Samuel Schulter,  Pan Ji,  Manmohan Chandraker</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0356</td></tr>
<tr><td>Bi-Directional Relationship Inferring Network for Referring Image Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Bi-Directional_Relationship_Inferring_Network_for_Referring_Image_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiwei Hu,  Guang Feng,  Jiayu Sun,  Lihe Zhang,  Huchuan Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1427</td></tr>
<tr><td>Perspective Plane Program Induction From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Perspective_Plane_Program_Induction_From_a_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yikai Li,  Jiayuan Mao,  Xiuming Zhang,  William T. Freeman,  Joshua B. Tenenbaum,  Jiajun Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0190</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DeepFLASH: An Efficient Network for Learning-Based Medical Image Registration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_DeepFLASH_An_Efficient_Network_for_Learning-Based_Medical_Image_Registration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jian Wang,  Miaomiao Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0154</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0269</td> <td>0.0077</td></tr>
<tr><td>Semi-Supervised Learning for Few-Shot Image-to-Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Semi-Supervised_Learning_for_Few-Shot_Image-to-Image_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaxing Wang,  Salman Khan,  Abel Gonzalez-Garcia,  Joost van de Weijer,  Fahad Shahbaz Khan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0642</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Semantic Correspondence as an Optimal Transport Problem <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Semantic_Correspondence_as_an_Optimal_Transport_Problem_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanbin Liu,  Linchao Zhu,  Makoto Yamada,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0152</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0050</td></tr>
<tr><td>How Much Time Do You Have? Modeling Multi-Duration Saliency <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fosco_How_Much_Time_Do_You_Have_Modeling_Multi-Duration_Saliency_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Camilo Fosco,  Anelise Newman,  Pat Sukhum,  Yun Bin Zhang,  Nanxuan Zhao,  Aude Oliva,  Zoya Bylinskii</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fine-Grained Generalized Zero-Shot Learning via Dense Attribute-Based Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huynh_Fine-Grained_Generalized_Zero-Shot_Learning_via_Dense_Attribute-Based_Attention_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dat Huynh,  Ehsan Elhamifar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Online Depth Learning Against Forgetting in Monocular Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenyu Zhang,  Stephane Lathuiliere,  Elisa Ricci,  Nicu Sebe,  Yan Yan,  Jian Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0421</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0137</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0640</td></tr>
<tr><td>Few-Shot Learning of Part-Specific Probability Space for 3D Shape Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Few-Shot_Learning_of_Part-Specific_Probability_Space_for_3D_Shape_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lingjing Wang,  Xiang Li,  Yi Fang</td> <td>0.0000</td> <td>0.0364</td> <td>0.0512</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0206</td> <td>0.0217</td></tr>
<tr><td>Pattern-Structure Diffusion for Multi-Task Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Pattern-Structure_Diffusion_for_Multi-Task_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ling Zhou,  Zhen Cui,  Chunyan Xu,  Zhenyu Zhang,  Chaoqun Wang,  Tong Zhang,  Jian Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0026</td></tr>
<tr><td>Training Noise-Robust Deep Neural Networks via Meta-Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Training_Noise-Robust_Deep_Neural_Networks_via_Meta-Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhen Wang,  Guosheng Hu,  Qinghua Hu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0476</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0405</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0146</td></tr>
<tr><td>Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Fusion-Aware_Point_Convolution_for_Online_Semantic_3D_Scene_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiazhao Zhang,  Chenyang Zhu,  Lintao Zheng,  Kai Xu</td> <td>0.0000</td> <td>0.0850</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Universal Source-Free Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Universal_Source-Free_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jogendra Nath Kundu,  Naveen Venkat,  Rahul M V,  R. Venkatesh Babu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1984</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0082</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_Exploring_Spatial-Temporal_Multi-Frequency_Analysis_for_High-Fidelity_and_Temporal-Consistency_Video_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Beibei Jin,  Yu Hu,  Qiankun Tang,  Jingyu Niu,  Zhiping Shi,  Yinhe Han,  Xiaowei Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0233</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td></tr>
<tr><td>Varicolored Image De-Hazing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dudhane_Varicolored_Image_De-Hazing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Akshay Dudhane,  Kuldeep M. Biradar,  Prashant W. Patil,  Praful Hambarde,  Subrahmanyam Murala</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0117</td></tr>
<tr><td>SpSequenceNet: Semantic Segmentation Network on 4D Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_SpSequenceNet_Semantic_Segmentation_Network_on_4D_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hanyu Shi,  Guosheng Lin,  Hao Wang,  Tzu-Yi Hung,  Zhenhua Wang</td> <td>0.0000</td> <td>0.1857</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0185</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Separating Particulate Matter From a Single Microscopic Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sandhan_Separating_Particulate_Matter_From_a_Single_Microscopic_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tushar Sandhan,  Jin Young Choi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0306</td></tr>
<tr><td>Adaptive Dilated Network With Self-Correction Supervision for Counting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bai_Adaptive_Dilated_Network_With_Self-Correction_Supervision_for_Counting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuai Bai,  Zhiqun He,  Yu Qiao,  Hanzhe Hu,  Wei Wu,  Junjie Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0214</td></tr>
<tr><td>PointPainting: Sequential Fusion for 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Vora_PointPainting_Sequential_Fusion_for_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sourabh Vora,  Alex H. Lang,  Bassam Helou,  Oscar Beijbom</td> <td>0.0000</td> <td>0.0397</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1085</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td></tr>
<tr><td>Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Brattoli_Rethinking_Zero-Shot_Video_Classification_End-to-End_Training_for_Realistic_Applications_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Biagio Brattoli,  Joseph Tighe,  Fedor Zhdanov,  Pietro Perona,  Krzysztof Chalupka</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Select Base Classes for Few-Shot Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Learning_to_Select_Base_Classes_for_Few-Shot_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Linjun Zhou,  Peng Cui,  Xu Jia,  Shiqiang Yang,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0339</td></tr>
<tr><td>CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kluger_CONSAC_Robust_Multi-Model_Fitting_by_Conditional_Sample_Consensus_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Florian Kluger,  Eric Brachmann,  Hanno Ackermann,  Carsten Rother,  Michael Ying Yang,  Bodo Rosenhahn</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fast Symmetric Diffeomorphic Image Registration with Convolutional Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mok_Fast_Symmetric_Diffeomorphic_Image_Registration_with_Convolutional_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tony C.W. Mok,  Albert C.S. Chung</td> <td>0.0000</td> <td>0.0000</td> <td>0.0244</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Distilled Semantics for Comprehensive Scene Understanding from Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tosi_Distilled_Semantics_for_Comprehensive_Scene_Understanding_from_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fabio Tosi,  Filippo Aleotti,  Pierluigi Zama Ramirez,  Matteo Poggi,  Samuele Salti,  Luigi Di Stefano,  Stefano Mattoccia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0381</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0310</td> <td>0.0000</td> <td>0.0000</td> <td>0.1326</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Modeling Biological Immunity to Adversarial Examples <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Modeling_Biological_Immunity_to_Adversarial_Examples_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Edward Kim,  Jocelyn Rego,  Yijing Watkins,  Garrett T. Kenyon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0030</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1257</td> <td>0.0031</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0411</td> <td>0.0000</td></tr>
<tr><td>DOA-GAN: Dual-Order Attentive Generative Adversarial Network for Image Copy-Move Forgery Detection and Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Islam_DOA-GAN_Dual-Order_Attentive_Generative_Adversarial_Network_for_Image_Copy-Move_Forgery_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ashraful Islam,  Chengjiang Long,  Arslan Basharat,  Anthony Hoogs</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1426</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td></tr>
<tr><td>Correspondence-Free Material Reconstruction using Sparse Surface Constraints <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Weiss_Correspondence-Free_Material_Reconstruction_using_Sparse_Surface_Constraints_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sebastian Weiss,  Robert Maier,  Daniel Cremers,  Rudiger Westermann,  Nils Thuerey</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Augmenting Colonoscopy Using Extended and Directional CycleGAN for Lossy Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mathew_Augmenting_Colonoscopy_Using_Extended_and_Directional_CycleGAN_for_Lossy_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shawn Mathew,  Saad Nadeem,  Sruti Kumari,  Arie Kaufman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Attention Scaling for Crowd Counting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Attention_Scaling_for_Crowd_Counting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoheng Jiang,  Li Zhang,  Mingliang Xu,  Tianzhu Zhang,  Pei Lv,  Bing Zhou,  Xin Yang,  Yanwei Pang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0239</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Shape Reconstruction by Learning Differentiable Surface Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bednarik_Shape_Reconstruction_by_Learning_Differentiable_Surface_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jan Bednarik,  Shaifali Parashar,  Erhan Gundogdu,  Mathieu Salzmann,  Pascal Fua</td> <td>0.0000</td> <td>0.0418</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0040</td></tr>
<tr><td>A Spatiotemporal Volumetric Interpolation Network for 4D Dynamic Medical Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_A_Spatiotemporal_Volumetric_Interpolation_Network_for_4D_Dynamic_Medical_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuyu Guo,  Lei Bi,  Euijoon Ahn,  Dagan Feng,  Qian Wang,  Jinman Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0475</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0195</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0123</td></tr>
<tr><td>Attention-Based Context Aware Reasoning for Situation Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cooray_Attention-Based_Context_Aware_Reasoning_for_Situation_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thilini Cooray,  Ngai-Man Cheung,  Wei Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0417</td> <td>0.0000</td> <td>0.0430</td></tr>
<tr><td>PatchVAE: Learning Local Latent Codes for Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gupta_PatchVAE_Learning_Local_Latent_Codes_for_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kamal Gupta,  Saurabh Singh,  Abhinav Shrivastava</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Monocular Trained Depth Estimation Using Self-Attention and Discrete Disparity Volume <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Johnston_Self-Supervised_Monocular_Trained_Depth_Estimation_Using_Self-Attention_and_Discrete_Disparity_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Adrian Johnston,  Gustavo Carneiro</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0559</td> <td>0.0000</td> <td>0.0211</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0211</td> <td>0.0000</td></tr>
<tr><td>STAViS: Spatio-Temporal AudioVisual Saliency Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tsiami_STAViS_Spatio-Temporal_AudioVisual_Saliency_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Antigoni Tsiami,  Petros Koutras,  Petros Maragos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0691</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>More Grounded Image Captioning by Distilling Image-Text Matching Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_More_Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_Model_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuanen Zhou,  Meng Wang,  Daqing Liu,  Zhenzhen Hu,  Hanwang Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0140</td></tr>
<tr><td>DUNIT: Detection-Based Unsupervised Image-to-Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bhattacharjee_DUNIT_Detection-Based_Unsupervised_Image-to-Image_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Deblina Bhattacharjee,  Seungryong Kim,  Guillaume Vizier,  Mathieu Salzmann</td> <td>0.0141</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0179</td> <td>0.0000</td> <td>0.0000</td> <td>0.0556</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Observe: Approximating Human Perceptual Thresholds for Detection of Suprathreshold Image Transformations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dolhasz_Learning_to_Observe_Approximating_Human_Perceptual_Thresholds_for_Detection_of_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alan Dolhasz,  Carlo Harvey,  Ian Williams</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0072</td> <td>0.0391</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Show, Edit and Tell: A Framework for Editing Image Captions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sammani_Show_Edit_and_Tell_A_Framework_for_Editing_Image_Captions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fawaz Sammani,  Luke Melas-Kyriazi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Structure Boundary Preserving Segmentation for Medical Image With Ambiguous Boundary <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Structure_Boundary_Preserving_Segmentation_for_Medical_Image_With_Ambiguous_Boundary_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hong Joo Lee,  Jung Uk Kim,  Sangmin Lee,  Hak Gu Kim,  Yong Man Ro</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0463</td></tr>
<tr><td>Predicting Cognitive Declines Using Longitudinally Enriched Representations for Imaging Biomarkers <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Predicting_Cognitive_Declines_Using_Longitudinally_Enriched_Representations_for_Imaging_Biomarkers_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lyujian Lu,  Hua Wang,  Saad Elbeleidy,  Feiping Nie</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0270</td></tr>
<tr><td>Predicting Lymph Node Metastasis Using Histopathological Images Based on Multiple Instance Learning With Deep Graph Convolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Predicting_Lymph_Node_Metastasis_Using_Histopathological_Images_Based_on_Multiple_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu Zhao,  Fan Yang,  Yuqi Fang,  Hailing Liu,  Niyun Zhou,  Jun Zhang,  Jiarui Sun,  Sen Yang,  Bjoern Menze,  Xinjuan Fan,  Jianhua Yao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0539</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1064</td></tr>
<tr><td>Extremely Dense Point Correspondences Using a Learned Feature Descriptor <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Extremely_Dense_Point_Correspondences_Using_a_Learned_Feature_Descriptor_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xingtong Liu,  Yiping Zheng,  Benjamin Killeen,  Masaru Ishii,  Gregory D. Hager,  Russell H. Taylor,  Mathias Unberath</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0445</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0372</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Local Deep Implicit Functions for 3D Shape <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Genova_Local_Deep_Implicit_Functions_for_3D_Shape_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kyle Genova,  Forrester Cole,  Avneesh Sud,  Aaron Sarna,  Thomas Funkhouser</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_PointGroup_Dual-Set_Point_Grouping_for_3D_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li Jiang,  Hengshuang Zhao,  Shaoshuai Shi,  Shu Liu,  Chi-Wing Fu,  Jiaya Jia</td> <td>0.0000</td> <td>0.0397</td> <td>0.0000</td> <td>0.0000</td> <td>0.1586</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0025</td></tr>
<tr><td>Cost Volume Pyramid Based Depth Inference for Multi-View Stereo <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiayu Yang,  Wei Mao,  Jose M. Alvarez,  Miaomiao Liu</td> <td>0.0000</td> <td>0.0014</td> <td>0.0062</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0374</td> <td>0.0000</td> <td>0.0000</td> <td>0.1481</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0066</td></tr>
<tr><td>RoutedFusion: Learning Real-Time Depth Map Fusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Weder_RoutedFusion_Learning_Real-Time_Depth_Map_Fusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Silvan Weder,  Johannes Schonberger,  Marc Pollefeys,  Martin R. Oswald</td> <td>0.0000</td> <td>0.0000</td> <td>0.0115</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2673</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>VOLDOR: Visual Odometry From Log-Logistic Dense Optical Flow Residuals <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Min_VOLDOR_Visual_Odometry_From_Log-Logistic_Dense_Optical_Flow_Residuals_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhixiang Min,  Yiding Yang,  Enrique Dunn</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3657</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Optimize Non-Rigid Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Learning_to_Optimize_Non-Rigid_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Li,  Aljaz Bozic,  Tianwei Zhang,  Yanli Ji,  Tatsuya Harada,  Matthias Niessner</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0095</td></tr>
<tr><td>KFNet: Learning Temporal Camera Relocalization Using Kalman Filtering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_KFNet_Learning_Temporal_Camera_Relocalization_Using_Kalman_Filtering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Zhou,  Zixin Luo,  Tianwei Shen,  Jiahui Zhang,  Mingmin Zhen,  Yao Yao,  Tian Fang,  Long Quan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0012</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Information-Driven Direct RGB-D Odometry <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fontan_Information-Driven_Direct_RGB-D_Odometry_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alejandro Fontan,  Javier Civera,  Rudolph Triebel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0134</td></tr>
<tr><td>SuperGlue: Learning Feature Matching With Graph Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sarlin_SuperGlue_Learning_Feature_Matching_With_Graph_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Paul-Edouard Sarlin,  Daniel DeTone,  Tomasz Malisiewicz,  Andrew Rabinovich</td> <td>0.0000</td> <td>0.0000</td> <td>0.0287</td> <td>0.0000</td> <td>0.0000</td> <td>0.0297</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0241</td></tr>
<tr><td>Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bhowmik_Reinforced_Feature_Points_Optimizing_Feature_Detection_and_Description_for_a_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aritra Bhowmik,  Stefan Gumhold,  Carsten Rother,  Eric Brachmann</td> <td>0.0000</td> <td>0.0000</td> <td>0.0062</td> <td>0.0000</td> <td>0.0000</td> <td>0.0112</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0286</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ReDA:Reinforced Differentiable Attribute for 3D Face Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_ReDAReinforced_Differentiable_Attribute_for_3D_Face_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenbin Zhu,  HsiangTao Wu,  Zeyu Chen,  Noranart Vesdapunt,  Baoyuan Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.6066</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td></tr>
<tr><td>EventCap: Monocular 3D Capture of High-Speed Human Motions Using an Event Camera <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_EventCap_Monocular_3D_Capture_of_High-Speed_Human_Motions_Using_an_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lan Xu,  Weipeng Xu,  Vladislav Golyanik,  Marc Habermann,  Lu Fang,  Christian Theobalt</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0418</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td></tr>
<tr><td>Cross-Modal Deep Face Normals With Deactivable Skip Connections <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abrevaya_Cross-Modal_Deep_Face_Normals_With_Deactivable_Skip_Connections_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Victoria Fernandez Abrevaya,  Adnane Boukhayma,  Philip H.S. Torr,  Edmond Boyer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0027</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0023</td></tr>
<tr><td>Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kulon_Weakly-Supervised_Mesh-Convolutional_Hand_Reconstruction_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dominik Kulon,  Riza Alp Guler,  Iasonas Kokkinos,  Michael M. Bronstein,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0381</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0155</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0300</td></tr>
<tr><td>Face X-Ray for More General Face Forgery Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Face_X-Ray_for_More_General_Face_Forgery_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lingzhi Li,  Jianmin Bao,  Ting Zhang,  Hao Yang,  Dong Chen,  Fang Wen,  Baining Guo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0109</td></tr>
<tr><td>A Morphable Face Albedo Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Smith_A_Morphable_Face_Albedo_Model_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>William A. P. Smith,  Alassane Seck,  Hannah Dee,  Bernard Tiddeman,  Joshua B. Tenenbaum,  Bernhard Egger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1575</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td></tr>
<tr><td>Cascade EF-GAN: Progressive Facial Expression Editing With Local Focuses <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Cascade_EF-GAN_Progressive_Facial_Expression_Editing_With_Local_Focuses_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rongliang Wu,  Gongjie Zhang,  Shijian Lu,  Tao Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0287</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Corona_GanHand_Predicting_Human_Grasp_Affordances_in_Multi-Object_Scenes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Enric Corona,  Albert Pumarola,  Guillem Alenya,  Francesc Moreno-Noguer,  Gregory Rogez</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0105</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Spatial Gradient and Temporal Depth Learning for Face Anti-Spoofing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Deep_Spatial_Gradient_and_Temporal_Depth_Learning_for_Face_Anti-Spoofing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zezheng Wang,  Zitong Yu,  Chenxu Zhao,  Xiangyu Zhu,  Yunxiao Qin,  Qiusheng Zhou,  Feng Zhou,  Zhen Lei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0531</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0396</td></tr>
<tr><td>DeepCap: Monocular Human Performance Capture Using Weak Supervision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Habermann_DeepCap_Monocular_Human_Performance_Capture_Using_Weak_Supervision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Marc Habermann,  Weipeng Xu,  Michael Zollhofer,  Gerard Pons-Moll,  Christian Theobalt</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0202</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0342</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0286</td> <td>0.0000</td></tr>
<tr><td>Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Attention_Mechanism_Exploits_Temporal_Contexts_Real-Time_3D_Human_Pose_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruixu Liu,  Ju Shen,  He Wang,  Chen Chen,  Sen-ching Cheung,  Vijayan Asari</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td> <td>0.0000</td> <td>0.0000</td> <td>0.1841</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0544</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0157</td></tr>
<tr><td>Advancing High Fidelity Identity Swapping for Forgery Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Advancing_High_Fidelity_Identity_Swapping_for_Forgery_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lingzhi Li,  Jianmin Bao,  Hao Yang,  Dong Chen,  Fang Wen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Controllable Person Image Synthesis With Attribute-Decomposed GAN <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Men_Controllable_Person_Image_Synthesis_With_Attribute-Decomposed_GAN_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yifang Men,  Yiming Mao,  Yuning Jiang,  Wei-Ying Ma,  Zhouhui Lian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0228</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1076</td></tr>
<tr><td>Attentive Normalization for Conditional Image Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Attentive_Normalization_for_Conditional_Image_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yi Wang,  Ying-Cong Chen,  Xiangyu Zhang,  Jian Sun,  Jiaya Jia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0032</td> <td>0.0000</td> <td>0.1612</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SEAN: Image Synthesis With Semantic Region-Adaptive Normalization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_SEAN_Image_Synthesis_With_Semantic_Region-Adaptive_Normalization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peihao Zhu,  Rameen Abdal,  Yipeng Qin,  Peter Wonka</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1207</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Blurry Video Frame Interpolation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Blurry_Video_Frame_Interpolation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wang Shen,  Wenbo Bao,  Guangtao Zhai,  Li Chen,  Xiongkuo Min,  Zhiyong Gao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0160</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0443</td></tr>
<tr><td>Learning Physics-Guided Face Relighting Under Directional Light <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nestmeyer_Learning_Physics-Guided_Face_Relighting_Under_Directional_Light_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thomas Nestmeyer,  Jean-Francois Lalonde,  Iain Matthews,  Andreas Lehrmann</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Disentangled Image Generation Through Structured Noise Injection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Alharbi_Disentangled_Image_Generation_Through_Structured_Noise_Injection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yazeed Alharbi,  Peter Wonka</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2293</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cross-Domain Correspondence Learning for Exemplar-Based Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Cross-Domain_Correspondence_Learning_for_Exemplar-Based_Image_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pan Zhang,  Bo Zhang,  Dong Chen,  Lu Yuan,  Fang Wen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0371</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0052</td></tr>
<tr><td>Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu Deng,  Jiaolong Yang,  Dong Chen,  Fang Wen,  Xin Tong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0490</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1133</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Single Image Reflection Removal With Physically-Based Training Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Single_Image_Reflection_Removal_With_Physically-Based_Training_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Soomin Kim,  Yuchi Huo,  Sung-Eui Yoon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1043</td> <td>0.0270</td></tr>
<tr><td>SketchyCOCO: Image Generation From Freehand Scene Sketches <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_SketchyCOCO_Image_Generation_From_Freehand_Scene_Sketches_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chengying Gao,  Qi Liu,  Qi Xu,  Limin Wang,  Jianzhuang Liu,  Changqing Zou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0244</td> <td>0.0000</td></tr>
<tr><td>Image Based Virtual Try-On Network From Unpaired Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Neuberger_Image_Based_Virtual_Try-On_Network_From_Unpaired_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Assaf Neuberger,  Eran Borenstein,  Bar Hilleli,  Eduard Oks,  Sharon Alpert</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0019</td></tr>
<tr><td>PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_PSGAN_Pose_and_Expression_Robust_Spatial-Aware_GAN_for_Customizable_Makeup_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wentao Jiang,  Si Liu,  Chen Gao,  Jie Cao,  Ran He,  Jiashi Feng,  Shuicheng Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0068</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiankang Deng,  Jia Guo,  Evangelos Ververas,  Irene Kotsia,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.7099</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Semantic Image Manipulation Using Scene Graphs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dhamo_Semantic_Image_Manipulation_Using_Scene_Graphs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Helisa Dhamo,  Azade Farshad,  Iro Laina,  Nassir Navab,  Gregory D. Hager,  Federico Tombari,  Christian Rupprecht</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0113</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0319</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Stochastic Conditioning Scheme for Diverse Human Motion Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Aliakbarian_A_Stochastic_Conditioning_Scheme_for_Diverse_Human_Motion_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sadegh Aliakbarian,  Fatemeh Sadat Saleh,  Mathieu Salzmann,  Lars Petersson,  Stephen Gould</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0610</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Transferring Dense Pose to Proximal Animal Classes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sanakoyeu_Transferring_Dense_Pose_to_Proximal_Animal_Classes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Artsiom Sanakoyeu,  Vasil Khalidov,  Maureen S. McCarthy,  Andrea Vedaldi,  Natalia Neverova</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Iqbal_Weakly-Supervised_3D_Human_Pose_Learning_via_Multi-View_Images_in_the_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Umar Iqbal,  Pavlo Molchanov,  Jan Kautz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1637</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0286</td> <td>0.0102</td></tr>
<tr><td>VIBE: Video Inference for Human Body Pose and Shape Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kocabas_VIBE_Video_Inference_for_Human_Body_Pose_and_Shape_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muhammed Kocabas,  Nikos Athanasiou,  Michael J. Black</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0549</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0067</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0072</td></tr>
<tr><td>G3AN: Disentangling Appearance and Motion for Video Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_G3AN_Disentangling_Appearance_and_Motion_for_Video_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaohui Wang,  Piotr Bilinski,  Francois Bremond,  Antitza Dantcheva</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0200</td></tr>
<tr><td>Domain Adaptive Image-to-Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Domain_Adaptive_Image-to-Image_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ying-Cong Chen,  Xiaogang Xu,  Jiaya Jia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GAN Compression: Efficient Architectures for Interactive Conditional GANs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_GAN_Compression_Efficient_Architectures_for_Interactive_Conditional_GANs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muyang Li,  Ji Lin,  Yaoyao Ding,  Zhijian Liu,  Jun-Yan Zhu,  Song Han</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1649</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1263</td> <td>0.0000</td> <td>0.0000</td> <td>0.0254</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Searching Central Difference Convolutional Networks for Face Anti-Spoofing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Searching_Central_Difference_Convolutional_Networks_for_Face_Anti-Spoofing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zitong Yu,  Chenxu Zhao,  Zezheng Wang,  Yunxiao Qin,  Zhuo Su,  Xiaobai Li,  Feng Zhou,  Guoying Zhao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1932</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0559</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0476</td></tr>
<tr><td>TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_TransMoMo_Invariance-Driven_Unsupervised_Video_Motion_Retargeting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhuoqian Yang,  Wentao Zhu,  Wayne Wu,  Chen Qian,  Qiang Zhou,  Bolei Zhou,  Chen Change Loy</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0020</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0319</td></tr>
<tr><td>AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_AdaCoF_Adaptive_Collaboration_of_Flows_for_Video_Frame_Interpolation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hyeongmin Lee,  Taeoh Kim,  Tae-young Chung,  Daehyun Pak,  Yuseok Ban,  Sangyoun Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0582</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0540</td></tr>
<tr><td>FReeNet: Multi-Identity Face Reenactment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_FReeNet_Multi-Identity_Face_Reenactment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiangning Zhang,  Xianfang Zeng,  Mengmeng Wang,  Yusu Pan,  Liang Liu,  Yong Liu,  Yu Ding,  Changjie Fan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Novel View Synthesis of Dynamic Scenes With Globally Coherent Depths From a Monocular Camera <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yoon_Novel_View_Synthesis_of_Dynamic_Scenes_With_Globally_Coherent_Depths_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jae Shin Yoon,  Kihwan Kim,  Orazio Gallo,  Hyun Soo Park,  Jan Kautz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1391</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Monocular Real-Time Hand Shape and Motion Capture Using Multi-Modal Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Monocular_Real-Time_Hand_Shape_and_Motion_Capture_Using_Multi-Modal_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuxiao Zhou,  Marc Habermann,  Weipeng Xu,  Ikhsanul Habibie,  Christian Theobalt,  Feng Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0263</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0268</td> <td>0.0000</td></tr>
<tr><td>The GAN That Warped: Semantic Attribute Editing With Unpaired Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dorta_The_GAN_That_Warped_Semantic_Attribute_Editing_With_Unpaired_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Garoe Dorta,  Sara Vicente,  Neill D. F. Campbell,  Ivor J. A. Simpson</td> <td>0.0000</td> <td>0.0000</td> <td>0.0407</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1395</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>4D Visualization of Dynamic Events From Unconstrained Multi-View Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bansal_4D_Visualization_of_Dynamic_Events_From_Unconstrained_Multi-View_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aayush Bansal,  Minh Vo,  Yaser Sheikh,  Deva Ramanan,  Srinivasa Narasimhan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_Global-Local_Bidirectional_Reasoning_for_Unsupervised_Representation_Learning_of_3D_Point_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yongming Rao,  Jiwen Lu,  Jie Zhou</td> <td>0.0486</td> <td>0.1011</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0075</td></tr>
<tr><td>HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_HigherHRNet_Scale-Aware_Representation_Learning_for_Bottom-Up_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bowen Cheng,  Bin Xiao,  Jingdong Wang,  Honghui Shi,  Thomas S. Huang,  Lei Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1918</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Detecting Attended Visual Targets in Video <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eunji Chong,  Yongxin Wang,  Nataniel Ruiz,  James M. Rehg</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td></tr>
<tr><td>Closed-Loop Matters: Dual Regression Networks for Single Image Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Closed-Loop_Matters_Dual_Regression_Networks_for_Single_Image_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yong Guo,  Jian Chen,  Jingdong Wang,  Qi Chen,  Jiezhang Cao,  Zeshuai Deng,  Yanwu Xu,  Mingkui Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0196</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0175</td> <td>0.0000</td></tr>
<tr><td>Neural Voxel Renderer: Learning an Accurate and Controllable Rendering Tool <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rematas_Neural_Voxel_Renderer_Learning_an_Accurate_and_Controllable_Rendering_Tool_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Konstantinos Rematas,  Vittorio Ferrari</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0125</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Neural Contours: Learning to Draw Lines From 3D Shapes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Neural_Contours_Learning_to_Draw_Lines_From_3D_Shapes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Difan Liu,  Mohamed Nabail,  Aaron Hertzmann,  Evangelos Kalogerakis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Softmax Splatting for Video Frame Interpolation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Niklaus_Softmax_Splatting_for_Video_Frame_Interpolation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Simon Niklaus,  Feng Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1443</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Maximov_CIAGAN_Conditional_Identity_Anonymization_Generative_Adversarial_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maxim Maximov,  Ismail Elezi,  Laura Leal-Taixe</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1299</td> <td>0.0000</td> <td>0.0000</td> <td>0.1259</td> <td>0.0000</td> <td>0.0000</td> <td>0.0804</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Probabilistic Structural Latent Representation for Unsupervised Embedding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Probabilistic_Structural_Latent_Representation_for_Unsupervised_Embedding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mang Ye,  Jianbing Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0611</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Semantically Multi-Modal Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Semantically_Multi-Modal_Image_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhen Zhu,  Zhiliang Xu,  Ansheng You,  Xiang Bai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0183</td> <td>0.0000</td> <td>0.0116</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0037</td></tr>
<tr><td>Nested Scale-Editing for Conditional Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Nested_Scale-Editing_for_Conditional_Image_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lingzhi Zhang,  Jiancong Wang,  Yinshuang Xu,  Jie Min,  Tarmily Wen,  James C. Gee,  Jianbo Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0363</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>UnrealText: Synthesizing Realistic Scene Text Images From the Unreal World <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Long_UnrealText_Synthesizing_Realistic_Scene_Text_Images_From_the_Unreal_World_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shangbang Long,  Cong Yao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fast Texture Synthesis via Pseudo Optimizer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Fast_Texture_Synthesis_via_Pseudo_Optimizer_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wu Shi,  Yu Qiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0367</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td></tr>
<tr><td>Towards Learning Structure via Consensus for Face Segmentation and Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Masi_Towards_Learning_Structure_via_Consensus_for_Face_Segmentation_and_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Iacopo Masi,  Joe Mathai,  Wael AbdAlmageed</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0127</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0142</td></tr>
<tr><td>CookGAN: Causality Based Text-to-Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_CookGAN_Causality_Based_Text-to-Image_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bin Zhu,  Chong-Wah Ngo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0086</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly Supervised Discriminative Feature Learning With State Information for Person Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Weakly_Supervised_Discriminative_Feature_Learning_With_State_Information_for_Person_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hong-Xing Yu,  Wei-Shi Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0409</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Future Video Synthesis With Object Motion Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Future_Video_Synthesis_With_Object_Motion_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Wu,  Rongrong Gao,  Jaesik Park,  Qifeng Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0225</td></tr>
<tr><td>MaskGAN: Towards Diverse and Interactive Facial Image Manipulation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_MaskGAN_Towards_Diverse_and_Interactive_Facial_Image_Manipulation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cheng-Han Lee,  Ziwei Liu,  Lingyun Wu,  Ping Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0124</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0124</td></tr>
<tr><td>A Graduated Filter Method for Large Scale Robust Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Le_A_Graduated_Filter_Method_for_Large_Scale_Robust_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huu Le,  Christopher Zach</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0692</td></tr>
<tr><td>Deep Face Super-Resolution With Iterative Collaboration Between Attentive Recovery and Landmark Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ma_Deep_Face_Super-Resolution_With_Iterative_Collaboration_Between_Attentive_Recovery_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cheng Ma,  Zhenyu Jiang,  Yongming Rao,  Jiwen Lu,  Jie Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0817</td></tr>
<tr><td>Coherent Reconstruction of Multiple Humans From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Coherent_Reconstruction_of_Multiple_Humans_From_a_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wen Jiang,  Nikos Kolotouros,  Georgios Pavlakos,  Xiaowei Zhou,  Kostas Daniilidis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0507</td> <td>0.0626</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PointASNL: Robust Point Clouds Processing Using Nonlocal Neural Networks With Adaptive Sampling <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_PointASNL_Robust_Point_Clouds_Processing_Using_Nonlocal_Neural_Networks_With_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xu Yan,  Chaoda Zheng,  Zhen Li,  Sheng Wang,  Shuguang Cui</td> <td>0.0000</td> <td>0.1624</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Neural Rendering Framework for Free-Viewpoint Relighting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_A_Neural_Rendering_Framework_for_Free-Viewpoint_Relighting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhang Chen,  Anpei Chen,  Guli Zhang,  Chengyuan Wang,  Yu Ji,  Kiriakos N. Kutulakos,  Jingyi Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Multi-Task Mean Teacher for Semi-Supervised Shadow Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_A_Multi-Task_Mean_Teacher_for_Semi-Supervised_Shadow_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihao Chen,  Lei Zhu,  Liang Wan,  Song Wang,  Wei Feng,  Pheng-Ann Heng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0021</td></tr>
<tr><td>GroupFace: Learning Latent Groups and Constructing Group-Based Representations for Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_GroupFace_Learning_Latent_Groups_and_Constructing_Group-Based_Representations_for_Face_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yonghyun Kim,  Wonpyo Park,  Myung-Cheol Roh,  Jongju Shin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0161</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1307</td></tr>
<tr><td>Channel Attention Based Iterative Residual Learning for Depth Map Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Song_Channel_Attention_Based_Iterative_Residual_Learning_for_Depth_Map_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xibin Song,  Yuchao Dai,  Dingfu Zhou,  Liu Liu,  Wei Li,  Hongdong Li,  Ruigang Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3143</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Time Flies: Animating a Still Image With Time-Lapse Video As Reference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Time_Flies_Animating_a_Still_Image_With_Time-Lapse_Video_As_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chia-Chi Cheng,  Hung-Yu Chen,  Wei-Chen Chiu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0504</td></tr>
<tr><td>SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Terhorst_SER-FIQ_Unsupervised_Estimation_of_Face_Image_Quality_Based_on_Stochastic_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Philipp Terhorst,  Jan Niklas Kolf,  Naser Damer,  Florian Kirchbuchner,  Arjan Kuijper</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td></tr>
<tr><td>Grid-GCN for Fast and Scalable Point Cloud Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Grid-GCN_for_Fast_and_Scalable_Point_Cloud_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiangeng Xu,  Xudong Sun,  Cho-Ying Wu,  Panqu Wang,  Ulrich Neumann</td> <td>0.0000</td> <td>0.1043</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Domain Balancing: Face Recognition on Long-Tailed Domains <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_Domain_Balancing_Face_Recognition_on_Long-Tailed_Domains_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dong Cao,  Xiangyu Zhu,  Xingyu Huang,  Jianzhu Guo,  Zhen Lei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0710</td></tr>
<tr><td>AdversarialNAS: Adversarial Neural Architecture Search for GANs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_AdversarialNAS_Adversarial_Neural_Architecture_Search_for_GANs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chen Gao,  Yunpeng Chen,  Si Liu,  Zhenxiong Tan,  Shuicheng Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2143</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1437</td> <td>0.0000</td> <td>0.0000</td> <td>0.0232</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td></tr>
<tr><td>Image Super-Resolution With Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mei_Image_Super-Resolution_With_Cross-Scale_Non-Local_Attention_and_Exhaustive_Self-Exemplars_Mining_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiqun Mei,  Yuchen Fan,  Yuqian Zhou,  Lichao Huang,  Thomas S. Huang,  Honghui Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0104</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0216</td></tr>
<tr><td>The Devil Is in the Details: Delving Into Unbiased Data Processing for Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junjie Huang,  Zheng Zhu,  Feng Guo,  Guan Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2688</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Data Uncertainty Learning in Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chang_Data_Uncertainty_Learning_in_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Chang,  Zhonghao Lan,  Changmao Cheng,  Yichen Wei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Regularizing Discriminative Capability of CGANs for Semi-Supervised Generative Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Regularizing_Discriminative_Capability_of_CGANs_for_Semi-Supervised_Generative_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yi Liu,  Guangchang Deng,  Xiangping Zeng,  Si Wu,  Zhiwen Yu,  Hau-San Wong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2058</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FM2u-Net: Face Morphological Multi-Branch Network for Makeup-Invariant Face Verification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_FM2u-Net_Face_Morphological_Multi-Branch_Network_for_Makeup-Invariant_Face_Verification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenxuan Wang,  Yanwei Fu,  Xuelin Qian,  Yu-Gang Jiang,  Qi Tian,  Xiangyang Xue</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0024</td></tr>
<tr><td>UCTGAN: Diverse Image Inpainting Based on Unsupervised Cross-Space Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_UCTGAN_Diverse_Image_Inpainting_Based_on_Unsupervised_Cross-Space_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Zhao,  Qihang Mo,  Sihuan Lin,  Zhizhong Wang,  Zhiwen Zuo,  Haibo Chen,  Wei Xing,  Dongming Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1217</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Decoupled Representation Learning for Skeleton-Based Gesture Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Decoupled_Representation_Learning_for_Skeleton-Based_Gesture_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianbo Liu,  Yongcheng Liu,  Ying Wang,  Veronique Prinet,  Shiming Xiang,  Chunhong Pan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0171</td></tr>
<tr><td>An Efficient PointLSTM for Point Clouds Based Gesture Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Min_An_Efficient_PointLSTM_for_Point_Clouds_Based_Gesture_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuecong Min,  Yanxiao Zhang,  Xiujuan Chai,  Xilin Chen</td> <td>0.0000</td> <td>0.0739</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Editing in Style: Uncovering the Local Semantics of GANs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Collins_Editing_in_Style_Uncovering_the_Local_Semantics_of_GANs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Edo Collins,  Raja Bala,  Bob Price,  Sabine Susstrunk</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0191</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On the Detection of Digital Face Manipulation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dang_On_the_Detection_of_Digital_Face_Manipulation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hao Dang,  Feng Liu,  Joel Stehouwer,  Xiaoming Liu,  Anil K. Jain</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Texture Transformer Network for Image Super-Resolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fuzhi Yang,  Huan Yang,  Jianlong Fu,  Hongtao Lu,  Baining Guo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0150</td></tr>
<tr><td>Reference-Based Sketch Image Colorization Using Augmented-Self Reference and Dense Semantic Correspondence <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Reference-Based_Sketch_Image_Colorization_Using_Augmented-Self_Reference_and_Dense_Semantic_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junsoo Lee,  Eungyeup Kim,  Yunsung Lee,  Dongjun Kim,  Jaehyuk Chang,  Jaegul Choo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0228</td> <td>0.0000</td></tr>
<tr><td>Deblurring Using Analysis-Synthesis Networks Pair <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kaufman_Deblurring_Using_Analysis-Synthesis_Networks_Pair_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Adam Kaufman,  Raanan Fattal</td> <td>0.0000</td> <td>0.0000</td> <td>0.0116</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Exploring Unlabeled Faces for Novel Attribute Discovery <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bahng_Exploring_Unlabeled_Faces_for_Novel_Attribute_Discovery_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hyojin Bahng,  Sunghyo Chung,  Seungjoo Yoo,  Jaegul Choo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0065</td></tr>
<tr><td>Neural Pose Transfer by Spatially Adaptive Instance Normalization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Neural_Pose_Transfer_by_Spatially_Adaptive_Instance_Normalization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiashun Wang,  Chao Wen,  Yanwei Fu,  Haitao Lin,  Tianyun Zou,  Xiangyang Xue,  Yinda Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td> <td>0.0000</td> <td>0.0220</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fine-Grained Image-to-Image Transformation Towards Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiong_Fine-Grained_Image-to-Image_Transformation_Towards_Visual_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wei Xiong,  Yutong He,  Yixuan Zhang,  Wenhan Luo,  Lin Ma,  Jiebo Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1260</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0104</td></tr>
<tr><td>Deep Facial Non-Rigid Multi-View Stereo <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bai_Deep_Facial_Non-Rigid_Multi-View_Stereo_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziqian Bai,  Zhaopeng Cui,  Jamal Ahmed Rahim,  Xiaoming Liu,  Ping Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.5623</td> <td>0.0000</td> <td>0.0000</td> <td>0.0099</td></tr>
<tr><td>Attention-Driven Cropping for Very High Resolution Facial Landmark Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chandran_Attention-Driven_Cropping_for_Very_High_Resolution_Facial_Landmark_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Prashanth Chandran,  Derek Bradley,  Markus Gross,  Thabo Beeler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_Towards_Unsupervised_Learning_of_Generative_Models_for_3D_Controllable_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiyi Liao,  Katja Schwarz,  Lars Mescheder,  Andreas Geiger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1424</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qian_End-to-End_Pseudo-LiDAR_for_Image-Based_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Qian,  Divyansh Garg,  Yan Wang,  Yurong You,  Serge Belongie,  Bharath Hariharan,  Mark Campbell,  Kilian Q. Weinberger,  Wei-Lun Chao</td> <td>0.2566</td> <td>0.1038</td> <td>0.0307</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0472</td> <td>0.0000</td> <td>0.0000</td> <td>0.0825</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards High-Fidelity 3D Face Reconstruction From In-the-Wild Images Using Graph Convolutional Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Towards_High-Fidelity_3D_Face_Reconstruction_From_In-the-Wild_Images_Using_Graph_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiangke Lin,  Yi Yuan,  Tianjia Shao,  Kun Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1363</td> <td>0.0000</td> <td>0.0000</td> <td>0.0234</td></tr>
<tr><td>CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_CurricularFace_Adaptive_Curriculum_Learning_Loss_for_Deep_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuge Huang,  Yuhan Wang,  Ying Tai,  Xiaoming Liu,  Pengcheng Shen,  Shaoxin Li,  Jilin Li,  Feiyue Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0157</td></tr>
<tr><td>Rotate-and-Render: Unsupervised Photorealistic Face Rotation From Single-View Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Rotate-and-Render_Unsupervised_Photorealistic_Face_Rotation_From_Single-View_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hang Zhou,  Jihao Liu,  Ziwei Liu,  Yu Liu,  Xiaogang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0810</td> <td>0.0000</td> <td>0.0292</td> <td>0.0314</td></tr>
<tr><td>One-Shot Domain Adaptation for Face Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_One-Shot_Domain_Adaptation_for_Face_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chao Yang,  Ser-Nam Lim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0980</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0350</td> <td>0.0000</td></tr>
<tr><td>BidNet: Binocular Image Dehazing Without Explicit Disparity Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_BidNet_Binocular_Image_Dehazing_Without_Explicit_Disparity_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanwei Pang,  Jing Nie,  Jin Xie,  Jungong Han,  Xuelong Li</td> <td>0.0761</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0032</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0218</td></tr>
<tr><td>Deep Shutter Unrolling Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Deep_Shutter_Unrolling_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peidong Liu,  Zhaopeng Cui,  Viktor Larsson,  Marc Pollefeys</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0157</td></tr>
<tr><td>Joint Texture and Geometry Optimization for RGB-D Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_Joint_Texture_and_Geometry_Optimization_for_RGB-D_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanping Fu,  Qingan Yan,  Jie Liao,  Chunxia Xiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0267</td></tr>
<tr><td>Deep 3D Capture: Geometry and Reflectance From Sparse Multi-View Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bi_Deep_3D_Capture_Geometry_and_Reflectance_From_Sparse_Multi-View_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sai Bi,  Zexiang Xu,  Kalyan Sunkavalli,  David Kriegman,  Ravi Ramamoorthi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1518</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0163</td></tr>
<tr><td>Auto-Tuning Structured Light by Optical Stochastic Gradient Descent <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Auto-Tuning_Structured_Light_by_Optical_Stochastic_Gradient_Descent_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenzheng Chen,  Parsa Mirdehghan,  Sanja Fidler,  Kiriakos N. Kutulakos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0552</td> <td>0.0000</td></tr>
<tr><td>MARMVS: Matching Ambiguity Reduced Multiple View Stereo for Efficient Large Scale Scene Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_MARMVS_Matching_Ambiguity_Reduced_Multiple_View_Stereo_for_Efficient_Large_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenyu Xu,  Yiguang Liu,  Xuelei Shi,  Ying Wang,  Yunan Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0686</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0495</td></tr>
<tr><td>Uncertainty Based Camera Model Selection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Polic_Uncertainty_Based_Camera_Model_Selection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michal Polic,  Stanislav Steidl,  Cenek Albl,  Zuzana Kukelova,  Tomas Pajdla</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Local Implicit Grid Representations for 3D Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Local_Implicit_Grid_Representations_for_3D_Scenes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chiyu "Max" Jiang,  Avneesh Sud,  Ameesh Makadia,  Jingwei Huang,  Matthias Niessner,  Thomas Funkhouser</td> <td>0.0209</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>TetraTSDF: 3D Human Reconstruction From a Single Image With a Tetrahedral Outer Shell <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Onizuka_TetraTSDF_3D_Human_Reconstruction_From_a_Single_Image_With_a_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hayato Onizuka,  Zehra Hayirci,  Diego Thomas,  Akihiro Sugimoto,  Hideaki Uchiyama,  Rin-ichiro Taniguchi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0281</td> <td>0.0000</td> <td>0.0000</td> <td>0.0357</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0247</td></tr>
<tr><td>Averaging Essential and Fundamental Matrices in Collinear Camera Settings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Geifman_Averaging_Essential_and_Fundamental_Matrices_in_Collinear_Camera_Settings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Amnon Geifman,  Yoni Kasten,  Meirav Galun,  Ronen Basri</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On the Distribution of Minima in Intrinsic-Metric Rotation Averaging <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wilson_On_the_Distribution_of_Minima_in_Intrinsic-Metric_Rotation_Averaging_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kyle Wilson,  David Bindel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Remelli_Lightweight_Multi-View_3D_Pose_Estimation_Through_Camera-Disentangled_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Edoardo Remelli,  Shangchen Han,  Sina Honari,  Pascal Fua,  Robert Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0787</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0153</td></tr>
<tr><td>A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-View Stereo Reconstruction From an Open Aerial Dataset <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_A_Novel_Recurrent_Encoder-Decoder_Structure_for_Large-Scale_Multi-View_Stereo_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jin Liu,  Shunping Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0351</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Factorized Higher-Order CNNs With an Application to Spatio-Temporal Emotion Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kossaifi_Factorized_Higher-Order_CNNs_With_an_Application_to_Spatio-Temporal_Emotion_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jean Kossaifi,  Antoine Toisoul,  Adrian Bulat,  Yannis Panagakis,  Timothy M. Hospedales,  Maja Pantic</td> <td>0.0000</td> <td>0.0000</td> <td>0.0367</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Effectively Unbiased FID and Inception Score and Where to Find Them <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Effectively_Unbiased_FID_and_Inception_Score_and_Where_to_Find_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Min Jin Chong,  David Forsyth</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Robust Homography Estimation via Dual Principal Component Pursuit <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ding_Robust_Homography_Estimation_via_Dual_Principal_Component_Pursuit_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianjiao Ding,  Yunchen Yang,  Zhihui Zhu,  Daniel P. Robinson,  Rene Vidal,  Laurent Kneip,  Manolis C. Tsakiris</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Non-Adversarial Video Synthesis With Learned Priors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Aich_Non-Adversarial_Video_Synthesis_With_Learned_Priors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abhishek Aich,  Akash Gupta,  Rameswar Panda,  Rakib Hyder,  M. Salman Asif,  Amit K. Roy-Chowdhury</td> <td>0.0000</td> <td>0.0000</td> <td>0.0093</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0226</td></tr>
<tr><td>Uncertainty-Aware Mesh Decoder for High Fidelity 3D Face Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Uncertainty-Aware_Mesh_Decoder_for_High_Fidelity_3D_Face_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gun-Hee Lee,  Seong-Whan Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0523</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0798</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.4480</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td></tr>
<tr><td>3FabRec: Fast Few-Shot Face Alignment by Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Browatzki_3FabRec_Fast_Few-Shot_Face_Alignment_by_Reconstruction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bjorn Browatzki,  Christian Wallraven</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0457</td> <td>0.0022</td></tr>
<tr><td>Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Baek_Weakly-Supervised_Domain_Adaptation_via_GAN_and_Mesh_Model_for_Estimating_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Seungryul Baek,  Kwang In Kim,  Tae-Kyun Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0628</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0923</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0178</td></tr>
<tr><td>Vec2Face: Unveil Human Faces From Their Blackbox Features in Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Duong_Vec2Face_Unveil_Human_Faces_From_Their_Blackbox_Features_in_Face_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chi Nhan Duong,  Thanh-Dat Truong,  Khoa Luu,  Kha Gia Quach,  Hung Bui,  Kaushik Roy</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1508</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tewari_StyleRig_Rigging_StyleGAN_for_3D_Control_Over_Portrait_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ayush Tewari,  Mohamed Elgharib,  Gaurav Bharaj,  Florian Bernard,  Hans-Peter Seidel,  Patrick Perez,  Michael Zollhofer,  Christian Theobalt</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1288</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Self-Supervised_3D_Human_Pose_Estimation_via_Part_Guided_Novel_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jogendra Nath Kundu,  Siddharth Seth,  Varun Jampani,  Mugalodi Rakesh,  R. Venkatesh Babu,  Anirban Chakraborty</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2231</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Meta Face Recognition in Unseen Domains <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Learning_Meta_Face_Recognition_in_Unseen_Domains_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianzhu Guo,  Xiangyu Zhu,  Chenxu Zhao,  Dong Cao,  Zhen Lei,  Stan Z. Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0573</td> <td>0.0000</td> <td>0.0526</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0023</td></tr>
<tr><td>Cascaded Deep Monocular 3D Human Pose Estimation With Evolutionary Training Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Cascaded_Deep_Monocular_3D_Human_Pose_Estimation_With_Evolutionary_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shichao Li,  Lei Ke,  Kevin Pratama,  Yu-Wing Tai,  Chi-Keung Tang,  Kwang-Ting Cheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2101</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0582</td> <td>0.0011</td></tr>
<tr><td>GHUM & GHUML: Generative 3D Human Shape and Articulated Pose Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_GHUM__GHUML_Generative_3D_Human_Shape_and_Articulated_Pose_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hongyi Xu,  Eduard Gabriel Bazavan,  Andrei Zanfir,  William T. Freeman,  Rahul Sukthankar,  Cristian Sminchisescu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0575</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Generating 3D People in Scenes Without People <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Generating_3D_People_in_Scenes_Without_People_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yan Zhang,  Mohamed Hassan,  Heiko Neumann,  Michael J. Black,  Siyu Tang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1665</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0169</td> <td>0.0000</td></tr>
<tr><td>Transferring Cross-Domain Knowledge for Video Sign Language Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Transferring_Cross-Domain_Knowledge_for_Video_Sign_Language_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongxu Li,  Xin Yu,  Chenchen Xu,  Lars Petersson,  Hongdong Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0222</td></tr>
<tr><td>Bodies at Rest: 3D Human Pose and Shape Estimation From a Pressure Image Using Synthetic Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Clever_Bodies_at_Rest_3D_Human_Pose_and_Shape_Estimation_From_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Henry M. Clever,  Zackory Erickson,  Ariel Kapusta,  Greg Turk,  Karen Liu,  Charles C. Kemp</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1371</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0191</td> <td>0.0000</td></tr>
<tr><td>Bayesian Adversarial Human Motion Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Bayesian_Adversarial_Human_Motion_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Zhao,  Hui Su,  Qiang Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>LSM: Learning Subspace Minimization for Low-Level Vision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_LSM_Learning_Subspace_Minimization_for_Low-Level_Vision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chengzhou Tang,  Lu Yuan,  Ping Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0172</td> <td>0.0501</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0022</td></tr>
<tr><td>Learning a Neural Solver for Multiple Object Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Braso_Learning_a_Neural_Solver_for_Multiple_Object_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guillem Braso,  Laura Leal-Taixe</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0742</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Truong_GLU-Net_Global-Local_Universal_Network_for_Dense_Flow_and_Correspondences_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Prune Truong,  Martin Danelljan,  Radu Timofte</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0120</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1324</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0023</td></tr>
<tr><td>SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_SiamCAR_Siamese_Fully_Convolutional_Classification_and_Regression_for_Visual_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongyan Guo,  Jun Wang,  Ying Cui,  Zhenhua Wang,  Shengyong Chen</td> <td>0.0022</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0654</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MaskFlownet: Asymmetric Feature Matching With Learnable Occlusion Mask <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_MaskFlownet_Asymmetric_Feature_Matching_With_Learnable_Occlusion_Mask_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shengyu Zhao,  Yilun Sheng,  Yue Dong,  Eric I-Chao Chang,  Yan Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0784</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2142</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td></tr>
<tr><td>Tracking by Instance Detection: A Meta-Learning Approach <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Tracking_by_Instance_Detection_A_Meta-Learning_Approach_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guangting Wang,  Chong Luo,  Xiaoyan Sun,  Zhiwei Xiong,  Wenjun Zeng</td> <td>0.0558</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0644</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>High-Performance Long-Term Tracking With Meta-Updater <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dai_High-Performance_Long-Term_Tracking_With_Meta-Updater_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kenan Dai,  Yunhua Zhang,  Dong Wang,  Jianhua Li,  Huchuan Lu,  Xiaoyun Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0181</td></tr>
<tr><td>TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_TubeTK_Adopting_Tubes_to_Track_Multi-Object_in_a_One-Step_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Pang,  Yizhuo Li,  Yifan Zhang,  Muchen Li,  Cewu Lu</td> <td>0.0149</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0005</td></tr>
<tr><td>Collaborative Motion Prediction via Neural Motion Message Passing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Collaborative_Motion_Prediction_via_Neural_Motion_Message_Passing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Hu,  Siheng Chen,  Ya Zhang,  Xiao Gu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0126</td></tr>
<tr><td>P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_P2B_Point-to-Box_Network_for_3D_Object_Tracking_in_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haozhe Qi,  Chen Feng,  Zhiguo Cao,  Feng Zhao,  Yang Xiao</td> <td>0.0204</td> <td>0.0615</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0089</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Deep Visual Odometry With Online Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Self-Supervised_Deep_Visual_Odometry_With_Online_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shunkai Li,  Xin Wang,  Yingdian Cao,  Fei Xue,  Zike Yan,  Hongbin Zha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0293</td> <td>0.0395</td></tr>
<tr><td>Globally Optimal Contrast Maximisation for Event-Based Motion Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Globally_Optimal_Contrast_Maximisation_for_Event-Based_Motion_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daqi Liu,  Alvaro Parra,  Tat-Jun Chin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bai_D3Feat_Joint_Learning_of_Dense_Detection_and_Description_of_3D_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuyang Bai,  Zixin Luo,  Lei Zhou,  Hongbo Fu,  Long Quan,  Chiew-Lan Tai</td> <td>0.0000</td> <td>0.1728</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td></tr>
<tr><td>Towards Backward-Compatible Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Towards_Backward-Compatible_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yantao Shen,  Yuanjun Xiong,  Wei Xia,  Stefano Soatto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0146</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PointAugment: An Auto-Augmentation Framework for Point Cloud Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_PointAugment_An_Auto-Augmentation_Framework_for_Point_Cloud_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruihui Li,  Xianzhi Li,  Pheng-Ann Heng,  Chi-Wing Fu</td> <td>0.0000</td> <td>0.0482</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>Cross-Batch Memory for Embedding Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Cross-Batch_Memory_for_Embedding_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xun Wang,  Haozhi Zhang,  Weilin Huang,  Matthew R. Scott</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0109</td></tr>
<tr><td>Circle Loss: A Unified Perspective of Pair Similarity Optimization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Circle_Loss_A_Unified_Perspective_of_Pair_Similarity_Optimization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yifan Sun,  Changmao Cheng,  Yuhan Zhang,  Chi Zhang,  Liang Zheng,  Zhongdao Wang,  Yichen Wei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jenni_Steering_Self-Supervised_Feature_Learning_Beyond_Local_Pixel_Statistics_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Simon Jenni,  Hailin Jin,  Paolo Favaro</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Hyperbolic Image Embeddings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Khrulkov_Hyperbolic_Image_Embeddings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Valentin Khrulkov,  Leyla Mirvakhabova,  Evgeniya Ustinova,  Ivan Oseledets,  Victor Lempitsky</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1194</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Controllable Orthogonalization in Training DNNs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Controllable_Orthogonalization_in_Training_DNNs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Huang,  Li Liu,  Fan Zhu,  Diwen Wan,  Zehuan Yuan,  Bo Li,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0394</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1631</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>An Investigation Into the Stochasticity of Batch Whitening <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_An_Investigation_Into_the_Stochasticity_of_Batch_Whitening_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Huang,  Lei Zhao,  Yi Zhou,  Fan Zhu,  Li Liu,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2499</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_High-Order_Information_Matters_Learning_Relation_and_Topology_for_Occluded_Person_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guan'an Wang,  Shuo Yang,  Huanyu Liu,  Zhicheng Wang,  Yang Yang,  Shuliang Wang,  Gang Yu,  Erjin Zhou,  Jian Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0748</td></tr>
<tr><td>Same Features, Different Day: Weakly Supervised Feature Learning for Seasonal Invariance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Spencer_Same_Features_Different_Day_Weakly_Supervised_Feature_Learning_for_Seasonal_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jaime Spencer,  Richard Bowden,  Simon Hadfield</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0550</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Dress 3D People in Generative Clothing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ma_Learning_to_Dress_3D_People_in_Generative_Clothing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qianli Ma,  Jinlong Yang,  Anurag Ranjan,  Sergi Pujades,  Gerard Pons-Moll,  Siyu Tang,  Michael J. Black</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0475</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MAST: A Memory-Augmented Self-Supervised Tracker <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lai_MAST_A_Memory-Augmented_Self-Supervised_Tracker_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zihang Lai,  Erika Lu,  Weidi Xie</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning by Analogy: Reliable Supervision From Transformations for Unsupervised Optical Flow Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Learning_by_Analogy_Reliable_Supervision_From_Transformations_for_Unsupervised_Optical_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Liang Liu,  Jiangning Zhang,  Ruifei He,  Yong Liu,  Yabiao Wang,  Ying Tai,  Donghao Luo,  Chengjie Wang,  Jilin Li,  Feiyue Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0566</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking With 2D-3D Multi-Feature Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Weng_GNN3DMOT_Graph_Neural_Network_for_3D_Multi-Object_Tracking_With_2D-3D_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xinshuo Weng,  Yongxin Wang,  Yunze Man,  Kris M. Kitani</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0287</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0242</td></tr>
<tr><td>ClusterFit: Improving Generalization of Visual Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_ClusterFit_Improving_Generalization_of_Visual_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xueting Yan,  Ishan Misra,  Abhinav Gupta,  Deepti Ghadiyaram,  Dhruv Mahajan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0315</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0704</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Dynamic Relationships for 3D Human Motion Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cui_Learning_Dynamic_Relationships_for_3D_Human_Motion_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiongjie Cui,  Huaijiang Sun,  Fei Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0603</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td></tr>
<tr><td>Knowledge As Priors: Cross-Modal Knowledge Generalization for Datasets Without Superior Knowledge <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Knowledge_As_Priors_Cross-Modal_Knowledge_Generalization_for_Datasets_Without_Superior_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Long Zhao,  Xi Peng,  Yuxiao Chen,  Mubbasir Kapadia,  Dimitris N. Metaxas</td> <td>0.0000</td> <td>0.0263</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0479</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0525</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0104</td></tr>
<tr><td>S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_S3VAE_Self-Supervised_Sequential_VAE_for_Representation_Disentanglement_and_Data_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yizhe Zhu,  Martin Renqiang Min,  Asim Kadav,  Hans Peter Graf</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yao_Video_Playback_Rate_Perception_for_Self-Supervised_Spatio-Temporal_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuan Yao,  Chang Liu,  Dezhao Luo,  Yu Zhou,  Qixiang Ye</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0114</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0547</td> <td>0.0000</td> <td>0.0184</td></tr>
<tr><td>Learning to Manipulate Individual Objects in an Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_to_Manipulate_Individual_Objects_in_an_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanchao Yang,  Yutong Chen,  Stefano Soatto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PADS: Policy-Adapted Sampling for Visual Similarity Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Roth_PADS_Policy-Adapted_Sampling_for_Visual_Similarity_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Karsten Roth,  Timo Milbich,  Bjorn Ommer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0004</td></tr>
<tr><td>Siam R-CNN: Visual Tracking by Re-Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Voigtlaender_Siam_R-CNN_Visual_Tracking_by_Re-Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Paul Voigtlaender,  Jonathon Luiten,  Philip H.S. Torr,  Bastian Leibe</td> <td>0.0342</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ASLFeat: Learning Local Features of Accurate Shape and Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_ASLFeat_Learning_Local_Features_of_Accurate_Shape_and_Localization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zixin Luo,  Lei Zhou,  Xuyang Bai,  Hongkai Chen,  Jiahui Zhang,  Yao Yao,  Shiwei Li,  Tian Fang,  Long Quan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Filter Grafting for Deep Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Meng_Filter_Grafting_for_Deep_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fanxu Meng,  Hao Cheng,  Ke Li,  Zhixin Xu,  Rongrong Ji,  Xing Sun,  Guangming Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0351</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HOPE-Net: A Graph-Based Model for Hand-Object Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Doosti_HOPE-Net_A_Graph-Based_Model_for_Hand-Object_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bardia Doosti,  Shujon Naha,  Majid Mirbagheri,  David J. Crandall</td> <td>0.0000</td> <td>0.0000</td> <td>0.0343</td> <td>0.0000</td> <td>0.0000</td> <td>0.0268</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DeepFaceFlow: In-the-Wild Dense 3D Facial Motion Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Koujan_DeepFaceFlow_In-the-Wild_Dense_3D_Facial_Motion_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohammad Rami Koujan,  Anastasios Roussos,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning for Video Compression With Hierarchical Quality and Recurrent Enhancement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_for_Video_Compression_With_Hierarchical_Quality_and_Recurrent_Enhancement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ren Yang,  Fabian Mentzer,  Luc Van Gool,  Radu Timofte</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0106</td></tr>
<tr><td>Learning Better Lossless Compression Using Lossy Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mentzer_Learning_Better_Lossless_Compression_Using_Lossy_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fabian Mentzer,  Luc Van Gool,  Michael Tschannen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0079</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0051</td></tr>
<tr><td>Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Flow2Stereo_Effective_Self-Supervised_Learning_of_Optical_Flow_and_Stereo_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pengpeng Liu,  Irwin King,  Michael R. Lyu,  Jia Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1133</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0239</td></tr>
<tr><td>Multi-Scale Fusion Subspace Clustering Using Similarity Constraint <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dang_Multi-Scale_Fusion_Subspace_Clustering_Using_Similarity_Constraint_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiyuan Dang,  Cheng Deng,  Xu Yang,  Heng Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0119</td></tr>
<tr><td>Siamese Box Adaptive Network for Visual Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Siamese_Box_Adaptive_Network_for_Visual_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zedu Chen,  Bineng Zhong,  Guorong Li,  Shengping Zhang,  Rongrong Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0612</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0098</td></tr>
<tr><td>Cross-Domain Face Presentation Attack Detection via Multi-Domain Disentangled Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Cross-Domain_Face_Presentation_Attack_Detection_via_Multi-Domain_Disentangled_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guoqing Wang,  Hu Han,  Shiguang Shan,  Xilin Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td></tr>
<tr><td>Online Deep Clustering for Unsupervised Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhan_Online_Deep_Clustering_for_Unsupervised_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaohang Zhan,  Jiahao Xie,  Ziwei Liu,  Yew-Soon Ong,  Chen Change Loy</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0047</td></tr>
<tr><td>Density-Aware Feature Embedding for Face Clustering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Density-Aware_Feature_Embedding_for_Face_Clustering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Senhui Guo,  Jing Xu,  Dapeng Chen,  Chao Zhang,  Xiaogang Wang,  Rui Zhao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0126</td></tr>
<tr><td>Self-Supervised Learning of Pretext-Invariant Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ishan Misra,  Laurens van der Maaten</td> <td>0.0093</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ROAM: Recurrently Optimizing Tracking Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_ROAM_Recurrently_Optimizing_Tracking_Model_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianyu Yang,  Pengfei Xu,  Runbo Hu,  Hua Chai,  Antoni B. Chan</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deformable Siamese Attention Networks for Visual Object Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Deformable_Siamese_Attention_Networks_for_Visual_Object_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuechen Yu,  Yilei Xiong,  Weilin Huang,  Matthew R. Scott</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0041</td></tr>
<tr><td>15 Keypoints Is All You Need <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Snower_15_Keypoints_Is_All_You_Need_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michael Snower,  Asim Kadav,  Farley Lai,  Hans Peter Graf</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0875</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Optical Flow in the Dark <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Optical_Flow_in_the_Dark_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yinqiang Zheng,  Mingfang Zhang,  Feng Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.4167</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Sketch-BERT: Learning Sketch Bidirectional Encoder Representation From Transformers by Self-Supervised Learning of Sketch Gestalt <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Sketch-BERT_Learning_Sketch_Bidirectional_Encoder_Representation_From_Transformers_by_Self-Supervised_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hangyu Lin,  Yanwei Fu,  Xiangyang Xue,  Yu-Gang Jiang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Unified Object Motion and Affinity Model for Online Multi-Object Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_A_Unified_Object_Motion_and_Affinity_Model_for_Online_Multi-Object_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junbo Yin,  Wenguan Wang,  Qinghao Meng,  Ruigang Yang,  Jianbing Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0226</td></tr>
<tr><td>Sub-Frame Appearance and 6D Pose Estimation of Fast Moving Objects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rozumnyi_Sub-Frame_Appearance_and_6D_Pose_Estimation_of_Fast_Moving_Objects_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Denys Rozumnyi,  Jan Kotera,  Filip Sroubek,  Jiri Matas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0088</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td></tr>
<tr><td>How to Train Your Deep Multi-Object Tracker <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_How_to_Train_Your_Deep_Multi-Object_Tracker_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yihong Xu,  Aljosa Osep,  Yutong Ban,  Radu Horaud,  Laura Leal-Taixe,  Xavier Alameda-Pineda</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0121</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0187</td></tr>
<tr><td>TPNet: Trajectory Proposal Network for Motion Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_TPNet_Trajectory_Proposal_Network_for_Motion_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Liangji Fang,  Qinhong Jiang,  Jianping Shi,  Bolei Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0007</td></tr>
<tr><td>Large Scale Video Representation Learning via Relational Graph Clustering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Large_Scale_Video_Representation_Learning_via_Relational_Graph_Clustering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hyodong Lee,  Joonseok Lee,  Joe Yue-Hei Ng,  Paul Natsev</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0474</td></tr>
<tr><td>Towards Universal Representation Learning for Deep Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Towards_Universal_Representation_Learning_for_Deep_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yichun Shi,  Xiang Yu,  Kihyuk Sohn,  Manmohan Chandraker,  Anil K. Jain</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1178</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1089</td> <td>0.0000</td></tr>
<tr><td>Robust Partial Matching for Person Search in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhong_Robust_Partial_Matching_for_Person_Search_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yingji Zhong,  Xiaoyu Wang,  Shiliang Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Correlation-Guided Attention for Corner Detection Based Visual Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Correlation-Guided_Attention_for_Corner_Detection_Based_Visual_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fei Du,  Peng Liu,  Wei Zhao,  Xianglong Tang</td> <td>0.0022</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Multi-Object Tracking and Segmentation From Automatic Annotations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Porzi_Learning_Multi-Object_Tracking_and_Segmentation_From_Automatic_Annotations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lorenzo Porzi,  Markus Hofinger,  Idoia Ruiz,  Joan Serrat,  Samuel Rota Bulo,  Peter Kontschieder</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0626</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0677</td> <td>0.0000</td> <td>0.0000</td> <td>0.1641</td> <td>0.0000</td></tr>
<tr><td>PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Benzine_PandaNet_Anchor-Based_Single-Shot_Multi-Person_3D_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abdallah Benzine,  Florian Chabot,  Bertrand Luvison,  Quoc Cuong Pham,  Catherine Achard</td> <td>0.0024</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2452</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Rotation Consistent Margin Loss for Efficient Low-Bit Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Rotation_Consistent_Margin_Loss_for_Efficient_Low-Bit_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yudong Wu,  Yichao Wu,  Ruihao Gong,  Yuanhao Lv,  Ken Chen,  Ding Liang,  Xiaolin Hu,  Xianglong Liu,  Junjie Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0092</td></tr>
<tr><td>Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Joint_Spatial-Temporal_Optimization_for_Stereo_3D_Object_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peiliang Li,  Jieqi Shi,  Shaojie Shen</td> <td>0.0707</td> <td>0.0000</td> <td>0.0313</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unity Style Transfer for Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unity_Style_Transfer_for_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chong Liu,  Xiaojun Chang,  Yi-Dong Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0281</td></tr>
<tr><td>Suppressing Uncertainties for Large-Scale Facial Expression Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Suppressing_Uncertainties_for_Large-Scale_Facial_Expression_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Wang,  Xiaojiang Peng,  Jianfei Yang,  Shijian Lu,  Yu Qiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mitra_Multiview-Consistent_Semi-Supervised_Learning_for_3D_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rahul Mitra,  Nitesh B. Gundavarapu,  Abhishek Sharma,  Arjun Jain</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2737</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Regularizing Neural Networks via Minimizing Hyperspherical Energy <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Regularizing_Neural_Networks_via_Minimizing_Hyperspherical_Energy_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rongmei Lin,  Weiyang Liu,  Zhen Liu,  Chen Feng,  Zhiding Yu,  James M. Rehg,  Li Xiong,  Le Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0437</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Representations by Predicting Bags of Visual Words <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gidaris_Learning_Representations_by_Predicting_Bags_of_Visual_Words_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Spyros Gidaris,  Andrei Bursuc,  Nikos Komodakis,  Patrick Perez,  Matthieu Cord</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Khan_AnimalWeb_A_Large-Scale_Hierarchical_Dataset_of_Annotated_Animal_Faces_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muhammad Haris Khan,  John McDonagh,  Salman Khan,  Muhammad Shahabuddin,  Aditya Arora,  Fahad Shahbaz Khan,  Ling Shao,  Georgios Tzimiropoulos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0090</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Transductive Approach for Video Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_A_Transductive_Approach_for_Video_Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yizhuo Zhang,  Zhirong Wu,  Houwen Peng,  Stephen Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0574</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0100</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0509</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Dynamic Face Video Segmentation via Reinforcement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dynamic_Face_Video_Segmentation_via_Reinforcement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yujiang Wang,  Mingzhi Dong,  Jie Shen,  Yang Wu,  Shiyang Cheng,  Maja Pantic</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0239</td></tr>
<tr><td>Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chibane_Implicit_Functions_in_Feature_Space_for_3D_Shape_Reconstruction_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Julian Chibane,  Thiemo Alldieck,  Gerard Pons-Moll</td> <td>0.0162</td> <td>0.0242</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0039</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0220</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Semantic Drift Compensation for Class-Incremental Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Semantic_Drift_Compensation_for_Class-Incremental_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lu Yu,  Bartlomiej Twardowski,  Xialei Liu,  Luis Herranz,  Kai Wang,  Yongmei Cheng,  Shangling Jui,  Joost van de Weijer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td></tr>
<tr><td>Context-Aware Human Motion Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Corona_Context-Aware_Human_Motion_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Enric Corona,  Albert Pumarola,  Guillem Alenya,  Francesc Moreno-Noguer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0118</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0334</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DeepDeform: Learning Non-Rigid RGB-D Reconstruction With Semi-Supervised Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bozic_DeepDeform_Learning_Non-Rigid_RGB-D_Reconstruction_With_Semi-Supervised_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aljaz Bozic,  Michael Zollhofer,  Christian Theobalt,  Matthias Niessner</td> <td>0.0000</td> <td>0.0000</td> <td>0.0135</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Optical Non-Line-of-Sight Physics-Based 3D Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Isogawa_Optical_Non-Line-of-Sight_Physics-Based_3D_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mariko Isogawa,  Ye Yuan,  Matthew O'Toole,  Kris M. Kitani</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3047</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Transfer Texture From Clothing Images to 3D Humans <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mir_Learning_to_Transfer_Texture_From_Clothing_Images_to_3D_Humans_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aymen Mir,  Thiemo Alldieck,  Gerard Pons-Moll</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0410</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>UniPose: Unified Human Pose Estimation in Single Images and Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Artacho_UniPose_Unified_Human_Pose_Estimation_in_Single_Images_and_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bruno Artacho,  Andreas Savakis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2720</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Minimal Solutions to Relative Pose Estimation From Two Views Sharing a Common Direction With Unknown Focal Length <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ding_Minimal_Solutions_to_Relative_Pose_Estimation_From_Two_Views_Sharing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaqing Ding,  Jian Yang,  Jean Ponce,  Hui Kong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0277</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0033</td></tr>
<tr><td>3D Human Mesh Regression With Dense Correspondence <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_3D_Human_Mesh_Regression_With_Dense_Correspondence_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wang Zeng,  Wanli Ouyang,  Ping Luo,  Wentao Liu,  Xiaogang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0186</td> <td>0.0000</td> <td>0.0000</td> <td>0.0158</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0342</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0050</td></tr>
<tr><td>Cross-Modal Pattern-Propagation for RGB-T Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Cross-Modal_Pattern-Propagation_for_RGB-T_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaoqun Wang,  Chunyan Xu,  Zhen Cui,  Ling Zhou,  Tong Zhang,  Xiaoya Zhang,  Jian Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0346</td></tr>
<tr><td>Distilling Knowledge From Graph Convolutional Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Distilling_Knowledge_From_Graph_Convolutional_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiding Yang,  Jiayan Qiu,  Mingli Song,  Dacheng Tao,  Xinchao Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0288</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0302</td></tr>
<tr><td>Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Po-Hsiang Huang,  Fu-En Yang,  Yu-Chiang Frank Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1204</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td></tr>
<tr><td>Distribution-Aware Coordinate Representation for Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Distribution-Aware_Coordinate_Representation_for_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Feng Zhang,  Xiatian Zhu,  Hanbin Dai,  Mao Ye,  Ce Zhu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2517</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0204</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Parsing-Based View-Aware Embedding Network for Vehicle Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Meng_Parsing-Based_View-Aware_Embedding_Network_for_Vehicle_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dechao Meng,  Liang Li,  Xuejing Liu,  Yadong Li,  Shijie Yang,  Zheng-Jun Zha,  Xingyu Gao,  Shuhui Wang,  Qingming Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0432</td></tr>
<tr><td>HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation From a Single Depth Map <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Malik_HandVoxNet_Deep_Voxel-Based_Network_for_3D_Hand_Shape_and_Pose_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jameel Malik,  Ibrahim Abdelaziz,  Ahmed Elhayek,  Soshi Shimada,  Sk Aziz Ali,  Vladislav Golyanik,  Christian Theobalt,  Didier Stricker</td> <td>0.0000</td> <td>0.0000</td> <td>0.0165</td> <td>0.0000</td> <td>0.0000</td> <td>0.0340</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1472</td> <td>0.0000</td> <td>0.0178</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td></tr>
<tr><td>Determinant Regularization for Gradient-Efficient Graph Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Determinant_Regularization_for_Gradient-Efficient_Graph_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianshu Yu,  Junchi Yan,  Baoxin Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>D3S - A Discriminative Single Shot Segmentation Tracker <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lukezic_D3S_-_A_Discriminative_Single_Shot_Segmentation_Tracker_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alan Lukezic,  Jiri Matas,  Matej Kristan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Marchetti_MANTRA_Memory_Augmented_Networks_for_Multiple_Trajectory_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Francesco Marchetti,  Federico Becattini,  Lorenzo Seidenari,  Alberto Del Bimbo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0286</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Toromanoff_End-to-End_Model-Free_Reinforcement_Learning_for_Urban_Driving_Using_Implicit_Affordances_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Marin Toromanoff,  Emilie Wirbel,  Fabien Moutarde</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GraphTER: Unsupervised Learning of Graph Transformation Equivariant Representations via Auto-Encoding Node-Wise Transformations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_GraphTER_Unsupervised_Learning_of_Graph_Transformation_Equivariant_Representations_via_Auto-Encoding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiang Gao,  Wei Hu,  Guo-Jun Qi</td> <td>0.0000</td> <td>0.0908</td> <td>0.0279</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0497</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0065</td></tr>
<tr><td>Can Facial Pose and Expression Be Separated With Weak Perspective Camera? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sariyanidi_Can_Facial_Pose_and_Expression_Be_Separated_With_Weak_Perspective_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Evangelos Sariyanidi,  Casey J. Zampella,  Robert T. Schultz,  Birkan Tunc</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Probabilistic Regression for Visual Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Danelljan_Probabilistic_Regression_for_Visual_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Martin Danelljan,  Luc Van Gool,  Radu Timofte</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>3DRegNet: A Deep Neural Network for 3D Point Registration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pais_3DRegNet_A_Deep_Neural_Network_for_3D_Point_Registration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>G. Dias Pais,  Srikumar Ramalingam,  Venu Madhav Govindu,  Jacinto C. Nascimento,  Rama Chellappa,  Pedro Miraldo</td> <td>0.0000</td> <td>0.0055</td> <td>0.0754</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0153</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fabbri_Compressed_Volumetric_Heatmaps_for_Multi-Person_3D_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matteo Fabbri,  Fabio Lanzi,  Simone Calderara,  Stefano Alletto,  Rita Cucchiara</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2658</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0133</td></tr>
<tr><td>Three-Dimensional Reconstruction of Human Interactions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fieraru_Three-Dimensional_Reconstruction_of_Human_Interactions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mihai Fieraru,  Mihai Zanfir,  Elisabeta Oneata,  Alin-Ionut Popa,  Vlad Olaru,  Cristian Sminchisescu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0325</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Distribution-Induced Bidirectional Generative Adversarial Network for Graph Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Distribution-Induced_Bidirectional_Generative_Adversarial_Network_for_Graph_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuai Zheng,  Zhenfeng Zhu,  Xingxing Zhang,  Zhizhe Liu,  Jian Cheng,  Yao Zhao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1177</td> <td>0.0000</td> <td>0.0000</td> <td>0.0670</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Minimal Solvers for 3D Scan Alignment With Pairs of Intersecting Lines <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mateus_Minimal_Solvers_for_3D_Scan_Alignment_With_Pairs_of_Intersecting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Andre Mateus,  Srikumar Ramalingam,  Pedro Miraldo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0055</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Wavelet Integrated CNNs for Noise-Robust Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Wavelet_Integrated_CNNs_for_Noise-Robust_Image_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiufu Li,  Linlin Shen,  Sheng Guo,  Zhihui Lai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0635</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td></tr>
<tr><td>Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ko_Embedding_Expansion_Augmentation_in_Embedding_Space_for_Deep_Metric_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Byungsoo Ko,  Geonmo Gu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0095</td></tr>
<tr><td>PropagationNet: Propagate Points to Curve to Learn Structure Information <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_PropagationNet_Propagate_Points_to_Curve_to_Learn_Structure_Information_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiehe Huang,  Weihong Deng,  Haifeng Shen,  Xiubao Zhang,  Jieping Ye</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0120</td></tr>
<tr><td>Sequential 3D Human Pose and Shape Estimation From Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Sequential_3D_Human_Pose_and_Shape_Estimation_From_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kangkan Wang,  Jin Xie,  Guofeng Zhang,  Lei Liu,  Jian Yang</td> <td>0.0000</td> <td>0.1018</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0998</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0151</td></tr>
<tr><td>Improving the Robustness of Capsule Networks to Image Affine Transformations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Improving_the_Robustness_of_Capsule_Networks_to_Image_Affine_Transformations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jindong Gu,  Volker Tresp</td> <td>0.0000</td> <td>0.0000</td> <td>0.0501</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Noise Modeling, Synthesis and Classification for Generic Object Anti-Spoofing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Stehouwer_Noise_Modeling_Synthesis_and_Classification_for_Generic_Object_Anti-Spoofing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Joel Stehouwer,  Amin Jourabloo,  Yaojie Liu,  Xiaoming Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Quaternion Product Units for Deep Learning on 3D Rotation Groups <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Quaternion_Product_Units_for_Deep_Learning_on_3D_Rotation_Groups_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuan Zhang,  Shaofei Qin,  Yi Xu,  Hongteng Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0159</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Representation Learning for Gaze Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Unsupervised_Representation_Learning_for_Gaze_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu Yu,  Jean-Marc Odobez</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0128</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>P-nets: Deep Polynomial Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chrysos_P-nets_Deep_Polynomial_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Grigorios G. Chrysos,  Stylianos Moschoglou,  Giorgos Bouritsas,  Yannis Panagakis,  Jiankang Deng,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0558</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0355</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0025</td></tr>
<tr><td>Hierarchically Robust Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qian_Hierarchically_Robust_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Qian,  Juhua Hu,  Hao Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>How Useful Is Self-Supervised Pretraining for Visual Tasks? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Newell_How_Useful_Is_Self-Supervised_Pretraining_for_Visual_Tasks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alejandro Newell,  Jia Deng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0218</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0539</td> <td>0.0000</td></tr>
<tr><td>Copy and Paste GAN: Face Hallucination From Shaded Thumbnails <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Zhang,  Ivor W. Tsang,  Yawei Luo,  Chang-Hui Hu,  Xiaobo Lu,  Xin Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0291</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1093</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0292</td></tr>
<tr><td>TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Patel_TailorNet_Predicting_Clothing_in_3D_as_a_Function_of_Human_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaitanya Patel,  Zhouyingcheng Liao,  Gerard Pons-Moll</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0100</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0264</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Object-Occluded Human Shape and Pose Estimation From a Single Color Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object-Occluded_Human_Shape_and_Pose_Estimation_From_a_Single_Color_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianshu Zhang,  Buzhen Huang,  Yangang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0494</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0046</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0974</td></tr>
<tr><td>Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Recursive_Least-Squares_Estimator-Aided_Online_Learning_for_Visual_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jin Gao,  Weiming Hu,  Yan Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Monocular Scene Flow Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hur_Self-Supervised_Monocular_Scene_Flow_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junhwa Hur,  Stefan Roth</td> <td>0.0000</td> <td>0.0000</td> <td>0.0235</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0243</td> <td>0.0000</td> <td>0.0000</td> <td>0.1448</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Fast and Robust Target Models for Video Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Robinson_Learning_Fast_and_Robust_Target_Models_for_Video_Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Andreas Robinson,  Felix Jaremo Lawin,  Martin Danelljan,  Fahad Shahbaz Khan,  Michael Felsberg</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0021</td> <td>0.0000</td> <td>0.0000</td> <td>0.0116</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0655</td> <td>0.0028</td></tr>
<tr><td>Reciprocal Learning Networks for Human Trajectory Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Reciprocal_Learning_Networks_for_Human_Trajectory_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hao Sun,  Zhiqun Zhao,  Zhihai He</td> <td>0.0000</td> <td>0.0000</td> <td>0.0282</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0467</td></tr>
<tr><td>Nonparametric Object and Parts Modeling With Lie Group Dynamics <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hayden_Nonparametric_Object_and_Parts_Modeling_With_Lie_Group_Dynamics_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>David S. Hayden,  Jason Pacheco,  John W. Fisher III</td> <td>0.0000</td> <td>0.0621</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Shadow Hand-Drawn Sketches <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Learning_to_Shadow_Hand-Drawn_Sketches_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qingyuan Zheng,  Zhuoru Li,  Adam Bargteil</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Intuitive, Interactive Beard and Hair Synthesis With Generative Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Olszewski_Intuitive_Interactive_Beard_and_Hair_Synthesis_With_Generative_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kyle Olszewski,  Duygu Ceylan,  Jun Xing,  Jose Echevarria,  Zhili Chen,  Weikai Chen,  Hao Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0238</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Semantic Pyramid for Image Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shocher_Semantic_Pyramid_for_Image_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Assaf Shocher,  Yossi Gandelsman,  Inbar Mosseri,  Michal Yarom,  Michal Irani,  William T. Freeman,  Tali Dekel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SynSin: End-to-End View Synthesis From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wiles_SynSin_End-to-End_View_Synthesis_From_a_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Olivia Wiles,  Georgia Gkioxari,  Richard Szeliski,  Justin Johnson</td> <td>0.0000</td> <td>0.0769</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Characteristic Function Approach to Deep Implicit Generative Modeling <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ansari_A_Characteristic_Function_Approach_to_Deep_Implicit_Generative_Modeling_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abdul Fatir Ansari,  Jonathan Scarlett,  Harold Soh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0539</td></tr>
<tr><td>High-Resolution Daytime Translation Without Domain Labels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Anokhin_High-Resolution_Daytime_Translation_Without_Domain_Labels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ivan Anokhin,  Pavel Solovev,  Denis Korzhenkov,  Alexey Kharlamov,  Taras Khakhulin,  Aleksei Silvestrov,  Sergey Nikolenko,  Victor Lempitsky,  Gleb Sterkin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Leveraging 2D Data to Learn Textured 3D Mesh Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Henderson_Leveraging_2D_Data_to_Learn_Textured_3D_Mesh_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Paul Henderson,  Vagia Tsiminaki,  Christoph H. Lampert</td> <td>0.0196</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yi_Contextual_Residual_Aggregation_for_Ultra_High-Resolution_Image_Inpainting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zili Yi,  Qiang Tang,  Shekoofeh Azizi,  Daesik Jang,  Zhan Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0176</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0623</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Flow Contrastive Estimation of Energy-Based Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Flow_Contrastive_Estimation_of_Energy-Based_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruiqi Gao,  Erik Nijkamp,  Diederik P. Kingma,  Zhen Xu,  Andrew M. Dai,  Ying Nian Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0715</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td></tr>
<tr><td>Hardware-in-the-Loop End-to-End Optimization of Camera Image Processing Pipelines <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mosleh_Hardware-in-the-Loop_End-to-End_Optimization_of_Camera_Image_Processing_Pipelines_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ali Mosleh,  Avinash Sharma,  Emmanuel Onzon,  Fahim Mannan,  Nicolas Robidoux,  Felix Heide</td> <td>0.0547</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0608</td></tr>
<tr><td>Search to Distill: Pearls Are Everywhere but Not the Eyes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu Liu,  Xuhui Jia,  Mingxing Tan,  Raviteja Vemulapalli,  Yukun Zhu,  Bradley Green,  Xiaogang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0122</td> <td>0.1452</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0110</td></tr>
<tr><td>Total Deep Variation for Linear Inverse Problems <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kobler_Total_Deep_Variation_for_Linear_Inverse_Problems_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Erich Kobler,  Alexander Effland,  Karl Kunisch,  Thomas Pock</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0428</td></tr>
<tr><td>Relative Interior Rule in Block-Coordinate Descent <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Werner_Relative_Interior_Rule_in_Block-Coordinate_Descent_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tomas Werner,  Daniel Prusa,  Tomas Dlask</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Combinatorial Solver for Graph Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Learning_Combinatorial_Solver_for_Graph_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tao Wang,  He Liu,  Yidong Li,  Yi Jin,  Xiaohui Hou,  Haibin Ling</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0606</td></tr>
<tr><td>SampleNet: Differentiable Point Cloud Sampling <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_SampleNet_Differentiable_Point_Cloud_Sampling_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Itai Lang,  Asaf Manor,  Shai Avidan</td> <td>0.0000</td> <td>0.2052</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Can We Learn Heuristics for Graphical Model Inference Using Reinforcement Learning? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Messaoud_Can_We_Learn_Heuristics_for_Graphical_Model_Inference_Using_Reinforcement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Safa Messaoud,  Maghav Kumar,  Alexander G. Schwing</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1494</td> <td>0.1571</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0518</td> <td>0.0000</td> <td>0.0000</td> <td>0.1102</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Quasi-Newton Solver for Robust Non-Rigid Registration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yao_Quasi-Newton_Solver_for_Robust_Non-Rigid_Registration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuxin Yao,  Bailin Deng,  Weiwei Xu,  Juyong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0772</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0388</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0613</td></tr>
<tr><td>Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition From a Domain Adaptation Perspective <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jamal_Rethinking_Class-Balanced_Methods_for_Long-Tailed_Visual_Recognition_From_a_Domain_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muhammad Abdullah Jamal,  Matthew Brown,  Ming-Hsuan Yang,  Liqiang Wang,  Boqing Gong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0933</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0855</td> <td>0.0000</td></tr>
<tr><td>Optimizing Rank-Based Metrics With Blackbox Differentiation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rolinek_Optimizing_Rank-Based_Metrics_With_Blackbox_Differentiation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michal Rolinek,  Vit Musil,  Anselm Paulus,  Marin Vlastelica,  Claudio Michaelis,  Georg Martius</td> <td>0.0361</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0568</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DualSDF: Semantic Shape Manipulation Using a Two-Level Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hao_DualSDF_Semantic_Shape_Manipulation_Using_a_Two-Level_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zekun Hao,  Hadar Averbuch-Elor,  Noah Snavely,  Serge Belongie</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Dynamic_Hierarchical_Mimicking_Towards_Consistent_Optimization_Objectives_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Duo Li,  Qifeng Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0553</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0111</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0136</td></tr>
<tr><td>Deep Homography Estimation for Dynamic Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Deep_Homography_Estimation_for_Dynamic_Scenes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hoang Le,  Feng Liu,  Shu Zhang,  Aseem Agarwala</td> <td>0.0000</td> <td>0.0000</td> <td>0.0779</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0195</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PF-Net: Point Fractal Network for 3D Point Cloud Completion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_PF-Net_Point_Fractal_Network_for_3D_Point_Cloud_Completion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zitian Huang,  Yikuan Yu,  Jiawen Xu,  Feng Ni,  Xinyi Le</td> <td>0.0000</td> <td>0.1914</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On the Regularization Properties of Structured Dropout <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_On_the_Regularization_Properties_of_Structured_Dropout_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ambar Pal,  Connor Lane,  Rene Vidal,  Benjamin D. Haeffele</td> <td>0.0000</td> <td>0.0000</td> <td>0.0200</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Oracle Attention for High-Fidelity Face Completion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Learning_Oracle_Attention_for_High-Fidelity_Face_Completion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tong Zhou,  Changxing Ding,  Shaowen Lin,  Xinchao Wang,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0406</td></tr>
<tr><td>Deep Image Spatial Transformation for Person Image Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ren_Deep_Image_Spatial_Transformation_for_Person_Image_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yurui Ren,  Xiaoming Yu,  Junming Chen,  Thomas H. Li,  Ge Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0350</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0021</td> <td>0.0000</td> <td>0.0740</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0007</td></tr>
<tr><td>Learning to Optimize on SPD Manifolds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Learning_to_Optimize_on_SPD_Manifolds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhi Gao,  Yuwei Wu,  Yunde Jia,  Mehrtash Harandi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1227</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td></tr>
<tr><td>Deep 3D Portrait From a Single Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Deep_3D_Portrait_From_a_Single_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sicheng Xu,  Jiaolong Yang,  Dong Chen,  Fang Wen,  Yu Deng,  Yunde Jia,  Xin Tong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0064</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0668</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1782</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RDCFace: Radial Distortion Correction for Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_RDCFace_Radial_Distortion_Correction_for_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>He Zhao,  Xianghua Ying,  Yongjie Shi,  Xin Tong,  Jingsi Wen,  Hongbin Zha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0224</td></tr>
<tr><td>Global-Local GCN: Large-Scale Label Noise Cleansing for Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaobin Zhang,  Weihong Deng,  Mei Wang,  Jiani Hu,  Xian Li,  Dongyue Zhao,  Dongchao Wen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0068</td> <td>0.0000</td> <td>0.0357</td></tr>
<tr><td>MISC: Multi-Condition Injection and Spatially-Adaptive Compositing for Conditional Person Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Weng_MISC_Multi-Condition_Injection_and_Spatially-Adaptive_Compositing_for_Conditional_Person_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuchen Weng,  Wenbo Li,  Dawei Li,  Hongxia Jin,  Boxin Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_SAINT_Spatially_Aware_Interpolation_NeTwork_for_Medical_Slice_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cheng Peng,  Wei-An Lin,  Haofu Liao,  Rama Chellappa,  S. Kevin Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Recurrent Feature Reasoning for Image Inpainting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Recurrent_Feature_Reasoning_for_Image_Inpainting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingyuan Li,  Ning Wang,  Lefei Zhang,  Bo Du,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0969</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Structure-Preserving Super Resolution With Gradient Guidance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ma_Structure-Preserving_Super_Resolution_With_Gradient_Guidance_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cheng Ma,  Yongming Rao,  Yean Cheng,  Ce Chen,  Jiwen Lu,  Jie Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1319</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0257</td></tr>
<tr><td>Epipolar Transformers <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Epipolar_Transformers_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yihui He,  Rui Yan,  Katerina Fragkiadaki,  Shoou-I Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0526</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0011</td></tr>
<tr><td>Diversified Arbitrary Style Transfer via Deep Feature Perturbation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Diversified_Arbitrary_Style_Transfer_via_Deep_Feature_Perturbation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhizhong Wang,  Lei Zhao,  Haibo Chen,  Lihong Qiu,  Qihang Mo,  Sihuan Lin,  Wei Xing,  Dongming Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0239</td></tr>
<tr><td>MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Karnewar_MSG-GAN_Multi-Scale_Gradients_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Animesh Karnewar,  Oliver Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3882</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Overcoming Multi-Model Forgetting in One-Shot NAS With Diversity Maximization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Overcoming_Multi-Model_Forgetting_in_One-Shot_NAS_With_Diversity_Maximization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Miao Zhang,  Huiqi Li,  Shirui Pan,  Xiaojun Chang,  Steven Su</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1412</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0063</td></tr>
<tr><td>Select to Better Learn: Fast and Accurate Deep Learning Using Data Selection From Nonlinear Manifolds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Joneidi_Select_to_Better_Learn_Fast_and_Accurate_Deep_Learning_Using_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohsen Joneidi,  Saeed Vahidian,  Ashkan Esmaeili,  Weijia Wang,  Nazanin Rahnavard,  Bill Lin,  Mubarak Shah</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0495</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Neural Point Cloud Rendering via Multi-Plane Projection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dai_Neural_Point_Cloud_Rendering_via_Multi-Plane_Projection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peng Dai,  Yinda Zhang,  Zhuwen Li,  Shuaicheng Liu,  Bing Zeng</td> <td>0.0000</td> <td>0.1456</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Wish You Were Here: Context-Aware Human Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gafni_Wish_You_Were_Here_Context-Aware_Human_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Oran Gafni,  Lior Wolf</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Photo-Realistic Virtual Try-On by Adaptively Generating-Preserving Image Content <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Towards_Photo-Realistic_Virtual_Try-On_by_Adaptively_Generating-Preserving_Image_Content_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Han Yang,  Ruimao Zhang,  Xiaobao Guo,  Wei Liu,  Wangmeng Zuo,  Ping Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0376</td> <td>0.0206</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td></tr>
<tr><td>Breaking the Cycle - Colleagues Are All You Need <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nizan_Breaking_the_Cycle_-_Colleagues_Are_All_You_Need_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ori Nizan,  Ayellet Tal</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0063</td></tr>
<tr><td>Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Local_Class-Specific_and_Global_Image-Level_Generative_Adversarial_Networks_for_Semantic-Guided_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hao Tang,  Dan Xu,  Yan Yan,  Philip H.S. Torr,  Nicu Sebe</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0193</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0029</td></tr>
<tr><td>ManiGAN: Text-Guided Image Manipulation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_ManiGAN_Text-Guided_Image_Manipulation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bowen Li,  Xiaojuan Qi,  Thomas Lukasiewicz,  Philip H.S. Torr</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1506</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0421</td></tr>
<tr><td>Watch Your Up-Convolution: CNN Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Durall_Watch_Your_Up-Convolution_CNN_Based_Generative_Deep_Neural_Networks_Are_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ricard Durall,  Margret Keuper,  Janis Keuper</td> <td>0.0000</td> <td>0.0000</td> <td>0.0523</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0494</td> <td>0.0000</td></tr>
<tr><td>Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Knobelreiter_Belief_Propagation_Reloaded_Learning_BP-Layers_for_Labeling_Problems_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Patrick Knobelreiter,  Christian Sormann,  Alexander Shekhovtsov,  Friedrich Fraundorfer,  Thomas Pock</td> <td>0.0000</td> <td>0.0000</td> <td>0.1210</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0663</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Barycenters of Natural Images  Constrained Wasserstein Barycenters for Image Morphing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Barycenters_of_Natural_Images__Constrained_Wasserstein_Barycenters_for_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dror Simon,  Aviad Aberdam</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2696</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>Guided Variational Autoencoder for Disentanglement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ding_Guided_Variational_Autoencoder_for_Disentanglement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zheng Ding,  Yifan Xu,  Weijian Xu,  Gaurav Parmar,  Yang Yang,  Max Welling,  Zhuowen Tu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cross-Spectral Face Hallucination via Disentangling Independent Factors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Duan_Cross-Spectral_Face_Hallucination_via_Disentangling_Independent_Factors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Boyan Duan,  Chaoyou Fu,  Yi Li,  Xingguang Song,  Ran He</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0034</td></tr>
<tr><td>Learned Image Compression With Discretized Gaussian Mixture Likelihoods and Attention Modules <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Learned_Image_Compression_With_Discretized_Gaussian_Mixture_Likelihoods_and_Attention_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengxue Cheng,  Heming Sun,  Masaru Takeuchi,  Jiro Katto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0711</td></tr>
<tr><td>C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pumarola_C-Flow_Conditional_Generative_Flow_Models_for_Images_and_3D_Point_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Albert Pumarola,  Stefan Popov,  Francesc Moreno-Noguer,  Vittorio Ferrari</td> <td>0.0000</td> <td>0.1145</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cogradient Descent for Bilinear Optimization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhuo_Cogradient_Descent_for_Bilinear_Optimization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li'an Zhuo,  Baochang Zhang,  Linlin Yang,  Hanlin Chen,  Qixiang Ye,  David Doermann,  Rongrong Ji,  Guodong Guo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Instance-Aware Image Colorization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Instance-Aware_Image_Colorization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jheng-Wei Su,  Hung-Kuo Chu,  Jia-Bin Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0323</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0268</td></tr>
<tr><td>Joint Training of Variational Auto-Encoder and Latent Energy-Based Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_Joint_Training_of_Variational_Auto-Encoder_and_Latent_Energy-Based_Model_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tian Han,  Erik Nijkamp,  Linqi Zhou,  Bo Pang,  Song-Chun Zhu,  Ying Nian Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adaptive Loss-Aware Quantization for Multi-Bit Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qu_Adaptive_Loss-Aware_Quantization_for_Multi-Bit_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhongnan Qu,  Zimu Zhou,  Yun Cheng,  Lothar Thiele</td> <td>0.0000</td> <td>0.0000</td> <td>0.0634</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0131</td></tr>
<tr><td>ScopeFlow: Dynamic Scene Scoping for Optical Flow <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bar-Haim_ScopeFlow_Dynamic_Scene_Scoping_for_Optical_Flow_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aviram Bar-Haim,  Lior Wolf</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0851</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Video Super-Resolution With Temporal Group Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Isobe_Video_Super-Resolution_With_Temporal_Group_Attention_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Takashi Isobe,  Songjiang Li,  Xu Jia,  Shanxin Yuan,  Gregory Slabaugh,  Chunjing Xu,  Ya-Li Li,  Shengjin Wang,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0353</td></tr>
<tr><td>Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Group_Sparsity_The_Hinge_Between_Filter_Pruning_and_Decomposition_for_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yawei Li,  Shuhang Gu,  Christoph Mayer,  Luc Van Gool,  Radu Timofte</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0547</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>3D Photography Using Context-Aware Layered Depth Inpainting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shih_3D_Photography_Using_Context-Aware_Layered_Depth_Inpainting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Meng-Li Shih,  Shih-Yang Su,  Johannes Kopf,  Jia-Bin Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_MixNMatch_Multifactor_Disentanglement_and_Encoding_for_Conditional_Image_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuheng Li,  Krishna Kumar Singh,  Utkarsh Ojha,  Yong Jae Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0167</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0037</td></tr>
<tr><td>Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Idelbayev_Low-Rank_Compression_of_Neural_Nets_Learning_the_Rank_of_Each_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yerlan Idelbayev,  Miguel A. Carreira-Perpinan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0568</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Global Texture Enhancement for Fake Face Detection in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Global_Texture_Enhancement_for_Fake_Face_Detection_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengzhe Liu,  Xiaojuan Qi,  Philip H.S. Torr</td> <td>0.0000</td> <td>0.0000</td> <td>0.0256</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1334</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Panoptic-Based Image Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dundar_Panoptic-Based_Image_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aysegul Dundar,  Karan Sapra,  Guilin Liu,  Andrew Tao,  Bryan Catanzaro</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0296</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0466</td></tr>
<tr><td>Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Srinivasan_Lighthouse_Predicting_Lighting_Volumes_for_Spatially-Coherent_Illumination_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pratul P. Srinivasan,  Ben Mildenhall,  Matthew Tancik,  Jonathan T. Barron,  Richard Tucker,  Noah Snavely</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Cartoonize Using White-Box Cartoon Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Learning_to_Cartoonize_Using_White-Box_Cartoon_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xinrui Wang,  Jinze Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1383</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0034</td></tr>
<tr><td>End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Chen,  Alvaro Parra,  Jiewei Cao,  Nan Li,  Tat-Jun Chin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0132</td> <td>0.0000</td> <td>0.0000</td> <td>0.0261</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0115</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Analyzing and Improving the Image Quality of StyleGAN <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tero Karras,  Samuli Laine,  Miika Aittala,  Janne Hellsten,  Jaakko Lehtinen,  Timo Aila</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0058</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fashion Editing With Adversarial Parsing Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Fashion_Editing_With_Adversarial_Parsing_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haoye Dong,  Xiaodan Liang,  Yixuan Zhang,  Xujie Zhang,  Xiaohui Shen,  Zhenyu Xie,  Bowen Wu,  Jian Yin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0701</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0386</td></tr>
<tr><td>Augment Your Batch: Improving Generalization Through Instance Repetition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hoffer_Augment_Your_Batch_Improving_Generalization_Through_Instance_Repetition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Elad Hoffer,  Tal Ben-Nun,  Itay Hubara,  Niv Giladi,  Torsten Hoefler,  Daniel Soudry</td> <td>0.0000</td> <td>0.0000</td> <td>0.0309</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ARShadowGAN_Shadow_Generative_Adversarial_Network_for_Augmented_Reality_in_Single_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daquan Liu,  Chengjiang Long,  Hongpan Zhang,  Hanning Yu,  Xinzhi Dong,  Chunxia Xiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0604</td> <td>0.0000</td> <td>0.0954</td> <td>0.0000</td> <td>0.0000</td> <td>0.0287</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0080</td></tr>
<tr><td>An End-to-End Edge Aggregation Network for Moving Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Patil_An_End-to-End_Edge_Aggregation_Network_for_Moving_Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Prashant W. Patil,  Kuldeep M. Biradar,  Akshay Dudhane,  Subrahmanyam Murala</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0412</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0463</td></tr>
<tr><td>Learning Video Stabilization Using Optical Flow <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Learning_Video_Stabilization_Using_Optical_Flow_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiyang Yu,  Ravi Ramamoorthi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0062</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2416</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Reusing_Discriminators_for_Encoding_Towards_Unsupervised_Image-to-Image_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Runfa Chen,  Wenbing Huang,  Binghui Huang,  Fuchun Sun,  Bin Fang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0445</td> <td>0.0000</td> <td>0.0698</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0527</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0152</td></tr>
<tr><td>Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rahnama_Robust_Design_of_Deep_Neural_Networks_Against_Adversarial_Attacks_Based_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Arash Rahnama,  Andre T. Nguyen,  Edward Raff</td> <td>0.0000</td> <td>0.0000</td> <td>0.0417</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0657</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>StarGAN v2: Diverse Image Synthesis for Multiple Domains <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_StarGAN_v2_Diverse_Image_Synthesis_for_Multiple_Domains_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunjey Choi,  Youngjung Uh,  Jaejun Yoo,  Jung-Woo Ha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0106</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Warping Residual Based Image Stitching for Large Parallax <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Warping_Residual_Based_Image_Stitching_for_Large_Parallax_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kyu-Yul Lee,  Jae-Young Sim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0585</td></tr>
<tr><td>A U-Net Based Discriminator for Generative Adversarial Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Schonfeld_A_U-Net_Based_Discriminator_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Edgar Schonfeld,  Bernt Schiele,  Anna Khoreva</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1948</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unpaired Portrait Drawing Generation via Asymmetric Cycle Mapping <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yi_Unpaired_Portrait_Drawing_Generation_via_Asymmetric_Cycle_Mapping_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ran Yi,  Yong-Jin Liu,  Yu-Kun Lai,  Paul L. Rosin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0568</td> <td>0.0413</td></tr>
<tr><td>When to Use Convolutional Neural Networks for Inverse Problems <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chodosh_When_to_Use_Convolutional_Neural_Networks_for_Inverse_Problems_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nathaniel Chodosh,  Simon Lucey</td> <td>0.0000</td> <td>0.0000</td> <td>0.0444</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0576</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and Visibility Likelihood <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kumar_LUVLi_Face_Alignment_Estimating_Landmarks_Location_Uncertainty_and_Visibility_Likelihood_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abhinav Kumar,  Tim K. Marks,  Wenxuan Mou,  Ye Wang,  Michael Jones,  Anoop Cherian,  Toshiaki Koike-Akino,  Xiaoming Liu,  Chen Feng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Affinity Graph Supervision for Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Affinity_Graph_Supervision_for_Visual_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chu Wang,  Babak Samari,  Vladimir G. Kim,  Siddhartha Chaudhuri,  Kaleem Siddiqi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0542</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Magnification of Posture Deviations Across Subjects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dorkenwald_Unsupervised_Magnification_of_Posture_Deviations_Across_Subjects_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michael Dorkenwald,  Uta Buchler,  Bjorn Ommer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td></tr>
<tr><td>Accurate Estimation of Body Height From a Single Depth Image via a Four-Stage Developing Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Accurate_Estimation_of_Body_Height_From_a_Single_Depth_Image_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fukun Yin,  Shizhe Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0391</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fast Soft Color Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Akimoto_Fast_Soft_Color_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Naofumi Akimoto,  Huachun Zhu,  Yanghua Jin,  Yoshimitsu Aoki</td> <td>0.0000</td> <td>0.0000</td> <td>0.0107</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0265</td></tr>
<tr><td>Global Optimality for Point Set Registration Using Semidefinite Programming <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Iglesias_Global_Optimality_for_Point_Set_Registration_Using_Semidefinite_Programming_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jose Pedro Iglesias,  Carl Olsson,  Fredrik Kahl</td> <td>0.0000</td> <td>0.1869</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Image2StyleGAN++: How to Edit the Embedded Images? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abdal_Image2StyleGAN_How_to_Edit_the_Embedded_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rameen Abdal,  Yipeng Qin,  Peter Wonka</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_SQE_a_Self_Quality_Evaluation_Metric_for_Parameters_Optimization_in_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanru Huang,  Feiyu Zhu,  Zheni Zeng,  Xi Qiu,  Yuan Shen,  Jianan Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>EventSR: From Asynchronous Events to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_EventSR_From_Asynchronous_Events_to_Image_Reconstruction_Restoration_and_Super-Resolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lin Wang,  Tae-Kyun Kim,  Kuk-Jin Yoon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0094</td></tr>
<tr><td>Hierarchical Pyramid Diverse Attention Networks for Face Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Hierarchical_Pyramid_Diverse_Attention_Networks_for_Face_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiangchang Wang,  Tianyi Wu,  He Zheng,  Guodong Guo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td></tr>
<tr><td>RGBD-Dog: Predicting Canine Pose from RGBD Sensors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kearney_RGBD-Dog_Predicting_Canine_Pose_from_RGBD_Sensors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sinead Kearney,  Wenbin Li,  Martin Parsons,  Kwang In Kim,  Darren Cosker</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0356</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0270</td> <td>0.0000</td></tr>
<tr><td>Multi-Scale Progressive Fusion Network for Single Image Deraining <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Multi-Scale_Progressive_Fusion_Network_for_Single_Image_Deraining_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kui Jiang,  Zhongyuan Wang,  Peng Yi,  Chen Chen,  Baojin Huang,  Yimin Luo,  Jiayi Ma,  Junjun Jiang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0422</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0145</td></tr>
<tr><td>Learning a Neural 3D Texture Space From 2D Exemplars <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Henzler_Learning_a_Neural_3D_Texture_Space_From_2D_Exemplars_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Philipp Henzler,  Niloy J. Mitra,  Tobias Ritschel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>BachGAN: High-Resolution Image Synthesis From Salient Object Layout <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_BachGAN_High-Resolution_Image_Synthesis_From_Salient_Object_Layout_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yandong Li,  Yu Cheng,  Zhe Gan,  Licheng Yu,  Liqiang Wang,  Jingjing Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1538</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jaejun Yoo,  Namhyuk Ahn,  Kyung-Ah Sohn</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0259</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On Positive-Unlabeled Classification in GAN <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_On_Positive-Unlabeled_Classification_in_GAN_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianyu Guo,  Chang Xu,  Jiajun Huang,  Yunhe Wang,  Boxin Shi,  Chao Xu,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DoveNet: Deep Image Harmonization via Domain Verification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cong_DoveNet_Deep_Image_Harmonization_via_Domain_Verification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenyan Cong,  Jianfu Zhang,  Li Niu,  Liu Liu,  Zhixin Ling,  Weiyuan Li,  Liqing Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0409</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0302</td></tr>
<tr><td>Noise Robust Generative Adversarial Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kaneko_Noise_Robust_Generative_Adversarial_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Takuhiro Kaneko,  Tatsuya Harada</td> <td>0.0000</td> <td>0.0000</td> <td>0.0110</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0518</td> <td>0.0000</td> <td>0.1667</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0055</td></tr>
<tr><td>Normalizing Flows With Multi-Scale Autoregressive Priors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bhattacharyya_Normalizing_Flows_With_Multi-Scale_Autoregressive_Priors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Apratim Bhattacharyya,  Shweta Mahajan,  Mario Fritz,  Bernt Schiele,  Stefan Roth</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0399</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Robust Reference-Based Super-Resolution With Similarity-Aware Deformable Convolution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shim_Robust_Reference-Based_Super-Resolution_With_Similarity-Aware_Deformable_Convolution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gyumin Shim,  Jinsun Park,  In So Kweon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0680</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0360</td></tr>
<tr><td>Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Painting_Many_Pasts_Synthesizing_Time_Lapse_Videos_of_Paintings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Amy Zhao,  Guha Balakrishnan,  Kathleen M. Lewis,  Fredo Durand,  John V. Guttag,  Adrian V. Dalca</td> <td>0.0000</td> <td>0.0000</td> <td>0.0335</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GeoDA: A Geometric Framework for Black-Box Adversarial Attacks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rahmati_GeoDA_A_Geometric_Framework_for_Black-Box_Adversarial_Attacks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ali Rahmati,  Seyed-Mohsen Moosavi-Dezfooli,  Pascal Frossard,  Huaiyu Dai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1986</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GAMIN: Generative Adversarial Multiple Imputation Network for Highly Missing Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yoon_GAMIN_Generative_Adversarial_Multiple_Imputation_Network_for_Highly_Missing_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Seongwook Yoon,  Sanghoon Sull</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0968</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0051</td></tr>
<tr><td>An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks by Unitizing Layers' Outputs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_An_Internal_Covariate_Shift_Bounding_Algorithm_for_Deep_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>You Huang,  Yuanlong Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0252</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Unified Optimization Framework for Low-Rank Inducing Penalties <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ornhag_A_Unified_Optimization_Framework_for_Low-Rank_Inducing_Penalties_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Marcus Valtonen Ornhag,  Carl Olsson</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Single-Side Domain Generalization for Face Anti-Spoofing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jia_Single-Side_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunpei Jia,  Jie Zhang,  Shiguang Shan,  Xilin Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0154</td></tr>
<tr><td>The Knowledge Within: Methods for Data-Free Model Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Haroush_The_Knowledge_Within_Methods_for_Data-Free_Model_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matan Haroush,  Itay Hubara,  Elad Hoffer,  Daniel Soudry</td> <td>0.0000</td> <td>0.0000</td> <td>0.0387</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0342</td> <td>0.0000</td></tr>
<tr><td>Scale-Space Flow for End-to-End Optimized Video Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Agustsson_Scale-Space_Flow_for_End-to-End_Optimized_Video_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eirikur Agustsson,  David Minnen,  Nick Johnston,  Johannes Balle,  Sung Jin Hwang,  George Toderici</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2176</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0094</td></tr>
<tr><td>Dynamic Neural Relational Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Graber_Dynamic_Neural_Relational_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Colin Graber,  Alexander G. Schwing</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Real-Time Panoptic Segmentation From Dense Detections <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Real-Time_Panoptic_Segmentation_From_Dense_Detections_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Hou,  Jie Li,  Arjun Bhargava,  Allan Raventos,  Vitor Guizilini,  Chao Fang,  Jerome Lynch,  Adrien Gaidon</td> <td>0.0187</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0110</td> <td>0.0000</td> <td>0.1216</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Snake for Real-Time Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_Deep_Snake_for_Real-Time_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sida Peng,  Wen Jiang,  Huaijin Pi,  Xiuli Li,  Hujun Bao,  Xiaowei Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0071</td> <td>0.0000</td> <td>0.0930</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0383</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AdaCoSeg: Adaptive Shape Co-Segmentation With Group Consistency Loss <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_AdaCoSeg_Adaptive_Shape_Co-Segmentation_With_Group_Consistency_Loss_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenyang Zhu,  Kai Xu,  Siddhartha Chaudhuri,  Li Yi,  Leonidas J. Guibas,  Hao Zhang</td> <td>0.0000</td> <td>0.0342</td> <td>0.0417</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Dynamic Routing for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Learning_Dynamic_Routing_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanwei Li,  Lin Song,  Yukang Chen,  Zeming Li,  Xiangyu Zhang,  Xingang Wang,  Jian Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0714</td> <td>0.0000</td> <td>0.0000</td> <td>0.0779</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Boosting Semantic Human Matting With Coarse Annotations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Boosting_Semantic_Human_Matting_With_Coarse_Annotations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinlin Liu,  Yuan Yao,  Wendi Hou,  Miaomiao Cui,  Xuansong Xie,  Changshui Zhang,  Xian-Sheng Hua</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0261</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0497</td></tr>
<tr><td>BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_BlendMask_Top-Down_Meets_Bottom-Up_for_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hao Chen,  Kunyang Sun,  Zhi Tian,  Chunhua Shen,  Yongming Huang,  Youliang Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2604</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0057</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jing Zhang,  Deng-Ping Fan,  Yuchao Dai,  Saeed Anwar,  Fatemeh Sadat Saleh,  Tong Zhang,  Nick Barnes</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td></tr>
<tr><td>Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Donati_Deep_Geometric_Functional_Maps_Robust_Feature_Learning_for_Shape_Correspondence_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nicolas Donati,  Abhishek Sharma,  Maks Ovsjanikov</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0682</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0702</td> <td>0.0000</td></tr>
<tr><td>Deep Polarization Cues for Transparent Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kalra_Deep_Polarization_Cues_for_Transparent_Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Agastya Kalra,  Vage Taamazyan,  Supreeth Krishna Rao,  Kartik Venkataraman,  Ramesh Raskar,  Achuta Kadambi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0454</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0318</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DualConvMesh-Net: Joint Geodesic and Euclidean Convolutions on 3D Meshes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Schult_DualConvMesh-Net_Joint_Geodesic_and_Euclidean_Convolutions_on_3D_Meshes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jonas Schult,  Francis Engelmann,  Theodora Kontogianni,  Bastian Leibe</td> <td>0.0000</td> <td>0.0070</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0475</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>F-BRS: Rethinking Backpropagating Refinement for Interactive Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sofiiuk_F-BRS_Rethinking_Backpropagating_Refinement_for_Interactive_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Konstantin Sofiiuk,  Ilia Petrov,  Olga Barinova,  Anton Konushin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0301</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0091</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Approximating shapes in images with low-complexity polygons <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Approximating_shapes_in_images_with_low-complexity_polygons_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muxingzi Li,  Florent Lafarge,  Renaud Marlet</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Visually Explaining Variational Autoencoders <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenqian Liu,  Runze Li,  Meng Zheng,  Srikrishna Karanam,  Ziyan Wu,  Bir Bhanu,  Richard J. Radke,  Octavia Camps</td> <td>0.0000</td> <td>0.0000</td> <td>0.0366</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Global Explanations of Convolutional Neural Networks With Concept Attribution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Towards_Global_Explanations_of_Convolutional_Neural_Networks_With_Concept_Attribution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weibin Wu,  Yuxin Su,  Xixian Chen,  Shenglin Zhao,  Irwin King,  Michael R. Lyu,  Yu-Wing Tai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0600</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0240</td></tr>
<tr><td>Interpretable and Accurate Fine-grained Recognition via Region Grouping <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Interpretable_and_Accurate_Fine-grained_Recognition_via_Region_Grouping_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zixuan Huang,  Yin Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0570</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0019</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td></tr>
<tr><td>SAM: The Sensitivity of Attribution Methods to Hyperparameters <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bansal_SAM_The_Sensitivity_of_Attribution_Methods_to_Hyperparameters_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Naman Bansal,  Chirag Agarwal,  Anh Nguyen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haohan Wang,  Xindi Wu,  Zeyi Huang,  Eric P. Xing</td> <td>0.0000</td> <td>0.0000</td> <td>0.0721</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1545</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_FALCON_A_Fourier_Transform_Based_Approach_for_Fast_and_Secure_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaohua Li,  Kaiping Xue,  Bin Zhu,  Chenkai Ding,  Xindi Gao,  David Wei,  Tao Wan</td> <td>0.0000</td> <td>0.0000</td> <td>0.1235</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hongxu Yin,  Pavlo Molchanov,  Jose M. Alvarez,  Zhizhong Li,  Arun Mallya,  Derek Hoiem,  Niraj K. Jha,  Jan Kautz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0335</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0179</td></tr>
<tr><td>Unsupervised Domain Adaptation via Structurally Regularized Deep Clustering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Unsupervised_Domain_Adaptation_via_Structurally_Regularized_Deep_Clustering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hui Tang,  Ke Chen,  Kui Jia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2443</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0202</td></tr>
<tr><td>HyperSTAR: Task-Aware Hyperparameters for Deep Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mittal_HyperSTAR_Task-Aware_Hyperparameters_for_Deep_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gaurav Mittal,  Chang Liu,  Nikolaos Karianakis,  Victor Fragoso,  Mei Chen,  Yun Fu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0817</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td></tr>
<tr><td>ActBERT: Learning Global-Local Video-Text Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_ActBERT_Learning_Global-Local_Video-Text_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Linchao Zhu,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>State-Relabeling Adversarial Active Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_State-Relabeling_Adversarial_Active_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Beichen Zhang,  Liang Li,  Shijie Yang,  Shuhui Wang,  Zheng-Jun Zha,  Qingming Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0005</td></tr>
<tr><td>Erasing Integrated Learning: A Simple Yet Effective Approach for Weakly Supervised Object Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mai_Erasing_Integrated_Learning_A_Simple_Yet_Effective_Approach_for_Weakly_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinjie Mai,  Meng Yang,  Wenfeng Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huynh_A_Shared_Multi-Attention_Framework_for_Multi-Label_Zero-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dat Huynh,  Ehsan Elhamifar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td></tr>
<tr><td>Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tomas Jakab,  Ankush Gupta,  Hakan Bilen,  Andrea Vedaldi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0094</td></tr>
<tr><td>Few-Shot Open-Set Recognition Using Meta-Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Few-Shot_Open-Set_Recognition_Using_Meta-Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Liu,  Hao Kang,  Haoxiang Li,  Gang Hua,  Nuno Vasconcelos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0019</td></tr>
<tr><td>Few-Shot Learning via Embedding Adaptation With Set-to-Set Functions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Few-Shot_Learning_via_Embedding_Adaptation_With_Set-to-Set_Functions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Han-Jia Ye,  Hexiang Hu,  De-Chuan Zhan,  Fei Sha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0228</td></tr>
<tr><td>Temporally Distributed Networks for Fast Video Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Temporally_Distributed_Networks_for_Fast_Video_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ping Hu,  Fabian Caba,  Oliver Wang,  Zhe Lin,  Stan Sclaroff,  Federico Perazzi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0562</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0128</td></tr>
<tr><td>Benchmarking the Robustness of Semantic Segmentation Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kamann_Benchmarking_the_Robustness_of_Semantic_Segmentation_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christoph Kamann,  Carsten Rother</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1324</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>There and Back Again: Revisiting Backpropagation Saliency Methods <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rebuffi_There_and_Back_Again_Revisiting_Backpropagation_Saliency_Methods_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sylvestre-Alvise Rebuffi,  Ruth Fong,  Xu Ji,  Andrea Vedaldi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Semantic Clustering by Partition Confidence Maximisation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Deep_Semantic_Clustering_by_Partition_Confidence_Maximisation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiabo Huang,  Shaogang Gong,  Xiatian Zhu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0127</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>StructEdit: Learning Structural Shape Variations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mo_StructEdit_Learning_Structural_Shape_Variations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaichun Mo,  Paul Guerrero,  Li Yi,  Hao Su,  Peter Wonka,  Niloy J. Mitra,  Leonidas J. Guibas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0064</td></tr>
<tr><td>Harmonizing Transferability and Discriminability for Adapting Object Detectors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Harmonizing_Transferability_and_Discriminability_for_Adapting_Object_Detectors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaoqi Chen,  Zebiao Zheng,  Xinghao Ding,  Yue Huang,  Qi Dou</td> <td>0.0293</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0659</td></tr>
<tr><td>Fast Video Object Segmentation With Temporal Aggregation Network and Dynamic Template Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Fast_Video_Object_Segmentation_With_Temporal_Aggregation_Network_and_Dynamic_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuhua Huang,  Jiarui Xu,  Yu-Wing Tai,  Chi-Keung Tang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0362</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ho Kei Cheng,  Jihoon Chung,  Yu-Wing Tai,  Chi-Keung Tang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0488</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0358</td> <td>0.0143</td></tr>
<tr><td>Correlating Edge, Pose With Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Correlating_Edge_Pose_With_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziwei Zhang,  Chi Su,  Liang Zheng,  Xiaodong Xie</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>VecRoad: Point-Based Iterative Graph Exploration for Road Graphs Extraction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_VecRoad_Point-Based_Iterative_Graph_Exploration_for_Road_Graphs_Extraction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yong-Qiang Tan,  Shang-Hua Gao,  Xuan-Yi Li,  Ming-Ming Cheng,  Bo Ren</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0349</td></tr>
<tr><td>Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Towards_Fairness_in_Visual_Recognition_Effective_Strategies_for_Bias_Mitigation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zeyu Wang,  Klint Qinami,  Ioannis Christos Karakozis,  Kyle Genova,  Prem Nair,  Kenji Hata,  Olga Russakovsky</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td> <td>0.0586</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0378</td> <td>0.0001</td></tr>
<tr><td>Hierarchical Human Parsing With Typed Part-Relation Reasoning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Hierarchical_Human_Parsing_With_Typed_Part-Relation_Reasoning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenguan Wang,  Hailong Zhu,  Jifeng Dai,  Yanwei Pang,  Jianbing Shen,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Compositional Convolutional Neural Networks: A Deep Architecture With Innate Robustness to Partial Occlusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kortylewski_Compositional_Convolutional_Neural_Networks_A_Deep_Architecture_With_Innate_Robustness_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Adam Kortylewski,  Ju He,  Qing Liu,  Alan L. Yuille</td> <td>0.0000</td> <td>0.0000</td> <td>0.0365</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Spatial Pyramid Based Graph Reasoning for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Spatial_Pyramid_Based_Graph_Reasoning_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xia Li,  Yibo Yang,  Qijie Zhao,  Tiancheng Shen,  Zhouchen Lin,  Hong Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1383</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0271</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0605</td></tr>
<tr><td>Learning Video Object Segmentation From Unlabeled Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Learning_Video_Object_Segmentation_From_Unlabeled_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiankai Lu,  Wenguan Wang,  Jianbing Shen,  Yu-Wing Tai,  David J. Crandall,  Steven C. H. Hoi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0024</td></tr>
<tr><td>Part-Aware Context Network for Human Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Part-Aware_Context_Network_for_Human_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaomei Zhang,  Yingying Chen,  Bingke Zhu,  Jinqiao Wang,  Ming Tang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SCOUT: Self-Aware Discriminant Counterfactual Explanations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_SCOUT_Self-Aware_Discriminant_Counterfactual_Explanations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pei Wang,  Nuno Vasconcelos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly-Supervised Semantic Segmentation via Sub-Category Exploration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chang_Weakly-Supervised_Semantic_Segmentation_via_Sub-Category_Exploration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu-Ting Chang,  Qiaosong Wang,  Wei-Chih Hung,  Robinson Piramuthu,  Yi-Hsuan Tsai,  Ming-Hsuan Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0388</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0170</td></tr>
<tr><td>Continual Learning With Extended Kronecker-Factored Approximate Curvature <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Continual_Learning_With_Extended_Kronecker-Factored_Approximate_Curvature_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Janghyeon Lee,  Hyeong Gwon Hong,  Donggyu Joo,  Junmo Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0644</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Phase Consistent Ecological Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Phase_Consistent_Ecological_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanchao Yang,  Dong Lao,  Ganesh Sundaramoorthi,  Stefano Soatto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0351</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1052</td> <td>0.2311</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhai_AD-Cluster_Augmented_Discriminative_Clustering_for_Domain_Adaptive_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunpeng Zhai,  Shijian Lu,  Qixiang Ye,  Xuebo Shan,  Jie Chen,  Rongrong Ji,  Yonghong Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1467</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0177</td></tr>
<tr><td>3D-MPA: Multi-Proposal Aggregation for 3D Semantic Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Engelmann_3D-MPA_Multi-Proposal_Aggregation_for_3D_Semantic_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Francis Engelmann,  Martin Bokeloh,  Alireza Fathi,  Bastian Leibe,  Matthias Niessner</td> <td>0.0933</td> <td>0.0769</td> <td>0.0000</td> <td>0.0000</td> <td>0.1185</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td></tr>
<tr><td>Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gudovskiy_Deep_Active_Learning_for_Biased_Datasets_via_Fisher_Kernel_Self-Supervision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Denis Gudovskiy,  Alec Hodgkinson,  Takuya Yamaguchi,  Sotaro Tsukizawa</td> <td>0.0000</td> <td>0.0000</td> <td>0.0434</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0551</td></tr>
<tr><td>Adaptive Graph Convolutional Network With Attention Graph Clustering for Co-Saliency Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Adaptive_Graph_Convolutional_Network_With_Attention_Graph_Clustering_for_Co-Saliency_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaihua Zhang,  Tengpeng Li,  Shiwen Shen,  Bo Liu,  Jin Chen,  Qingshan Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0106</td> <td>0.0000</td> <td>0.0045</td></tr>
<tr><td>A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Piao_A2dele_Adaptive_and_Attentive_Depth_Distiller_for_Efficient_RGB-D_Salient_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yongri Piao,  Zhengkun Rong,  Miao Zhang,  Weisong Ren,  Huchuan Lu</td> <td>0.0154</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Fair Clustering for Visual Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deep_Fair_Clustering_for_Visual_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peizhao Li,  Han Zhao,  Hongfu Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0101</td></tr>
<tr><td>Bidirectional Graph Reasoning Network for Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Bidirectional_Graph_Reasoning_Network_for_Panoptic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yangxin Wu,  Gengwei Zhang,  Yiming Gao,  Xiajun Deng,  Ke Gong,  Xiaodan Liang,  Liang Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0891</td> <td>0.0000</td> <td>0.1259</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0148</td></tr>
<tr><td>Exploit Clues From Views: Self-Supervised and Regularized Learning for Multiview Object Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ho_Exploit_Clues_From_Views_Self-Supervised_and_Regularized_Learning_for_Multiview_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chih-Hui Ho,  Bo Liu,  Tz-Ying Wu,  Nuno Vasconcelos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0530</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Spherical Space Domain Adaptation With Robust Pseudo-Label Loss <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Spherical_Space_Domain_Adaptation_With_Robust_Pseudo-Label_Loss_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiang Gu,  Jian Sun,  Zongben Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0548</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0042</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0328</td></tr>
<tr><td>Stochastic Classifiers for Unsupervised Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Stochastic_Classifiers_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihe Lu,  Yongxin Yang,  Xiatian Zhu,  Cong Liu,  Yi-Zhe Song,  Tao Xiang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0434</td> <td>0.1917</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Learning of Intrinsic Structural Representation Points <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nenglun Chen,  Lingjie Liu,  Zhiming Cui,  Runnan Chen,  Duygu Ceylan,  Changhe Tu,  Wenping Wang</td> <td>0.0000</td> <td>0.0540</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0301</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0102</td></tr>
<tr><td>PolyTransform: Deep Polygon Transformer for Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liang_PolyTransform_Deep_Polygon_Transformer_for_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Justin Liang,  Namdar Homayounfar,  Wei-Chiu Ma,  Yuwen Xiong,  Rui Hu,  Raquel Urtasun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2322</td> <td>0.0000</td> <td>0.0030</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td></tr>
<tr><td>Interactive Two-Stream Decoder for Accurate and Fast Saliency Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Interactive_Two-Stream_Decoder_for_Accurate_and_Fast_Saliency_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huajun Zhou,  Xiaohua Xie,  Jian-Huang Lai,  Zixuan Chen,  Lingxiao Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0217</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0026</td></tr>
<tr><td>Towards Better Generalization: Joint Depth-Pose Learning Without PoseNet <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Towards_Better_Generalization_Joint_Depth-Pose_Learning_Without_PoseNet_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wang Zhao,  Shaohui Liu,  Yezhi Shu,  Yong-Jin Liu</td> <td>0.0000</td> <td>0.0271</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0224</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0447</td> <td>0.0000</td> <td>0.0000</td> <td>0.0543</td> <td>0.0000</td> <td>0.0000</td> <td>0.0615</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0122</td></tr>
<tr><td>LT-Net: Label Transfer by Learning Reversible Voxel-Wise Correspondence for One-Shot Medical Image Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_LT-Net_Label_Transfer_by_Learning_Reversible_Voxel-Wise_Correspondence_for_One-Shot_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuxin Wang,  Shilei Cao,  Dong Wei,  Renzhen Wang,  Kai Ma,  Liansheng Wang,  Deyu Meng,  Yefeng Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0046</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FGN: Fully Guided Network for Few-Shot Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_FGN_Fully_Guided_Network_for_Few-Shot_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhibo Fan,  Jin-Gang Yu,  Zhihao Liang,  Jiarong Ou,  Changxin Gao,  Gui-Song Xia,  Yuanqing Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3600</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0233</td></tr>
<tr><td>A Quantum Computational Approach to Correspondence Problems on Point Sets <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Golyanik_A_Quantum_Computational_Approach_to_Correspondence_Problems_on_Point_Sets_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vladislav Golyanik,  Christian Theobalt</td> <td>0.0000</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0633</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Data-Efficient Semi-Supervised Learning by Reliable Edge Mining <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Data-Efficient_Semi-Supervised_Learning_by_Reliable_Edge_Mining_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peibin Chen,  Tao Ma,  Xu Qin,  Weidi Xu,  Shuchang Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>NestedVAE: Isolating Common Factors via Weak Supervision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Vowels_NestedVAE_Isolating_Common_Factors_via_Weak_Supervision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matthew J. Vowels,  Necati Cihan Camgoz,  Richard Bowden</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Progressive Adversarial Networks for Fine-Grained Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Progressive_Adversarial_Networks_for_Fine-Grained_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sinan Wang,  Xinyang Chen,  Yunbo Wang,  Mingsheng Long,  Jianmin Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1422</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0214</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Disentangling Invertible Interpretation Network for Explaining Latent Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Esser_A_Disentangling_Invertible_Interpretation_Network_for_Explaining_Latent_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Patrick Esser,  Robin Rombach,  Bjorn Ommer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0106</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0297</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0301</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Modeling the Background for Incremental Learning in Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cermelli_Modeling_the_Background_for_Incremental_Learning_in_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fabio Cermelli,  Massimiliano Mancini,  Samuel Rota Bulo,  Elisa Ricci,  Barbara Caputo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0956</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>Interpreting the Latent Space of GANs for Semantic Face Editing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Interpreting_the_Latent_Space_of_GANs_for_Semantic_Face_Editing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yujun Shen,  Jinjin Gu,  Xiaoou Tang,  Bolei Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2090</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0184</td></tr>
<tr><td>Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_Super-BPD_Super_Boundary-to-Pixel_Direction_for_Fast_Image_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianqiang Wan,  Yang Liu,  Donglai Wei,  Xiang Bai,  Yongchao Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0091</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0202</td></tr>
<tr><td>Self-Learning With Rectification Strategy for Human Parsing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Self-Learning_With_Rectification_Strategy_for_Human_Parsing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tao Li,  Zhiyuan Liang,  Sanyuan Zhao,  Jiahao Gong,  Jianbing Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0568</td></tr>
<tr><td>Hyperbolic Visual Embedding Learning for Zero-Shot Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaoteng Liu,  Jingjing Chen,  Liangming Pan,  Chong-Wah Ngo,  Tat-Seng Chua,  Yu-Gang Jiang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0155</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0051</td></tr>
<tr><td>Sequential Mastery of Multiple Visual Tasks: Networks Naturally Learn to Learn and Forget to Forget <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Davidson_Sequential_Mastery_of_Multiple_Visual_Tasks_Networks_Naturally_Learn_to_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guy Davidson,  Michael C. Mozer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0084</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Distilling Effective Supervision From Severe Label Noise <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Distilling_Effective_Supervision_From_Severe_Label_Noise_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zizhao Zhang,  Han Zhang,  Sercan O. Arik,  Honglak Lee,  Tomas Pfister</td> <td>0.0000</td> <td>0.0000</td> <td>0.0424</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Golatkar_Eternal_Sunshine_of_the_Spotless_Net_Selective_Forgetting_in_Deep_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aditya Golatkar,  Alessandro Achille,  Stefano Soatto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0986</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0433</td> <td>0.0000</td></tr>
<tr><td>CenterMask: Single Shot Instance Segmentation With Point Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_CenterMask_Single_Shot_Instance_Segmentation_With_Point_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuqing Wang,  Zhaoliang Xu,  Hao Shen,  Baoshan Cheng,  Lirong Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3104</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Mitigating Bias in Face Recognition Using Skewness-Aware Reinforcement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Mitigating_Bias_in_Face_Recognition_Using_Skewness-Aware_Reinforcement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mei Wang,  Weihong Deng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0068</td></tr>
<tr><td>MineGAN: Effective Knowledge Transfer From GANs to Target Domains With Few Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_MineGAN_Effective_Knowledge_Transfer_From_GANs_to_Target_Domains_With_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaxing Wang,  Abel Gonzalez-Garcia,  David Berga,  Luis Herranz,  Fahad Shahbaz Khan,  Joost van de Weijer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0322</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0868</td> <td>0.0000</td> <td>0.0490</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0295</td> <td>0.0247</td></tr>
<tr><td>DLWL: Improving Detection for Lowshot Classes With Weakly Labelled Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ramanathan_DLWL_Improving_Detection_for_Lowshot_Classes_With_Weakly_Labelled_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vignesh Ramanathan,  Rui Wang,  Dhruv Mahajan</td> <td>0.0013</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0048</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Deep Shape Descriptor With Point Distribution Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Unsupervised_Deep_Shape_Descriptor_With_Point_Distribution_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yi Shi,  Mengchen Xu,  Shuaihang Yuan,  Yi Fang</td> <td>0.0000</td> <td>0.0423</td> <td>0.0383</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0275</td></tr>
<tr><td>Stylization-Based Architecture for Fast Deep Exemplar Colorization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Stylization-Based_Architecture_for_Fast_Deep_Exemplar_Colorization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhongyou Xu,  Tingting Wang,  Faming Fang,  Yun Sheng,  Guixu Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0327</td></tr>
<tr><td>Cars Can't Fly Up in the Sky: Improving Urban-Scene Segmentation via Height-Driven Attention Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_Cars_Cant_Fly_Up_in_the_Sky_Improving_Urban-Scene_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sungha Choi,  Joanne T. Kim,  Jaegul Choo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1240</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>State-Aware Tracker for Real-Time Video Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_State-Aware_Tracker_for_Real-Time_Video_Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xi Chen,  Zuoxin Li,  Ye Yuan,  Gang Yu,  Jianxin Shen,  Donglian Qi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Iteratively-Refined Interactive 3D Medical Image Segmentation With Multi-Agent Reinforcement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_Iteratively-Refined_Interactive_3D_Medical_Image_Segmentation_With_Multi-Agent_Reinforcement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuan Liao,  Wenhao Li,  Qisen Xu,  Xiangfeng Wang,  Bo Jin,  Xiaoyun Zhang,  Yanfeng Wang,  Ya Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0172</td></tr>
<tr><td>ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic Convolution for Privacy-Preserving Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bian_ENSEI_Efficient_Secure_Inference_via_Frequency-Domain_Homomorphic_Convolution_for_Privacy-Preserving_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Song Bian,  Tianchen Wang,  Masayuki Hiromoto,  Yiyu Shi,  Takashi Sato</td> <td>0.0000</td> <td>0.0000</td> <td>0.0311</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-Scale Interactive Network for Salient Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_Multi-Scale_Interactive_Network_for_Salient_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Youwei Pang,  Xiaoqi Zhao,  Lihe Zhang,  Huchuan Lu</td> <td>0.0304</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0719</td></tr>
<tr><td>Interactive Multi-Label CNN Learning With Partial Labels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huynh_Interactive_Multi-Label_CNN_Learning_With_Partial_Labels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dat Huynh,  Ehsan Elhamifar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0269</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0253</td> <td>0.0096</td></tr>
<tr><td>ViewAL: Active Learning With Viewpoint Entropy for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Siddiqui_ViewAL_Active_Learning_With_Viewpoint_Entropy_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yawar Siddiqui,  Julien Valentin,  Matthias Niessner</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0334</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0185</td></tr>
<tr><td>Scene-Adaptive Video Frame Interpolation via Meta-Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_Scene-Adaptive_Video_Frame_Interpolation_via_Meta-Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Myungsub Choi,  Janghoon Choi,  Sungyong Baik,  Tae Hyun Kim,  Kyoung Mu Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Action Segmentation With Joint Self-Supervised Temporal Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Action_Segmentation_With_Joint_Self-Supervised_Temporal_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Min-Hung Chen,  Baopu Li,  Yingze Bao,  Ghassan AlRegib,  Zsolt Kira</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0824</td> <td>0.0000</td> <td>0.0554</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0265</td> <td>0.0099</td></tr>
<tr><td>Pixel Consensus Voting for Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Pixel_Consensus_Voting_for_Panoptic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haochen Wang,  Ruotian Luo,  Michael Maire,  Greg Shakhnarovich</td> <td>0.0000</td> <td>0.0000</td> <td>0.0535</td> <td>0.0000</td> <td>0.2810</td> <td>0.0000</td> <td>0.0919</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Minimizing Discrete Total Curvature for Image Processing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhong_Minimizing_Discrete_Total_Curvature_for_Image_Processing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiuxiang Zhong,  Yutong Li,  Yijie Yang,  Yuping Duan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0137</td></tr>
<tr><td>Towards Robust Image Classification Using Sequential Attention Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zoran_Towards_Robust_Image_Classification_Using_Sequential_Attention_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daniel Zoran,  Mike Chrzanowski,  Po-Sen Huang,  Sven Gowal,  Alex Mott,  Pushmeet Kohli</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2392</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Discovering Synchronized Subsets of Sequences: A Large Scale Solution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sariyanidi_Discovering_Synchronized_Subsets_of_Sequences_A_Large_Scale_Solution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Evangelos Sariyanidi,  Casey J. Zampella,  Keith G. Bartley,  John D. Herrington,  Theodore D. Satterthwaite,  Robert T. Schultz,  Birkan Tunc</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0340</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0514</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Going Deeper With Lean Point Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Going_Deeper_With_Lean_Point_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eric-Tuan Le,  Iasonas Kokkinos,  Niloy J. Mitra</td> <td>0.0000</td> <td>0.0584</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Efficient and Robust Shape Correspondence via Sparsity-Enforced Quadratic Assignment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiang_Efficient_and_Robust_Shape_Correspondence_via_Sparsity-Enforced_Quadratic_Assignment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Xiang,  Rongjie Lai,  Hongkai Zhao</td> <td>0.0000</td> <td>0.0190</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Explainable Object-Induced Action Decision for Autonomous Vehicles <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Explainable_Object-Induced_Action_Decision_for_Autonomous_Vehicles_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiran Xu,  Xiaoyin Yang,  Lihang Gong,  Hsuan-Chu Lin,  Tz-Ying Wu,  Yunsheng Li,  Nuno Vasconcelos</td> <td>0.0047</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0067</td></tr>
<tr><td>Spatially Attentive Output Layer for Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Spatially_Attentive_Output_Layer_for_Image_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ildoo Kim,  Woonhyuk Baek,  Sungwoong Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0450</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0215</td></tr>
<tr><td>Attack to Explain Deep Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jalwana_Attack_to_Explain_Deep_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohammad A. A. K. Jalwana,  Naveed Akhtar,  Mohammed Bennamoun,  Ajmal Mian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0163</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Computing Valid P-Values for Image Segmentation by Selective Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tanizaki_Computing_Valid_P-Values_for_Image_Segmentation_by_Selective_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kosuke Tanizaki,  Noriaki Hashimoto,  Yu Inatsu,  Hidekata Hontani,  Ichiro Takeuchi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0125</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0419</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Learning From Video With Deep Neural Embeddings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhuang_Unsupervised_Learning_From_Video_With_Deep_Neural_Embeddings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chengxu Zhuang,  Tianwei She,  Alex Andonian,  Max Sobol Mark,  Daniel Yamins</td> <td>0.0000</td> <td>0.0000</td> <td>0.0091</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0851</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Partial Weight Adaptation for Robust DNN Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Partial_Weight_Adaptation_for_Robust_DNN_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiufeng Xie,  Kyu-Han Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0746</td> <td>0.0000</td></tr>
<tr><td>Probability Weighted Compact Feature for Domain Adaptive Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Probability_Weighted_Compact_Feature_for_Domain_Adaptive_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fuxiang Huang,  Lei Zhang,  Yang Yang,  Xichuan Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0579</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0276</td></tr>
<tr><td>Where Does It End? - Reasoning About Hidden Surfaces by Object Intersection Constraints <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Strecke_Where_Does_It_End_-_Reasoning_About_Hidden_Surfaces_by_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michael Strecke,  Jorg Stuckler</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_PolarNet_An_Improved_Grid_Representation_for_Online_LiDAR_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Zhang,  Zixiang Zhou,  Philip David,  Xiangyu Yue,  Zerong Xi,  Boqing Gong,  Hassan Foroosh</td> <td>0.0094</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2191</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Pathological Retinal Region Segmentation From OCT Images Using Geometric Relation Based Augmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mahapatra_Pathological_Retinal_Region_Segmentation_From_OCT_Images_Using_Geometric_Relation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dwarikanath Mahapatra,  Behzad Bozorgtabar,  Ling Shao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0108</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0023</td></tr>
<tr><td>Transferring and Regularizing Prediction for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Transferring_and_Regularizing_Prediction_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiheng Zhang,  Zhaofan Qiu,  Ting Yao,  Chong-Wah Ngo,  Dong Liu,  Tao Mei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0109</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1635</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Su_PREDICT__CLUSTER_Unsupervised_Skeleton_Based_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kun Su,  Xiulong Liu,  Eli Shlizerman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3418</td> <td>0.0000</td> <td>0.0036</td></tr>
<tr><td>Model Adaptation: Unsupervised Domain Adaptation Without Source Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Model_Adaptation_Unsupervised_Domain_Adaptation_Without_Source_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Li,  Qianfen Jiao,  Wenming Cao,  Hau-San Wong,  Si Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2177</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Evade Deep Image Retrieval by Stashing Private Images in the Hash Space <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiao_Evade_Deep_Image_Retrieval_by_Stashing_Private_Images_in_the_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanru Xiao,  Cong Wang,  Xing Gao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0716</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0954</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0237</td></tr>
<tr><td>Advisable Learning for Self-Driving Vehicles by Internalizing Observation-to-Action Rules <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Advisable_Learning_for_Self-Driving_Vehicles_by_Internalizing_Observation-to-Action_Rules_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinkyu Kim,  Suhong Moon,  Anna Rohrbach,  Trevor Darrell,  John Canny</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0926</td> <td>0.0000</td> <td>0.0000</td> <td>0.1216</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ProAlignNet: Unsupervised Learning for Progressively Aligning Noisy Contours <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Veeravasarapu_ProAlignNet_Unsupervised_Learning_for_Progressively_Aligning_Noisy_Contours_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>VSR Veeravasarapu,  Abhishek Goel,  Deepak Mittal,  Maneesh Singh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0393</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0424</td></tr>
<tr><td>Attribution in Scale and Space <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Attribution_in_Scale_and_Space_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shawn Xu,  Subhashini Venugopalan,  Mukund Sundararajan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0098</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Agarwal_Towards_Causal_VQA_Revealing_and_Reducing_Spurious_Correlations_by_Invariant_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vedika Agarwal,  Rakshith Shetty,  Mario Fritz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0124</td></tr>
<tr><td>Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deep_Relational_Reasoning_Graph_Network_for_Arbitrary_Shape_Text_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shi-Xue Zhang,  Xiaobin Zhu,  Jie-Bo Hou,  Chang Liu,  Chun Yang,  Hongfa Wang,  Xu-Cheng Yin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0285</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0456</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0151</td></tr>
<tr><td>Large-Scale Object Detection in the Wild From Imbalanced Multi-Labels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_Large-Scale_Object_Detection_in_the_Wild_From_Imbalanced_Multi-Labels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junran Peng,  Xingyuan Bu,  Ming Sun,  Zhaoxiang Zhang,  Tieniu Tan,  Junjie Yan</td> <td>0.1324</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0378</td> <td>0.0000</td></tr>
<tr><td>BBN: Bilateral-Branch Network With Cumulative Learning for Long-Tailed Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_BBN_Bilateral-Branch_Network_With_Cumulative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Boyan Zhou,  Quan Cui,  Xiu-Shen Wei,  Zhao-Min Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0130</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0203</td></tr>
<tr><td>Momentum Contrast for Unsupervised Visual Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaiming He,  Haoqi Fan,  Yuxin Wu,  Saining Xie,  Ross Girshick</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0144</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bertasius_Classifying_Segmenting_and_Tracking_Object_Instances_in_Video_with_Mask_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gedas Bertasius,  Lorenzo Torresani</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2603</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly Supervised Fine-Grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Weakly_Supervised_Fine-Grained_Image_Classification_via_Guassian_Mixture_Model_Oriented_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihui Wang,  Shijie Wang,  Shuhui Yang,  Haojie Li,  Jianjun Li,  Zezhou Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td></tr>
<tr><td>Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Bridging_the_Gap_Between_Anchor-Based_and_Anchor-Free_Detection_via_Adaptive_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shifeng Zhang,  Cheng Chi,  Yongqiang Yao,  Zhen Lei,  Stan Z. Li</td> <td>0.0167</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0390</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning User Representations for Open Vocabulary Image Hashtag Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Durand_Learning_User_Representations_for_Open_Vocabulary_Image_Hashtag_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thibaut Durand</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Sketch Less for More: On-the-Fly Fine-Grained Sketch-Based Image Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bhunia_Sketch_Less_for_More_On-the-Fly_Fine-Grained_Sketch-Based_Image_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ayan Kumar Bhunia,  Yongxin Yang,  Timothy M. Hospedales,  Tao Xiang,  Yi-Zhe Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0281</td></tr>
<tr><td>Few-Shot Pill Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ling_Few-Shot_Pill_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Suiyi Ling,  Andreas Pastor,  Jing Li,  Zhaohui Che,  Junle Wang,  Jieun Kim,  Patrick Le Callet</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0383</td> <td>0.0117</td></tr>
<tr><td>PointRend: Image Segmentation As Rendering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kirillov_PointRend_Image_Segmentation_As_Rendering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alexander Kirillov,  Yuxin Wu,  Kaiming He,  Ross Girshick</td> <td>0.0000</td> <td>0.0000</td> <td>0.0090</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0875</td> <td>0.0000</td> <td>0.0000</td> <td>0.0086</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ABCNet: Real-Time Scene Text Spotting With Adaptive Bezier-Curve Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ABCNet_Real-Time_Scene_Text_Spotting_With_Adaptive_Bezier-Curve_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuliang Liu,  Hao Chen,  Chunhua Shen,  Tong He,  Lianwen Jin,  Liangwei Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0594</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0199</td></tr>
<tr><td>Learning Temporal Co-Attention Models for Unsupervised Video Action Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gong_Learning_Temporal_Co-Attention_Models_for_Unsupervised_Video_Action_Localization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guoqiang Gong,  Xinghan Wang,  Yadong Mu,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Spatiotemporal Fusion in 3D CNNs: A Probabilistic View <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Spatiotemporal_Fusion_in_3D_CNNs_A_Probabilistic_View_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yizhou Zhou,  Xiaoyan Sun,  Chong Luo,  Zheng-Jun Zha,  Wenjun Zeng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0449</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1506</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Uncertainty-Aware Score Distribution Learning for Action Quality Assessment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Uncertainty-Aware_Score_Distribution_Learning_for_Action_Quality_Assessment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yansong Tang,  Zanlin Ni,  Jiahuan Zhou,  Danyang Zhang,  Jiwen Lu,  Ying Wu,  Jie Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0471</td></tr>
<tr><td>Learning Interactions and Relationships Between Movie Characters <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kukleva_Learning_Interactions_and_Relationships_Between_Movie_Characters_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anna Kukleva,  Makarand Tapaswi,  Ivan Laptev</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Video Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Video_Panoptic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dahun Kim,  Sanghyun Woo,  Joon-Young Lee,  In So Kweon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0666</td> <td>0.0000</td> <td>0.0846</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0201</td></tr>
<tr><td>Understanding Human Hands in Contact at Internet Scale <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dandan Shan,  Jiaqi Geng,  Michelle Shu,  David F. Fouhey</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>End-to-End Learning of Visual Representations From Uncurated Instructional Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Miech_End-to-End_Learning_of_Visual_Representations_From_Uncurated_Instructional_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Antoine Miech,  Jean-Baptiste Alayrac,  Lucas Smaira,  Ivan Laptev,  Josef Sivic,  Andrew Zisserman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0750</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ng_You2Me_Inferring_Body_Pose_in_Egocentric_Video_via_First_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Evonne Ng,  Donglai Xiang,  Hanbyul Joo,  Kristen Grauman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0438</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning a Weakly-Supervised Video Actor-Action Segmentation Model With a Wise Selection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Learning_a_Weakly-Supervised_Video_Actor-Action_Segmentation_Model_With_a_Wise_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Chen,  Zhiheng Li,  Jiebo Luo,  Chenliang Xu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0118</td></tr>
<tr><td>Learning to Measure the Static Friction Coefficient in Cloth Contact <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rasheed_Learning_to_Measure_the_Static_Friction_Coefficient_in_Cloth_Contact_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abdullah Haroon Rasheed,  Victor Romero,  Florence Bertails-Descoubes,  Stefanie Wuhrer,  Jean-Sebastien Franco,  Arnaud Lazarus</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0024</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SpeedNet: Learning the Speediness in Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Benaim_SpeedNet_Learning_the_Speediness_in_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sagie Benaim,  Ariel Ephrat,  Oran Lang,  Inbar Mosseri,  William T. Freeman,  Michael Rubinstein,  Michal Irani,  Tali Dekel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0885</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Telling Left From Right: Learning Spatial Correspondence of Sight and Sound <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Telling_Left_From_Right_Learning_Spatial_Correspondence_of_Sight_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Karren Yang,  Bryan Russell,  Justin Salamon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Visual-Textual Capsule Routing for Text-Based Video Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/McIntosh_Visual-Textual_Capsule_Routing_for_Text-Based_Video_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bruce McIntosh,  Kevin Duarte,  Yogesh S Rawat,  Mubarak Shah</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0093</td></tr>
<tr><td>Graph-Structured Referring Expression Reasoning in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Graph-Structured_Referring_Expression_Reasoning_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sibei Yang,  Guanbin Li,  Yizhou Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0576</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Say_As_You_Wish_Fine-Grained_Control_of_Image_Caption_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shizhe Chen,  Qin Jin,  Peng Wang,  Qi Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Hierarchical Conditional Relation Networks for Video Question Answering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thao Minh Le,  Vuong Le,  Svetha Venkatesh,  Truyen Tran</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_REVERIE_Remote_Embodied_Visual_Referring_Expression_in_Real_Indoor_Environments_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuankai Qi,  Qi Wu,  Peter Anderson,  Xin Wang,  William Yang Wang,  Chunhua Shen,  Anton van den Hengel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Iterative_Answer_Prediction_With_Pointer-Augmented_Multimodal_Transformers_for_TextVQA_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ronghang Hu,  Amanpreet Singh,  Trevor Darrell,  Marcus Rohrbach</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td></tr>
<tr><td>SQuINTing at VQA Models: Introspecting VQA Models With Sub-Questions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Selvaraju_SQuINTing_at_VQA_Models_Introspecting_VQA_Models_With_Sub-Questions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ramprasaath R. Selvaraju,  Purva Tendulkar,  Devi Parikh,  Eric Horvitz,  Marco Tulio Ribeiro,  Besmira Nushi,  Ece Kamar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Vision-Language_Navigation_With_Self-Supervised_Auxiliary_Reasoning_Tasks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fengda Zhu,  Yi Zhu,  Xiaojun Chang,  Xiaodan Liang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Sign Language Transformers: Joint End-to-End Sign Language Recognition and Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Camgoz_Sign_Language_Transformers_Joint_End-to-End_Sign_Language_Recognition_and_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Necati Cihan Camgoz,  Oscar Koller,  Simon Hadfield,  Richard Bowden</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Multi-Task_Collaborative_Network_for_Joint_Referring_Expression_Comprehension_and_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gen Luo,  Yiyi Zhou,  Xiaoshuai Sun,  Liujuan Cao,  Chenglin Wu,  Cheng Deng,  Rongrong Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0617</td></tr>
<tr><td>Counterfactual Vision and Language Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abbasnejad_Counterfactual_Vision_and_Language_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ehsan Abbasnejad,  Damien Teney,  Amin Parvaneh,  Javen Shi,  Anton van den Hengel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0555</td> <td>0.0000</td></tr>
<tr><td>Iterative Context-Aware Graph Inference for Visual Dialog <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Iterative_Context-Aware_Graph_Inference_for_Visual_Dialog_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dan Guo,  Hui Wang,  Hanwang Zhang,  Zheng-Jun Zha,  Meng Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0188</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0177</td></tr>
<tr><td>TA-Student VQA: Multi-Agents Training by Self-Questioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiong_TA-Student_VQA_Multi-Agents_Training_by_Self-Questioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peixi Xiong,  Ying Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Exploring Self-Attention for Image Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Exploring_Self-Attention_for_Image_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hengshuang Zhao,  Jiaya Jia,  Vladlen Koltun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cops-Ref: A New Dataset and Task on Compositional Referring Expression Comprehension <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Cops-Ref_A_New_Dataset_and_Task_on_Compositional_Referring_Expression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenfang Chen,  Peng Wang,  Lin Ma,  Kwan-Yee K. Wong,  Qi Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Improving Convolutional Networks With Self-Calibrated Convolutions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Improving_Convolutional_Networks_With_Self-Calibrated_Convolutions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiang-Jiang Liu,  Qibin Hou,  Ming-Ming Cheng,  Changhu Wang,  Jiashi Feng</td> <td>0.0115</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0368</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0098</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0035</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Modality Shifting Attention Network for Multi-Modal Video Question Answering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Modality_Shifting_Attention_Network_for_Multi-Modal_Video_Question_Answering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junyeong Kim,  Minuk Ma,  Trung Pham,  Kyungsu Kim,  Chang D. Yoo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0171</td></tr>
<tr><td>Learning to Structure an Image With Few Colors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Learning_to_Structure_an_Image_With_Few_Colors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunzhong Hou,  Liang Zheng,  Stephen Gould</td> <td>0.0000</td> <td>0.0000</td> <td>0.0243</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0438</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On the General Value of Evidence, and Bilingual Scene-Text Visual Question Answering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_On_the_General_Value_of_Evidence_and_Bilingual_Scene-Text_Visual_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xinyu Wang,  Yuliang Liu,  Chunhua Shen,  Chun Chet Ng,  Canjie Luo,  Lianwen Jin,  Chee Seng Chan,  Anton van den Hengel,  Liangwei Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>From Paris to Berlin: Discovering Fashion Style Influences Around the World <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Al-Halah_From_Paris_to_Berlin_Discovering_Fashion_Style_Influences_Around_the_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziad Al-Halah,  Kristen Grauman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td></tr>
<tr><td>A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_A_Local-to-Global_Approach_to_Multi-Modal_Movie_Scene_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anyi Rao,  Linning Xu,  Yu Xiong,  Guodong Xu,  Qingqiu Huang,  Bolei Zhou,  Dahua Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0871</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>G-TAD: Sub-Graph Localization for Temporal Action Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_G-TAD_Sub-Graph_Localization_for_Temporal_Action_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mengmeng Xu,  Chen Zhao,  David S. Rojas,  Ali Thabet,  Bernard Ghanem</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0118</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0064</td></tr>
<tr><td>Detailed 2D-3D Joint Representation for Human-Object Interaction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Detailed_2D-3D_Joint_Representation_for_Human-Object_Interaction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yong-Lu Li,  Xinpeng Liu,  Han Lu,  Shiyi Wang,  Junqi Liu,  Jiefeng Li,  Cewu Lu</td> <td>0.0176</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0183</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0124</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>One-Shot Adversarial Attacks on Visual Tracking With Dual Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_One-Shot_Adversarial_Attacks_on_Visual_Tracking_With_Dual_Attention_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuesong Chen,  Xiyu Yan,  Feng Zheng,  Yong Jiang,  Shu-Tao Xia,  Yong Zhao,  Rongrong Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0018</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1296</td> <td>0.0236</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0169</td></tr>
<tr><td>Rethinking Classification and Localization for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Rethinking_Classification_and_Localization_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Wu,  Yinpeng Chen,  Lu Yuan,  Zicheng Liu,  Lijuan Wang,  Hongzhi Li,  Yun Fu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Correspondence Networks With Adaptive Neighbourhood Consensus <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Correspondence_Networks_With_Adaptive_Neighbourhood_Consensus_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuda Li,  Kai Han,  Theo W. Costain,  Henry Howard-Jenkins,  Victor Prisacariu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0372</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0332</td></tr>
<tr><td>Multiple Anchor Learning for Visual Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ke_Multiple_Anchor_Learning_for_Visual_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wei Ke,  Tianliang Zhang,  Zeyi Huang,  Qixiang Ye,  Jianzhuang Liu,  Dong Huang</td> <td>0.0424</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PhraseCut: Language-Based Image Segmentation in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_PhraseCut_Language-Based_Image_Segmentation_in_the_Wild_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenyun Wu,  Zhe Lin,  Scott Cohen,  Trung Bui,  Subhransu Maji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Mask Encoding for Single Shot Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Mask_Encoding_for_Single_Shot_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rufeng Zhang,  Zhi Tian,  Chunhua Shen,  Mingyu You,  Youliang Yan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3833</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0102</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Action Genome: Actions As Compositions of Spatio-Temporal Scene Graphs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ji_Action_Genome_Actions_As_Compositions_of_Spatio-Temporal_Scene_Graphs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingwei Ji,  Ranjay Krishna,  Li Fei-Fei,  Juan Carlos Niebles</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0307</td> <td>0.0000</td> <td>0.0000</td> <td>0.3716</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Unseen Concepts via Hierarchical Decomposition and Composition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Unseen_Concepts_via_Hierarchical_Decomposition_and_Composition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muli Yang,  Cheng Deng,  Junchi Yan,  Xianglong Liu,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0119</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_Hi-CMD_Hierarchical_Cross-Modality_Disentanglement_for_Visible-Infrared_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Seokeon Choi,  Sumin Lee,  Youngeun Kim,  Taekyung Kim,  Changick Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0507</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0570</td></tr>
<tr><td>In Defense of Grid Features for Visual Question Answering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_In_Defense_of_Grid_Features_for_Visual_Question_Answering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huaizu Jiang,  Ishan Misra,  Marcus Rohrbach,  Erik Learned-Miller,  Xinlei Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-Mutual Consistency Induced Transfer Subspace Learning for Human Motion Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Multi-Mutual_Consistency_Induced_Transfer_Subspace_Learning_for_Human_Motion_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tao Zhou,  Huazhu Fu,  Chen Gong,  Jianbing Shen,  Ling Shao,  Fatih Porikli</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0058</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0125</td></tr>
<tr><td>Dense Regression Network for Video Grounding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Dense_Regression_Network_for_Video_Grounding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Runhao Zeng,  Haoming Xu,  Wenbing Huang,  Peihao Chen,  Mingkui Tan,  Chuang Gan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0093</td></tr>
<tr><td>Neural Architecture Search for Lightweight Non-Local Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Neural_Architecture_Search_for_Lightweight_Non-Local_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yingwei Li,  Xiaojie Jin,  Jieru Mei,  Xiaochen Lian,  Linjie Yang,  Cihang Xie,  Qihang Yu,  Yuyin Zhou,  Song Bai,  Alan L. Yuille</td> <td>0.0000</td> <td>0.0000</td> <td>0.0159</td> <td>0.0667</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0048</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Saliency Propagation for Semi-Supervised Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Learning_Saliency_Propagation_for_Semi-Supervised_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yanzhao Zhou,  Xin Wang,  Jianbin Jiao,  Trevor Darrell,  Fisher Yu</td> <td>0.0323</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2651</td> <td>0.0000</td> <td>0.0015</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Speech2Action: Cross-Modal Supervision for Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nagrani_Speech2Action_Cross-Modal_Supervision_for_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Arsha Nagrani,  Chen Sun,  David Ross,  Rahul Sukthankar,  Cordelia Schmid,  Andrew Zisserman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2620</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Normalized and Geometry-Aware Self-Attention Network for Image Captioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Longteng Guo,  Jing Liu,  Xinxin Zhu,  Peng Yao,  Shichen Lu,  Hanqing Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Memory Enhanced Global-Local Aggregation for Video Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Memory_Enhanced_Global-Local_Aggregation_for_Video_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yihong Chen,  Yue Cao,  Han Hu,  Liwei Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0519</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0088</td></tr>
<tr><td>Solving Mixed-Modal Jigsaw Puzzle for Fine-Grained Sketch-Based Image Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_Solving_Mixed-Modal_Jigsaw_Puzzle_for_Fine-Grained_Sketch-Based_Image_Retrieval_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaiyue Pang,  Yongxin Yang,  Timothy M. Hospedales,  Tao Xiang,  Yi-Zhe Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0122</td></tr>
<tr><td>LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of Point Cloud Based Deep Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_LG-GAN_Label_Guided_Adversarial_Network_for_Flexible_Targeted_Attack_of_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hang Zhou,  Dongdong Chen,  Jing Liao,  Kejiang Chen,  Xiaoyi Dong,  Kunlin Liu,  Weiming Zhang,  Gang Hua,  Nenghai Yu</td> <td>0.0000</td> <td>0.1631</td> <td>0.0248</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0438</td> <td>0.0000</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Memory Aggregation Networks for Efficient Interactive Video Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Miao_Memory_Aggregation_Networks_for_Efficient_Interactive_Video_Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiaxu Miao,  Yunchao Wei,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>VQA With No Questions-Answers Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Vatashsky_VQA_With_No_Questions-Answers_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ben-Zion Vatashsky,  Shimon Ullman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Counting Out Time: Class Agnostic Video Repetition Counting in the Wild <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dwibedi_Counting_Out_Time_Class_Agnostic_Video_Repetition_Counting_in_the_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Debidatta Dwibedi,  Yusuf Aytar,  Jonathan Tompson,  Pierre Sermanet,  Andrew Zisserman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SaccadeNet: A Fast and Accurate Object Detector <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lan_SaccadeNet_A_Fast_and_Accurate_Object_Detector_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shiyi Lan,  Zhou Ren,  Yi Wu,  Larry S. Davis,  Gang Hua</td> <td>0.0443</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-Based Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Multi-Granularity_Reference-Aided_Attentive_Feature_Aggregation_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhizheng Zhang,  Cuiling Lan,  Wenjun Zeng,  Zhibo Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0182</td></tr>
<tr><td>Video Object Grounding Using Semantic Roles in Language Description <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sadhu_Video_Object_Grounding_Using_Semantic_Roles_in_Language_Description_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Arka Sadhu,  Kan Chen,  Ram Nevatia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0025</td></tr>
<tr><td>Designing Network Design Spaces <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ilija Radosavovic,  Raj Prateek Kosaraju,  Ross Girshick,  Kaiming He,  Piotr Dollar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>12-in-1: Multi-Task Vision and Language Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_12-in-1_Multi-Task_Vision_and_Language_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiasen Lu,  Vedanuj Goswami,  Marcus Rohrbach,  Devi Parikh,  Stefan Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MLCVNet: Multi-Level Context VoteNet for 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_MLCVNet_Multi-Level_Context_VoteNet_for_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qian Xie,  Yu-Kun Lai,  Jing Wu,  Zhoutao Wang,  Yiming Zhang,  Kai Xu,  Jun Wang</td> <td>0.2066</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Listen to Look: Action Recognition by Previewing Audio <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Listen_to_Look_Action_Recognition_by_Previewing_Audio_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruohan Gao,  Tae-Hyun Oh,  Kristen Grauman,  Lorenzo Torresani</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2485</td> <td>0.0000</td> <td>0.0206</td></tr>
<tr><td>Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ji_Attention_Convolutional_Binary_Neural_Tree_for_Fine-Grained_Visual_Categorization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruyi Ji,  Longyin Wen,  Libo Zhang,  Dawei Du,  Yanjun Wu,  Chen Zhao,  Xianglong Liu,  Feiyue Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0506</td></tr>
<tr><td>Music Gesture for Visual Sound Separation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gan_Music_Gesture_for_Visual_Sound_Separation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chuang Gan,  Deng Huang,  Hang Zhao,  Joshua B. Tenenbaum,  Antonio Torralba</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0788</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0095</td></tr>
<tr><td>Referring Image Segmentation via Cross-Modal Progressive Comprehension <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Referring_Image_Segmentation_via_Cross-Modal_Progressive_Comprehension_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaofei Huang,  Tianrui Hui,  Si Liu,  Guanbin Li,  Yunchao Wei,  Jizhong Han,  Luoqi Liu,  Bo Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0582</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0089</td></tr>
<tr><td>Cloth in the Wind: A Case Study of Physical Measurement Through Simulation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Runia_Cloth_in_the_Wind_A_Case_Study_of_Physical_Measurement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tom F. H. Runia,  Kirill Gavrilyuk,  Cees G. M. Snoek,  Arnold W. M. Smeulders</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0623</td></tr>
<tr><td>The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liang_The_Garden_of_Forking_Paths_Towards_Multi-Future_Trajectory_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junwei Liang,  Lu Jiang,  Kevin Murphy,  Ting Yu,  Alexander Hauptmann</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CentripetalNet: Pursuing High-Quality Keypoint Pairs for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_CentripetalNet_Pursuing_High-Quality_Keypoint_Pairs_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiwei Dong,  Guoxuan Li,  Yue Liao,  Fei Wang,  Pengju Ren,  Chen Qian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1887</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0647</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaoshuai Shi,  Chaoxu Guo,  Li Jiang,  Zhe Wang,  Jianping Shi,  Xiaogang Wang,  Hongsheng Li</td> <td>0.1718</td> <td>0.0586</td> <td>0.0272</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0134</td></tr>
<tr><td>Graph Embedded Pose Clustering for Anomaly Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Markovitz_Graph_Embedded_Pose_Clustering_for_Anomaly_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Amir Markovitz,  Gilad Sharir,  Itamar Friedman,  Lihi Zelnik-Manor,  Shai Avidan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0231</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0029</td></tr>
<tr><td>Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Disp_R-CNN_Stereo_3D_Object_Detection_via_Shape_Prior_Guided_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiaming Sun,  Linghao Chen,  Yiming Xie,  Siyu Zhang,  Qinhong Jiang,  Xiaowei Zhou,  Hujun Bao</td> <td>0.0735</td> <td>0.0645</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0463</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td></tr>
<tr><td>Deepstrip: High-Resolution Boundary Refinement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Deepstrip_High-Resolution_Boundary_Refinement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peng Zhou,  Brian Price,  Scott Cohen,  Gregg Wilensky,  Larry S. Davis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guangcong Wang,  Jian-Huang Lai,  Wenqi Liang,  Guangrun Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2518</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0062</td></tr>
<tr><td>Meshed-Memory Transformer for Image Captioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cornia_Meshed-Memory_Transformer_for_Image_Captioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Marcella Cornia,  Matteo Stefanini,  Lorenzo Baraldi,  Rita Cucchiara</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0146</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning From Noisy Anchors for One-Stage Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Learning_From_Noisy_Anchors_for_One-Stage_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hengduo Li,  Zuxuan Wu,  Chen Zhu,  Caiming Xiong,  Richard Socher,  Larry S. Davis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0128</td></tr>
<tr><td>Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ren_Instance-Aware_Context-Focused_and_Memory-Efficient_Weakly_Supervised_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhongzheng Ren,  Zhiding Yu,  Xiaodong Yang,  Ming-Yu Liu,  Yong Jae Lee,  Alexander G. Schwing,  Jan Kautz</td> <td>0.0575</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0071</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0743</td></tr>
<tr><td>Density-Based Clustering for 3D Object Detection in Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ahmed_Density-Based_Clustering_for_3D_Object_Detection_in_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Syeda Mariam Ahmed,  Chee Meng Chew</td> <td>0.1221</td> <td>0.0708</td> <td>0.0000</td> <td>0.0000</td> <td>0.1283</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0026</td></tr>
<tr><td>Few-Shot Video Classification via Temporal Alignment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_Few-Shot_Video_Classification_via_Temporal_Alignment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaidi Cao,  Jingwei Ji,  Zhangjie Cao,  Chien-Yi Chang,  Juan Carlos Niebles</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0216</td></tr>
<tr><td>Densely Connected Search Space for More Flexible Neural Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_Densely_Connected_Search_Space_for_More_Flexible_Neural_Architecture_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiemin Fang,  Yuzhu Sun,  Qian Zhang,  Yuan Li,  Wenyu Liu,  Xinggang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0093</td> <td>0.3390</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0511</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fine-Grained Video-Text Retrieval With Hierarchical Graph Reasoning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Fine-Grained_Video-Text_Retrieval_With_Hierarchical_Graph_Reasoning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shizhe Chen,  Yida Zhao,  Qin Jin,  Qi Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0069</td></tr>
<tr><td>Warp to the Future: Joint Forecasting of Features and Feature Motion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Saric_Warp_to_the_Future_Joint_Forecasting_of_Features_and_Feature_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Josip Saric,  Marin Orsic,  Tonci Antunovic,  Sacha Vrazic,  Sinisa Segvic</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0776</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Network Adjustment: Channel Search Guided by FLOPs Utilization Ratio <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Network_Adjustment_Channel_Search_Guided_by_FLOPs_Utilization_Ratio_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengsu Chen,  Jianwei Niu,  Lingxi Xie,  Xuefeng Liu,  Longhui Wei,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0162</td> <td>0.0401</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0509</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Where_Does_It_Exist_Spatio-Temporal_Video_Grounding_for_Multi-Form_Sentences_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhu Zhang,  Zhou Zhao,  Yang Zhao,  Qi Wang,  Huasheng Liu,  Lianli Gao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0173</td></tr>
<tr><td>Cross-Modal Cross-Domain Moment Alignment Network for Person Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jing_Cross-Modal_Cross-Domain_Moment_Alignment_Network_for_Person_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ya Jing,  Wei Wang,  Liang Wang,  Tieniu Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0376</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0180</td></tr>
<tr><td>Self-Training With Noisy Student Improves ImageNet Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qizhe Xie,  Minh-Thang Luong,  Eduard Hovy,  Quoc V. Le</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Longterm Representations for Person Re-Identification Using Radio Signals <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Learning_Longterm_Representations_for_Person_Re-Identification_Using_Radio_Signals_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lijie Fan,  Tianhong Li,  Rongyao Fang,  Rumen Hristov,  Yuan Yuan,  Dina Katabi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td></tr>
<tr><td>LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Park_LatentFusion_End-to-End_Differentiable_Reconstruction_and_Rendering_for_Unseen_Object_Pose_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Keunhong Park,  Arsalan Mousavian,  Yu Xiang,  Dieter Fox</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0704</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0033</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Instance Occlusion for Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lazarow_Learning_Instance_Occlusion_for_Panoptic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Justin Lazarow,  Kwonjoon Lee,  Kunyu Shi,  Zhuowen Tu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1759</td> <td>0.0000</td> <td>0.1099</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Vision-Dialog Navigation by Exploring Cross-Modal Memory <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Vision-Dialog_Navigation_by_Exploring_Cross-Modal_Memory_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yi Zhu,  Fengda Zhu,  Zhaohuan Zhan,  Bingqian Lin,  Jianbin Jiao,  Xiaojun Chang,  Xiaodan Liang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shridhar_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohit Shridhar,  Jesse Thomason,  Daniel Gordon,  Yonatan Bisk,  Winson Han,  Roozbeh Mottaghi,  Luke Zettlemoyer,  Dieter Fox</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_NMS_by_Representative_Region_Towards_Crowded_Pedestrian_Detection_by_Proposal_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Huang,  Zheng Ge,  Zequn Jie,  Osamu Yoshie</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Visual Commonsense R-CNN <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Visual_Commonsense_R-CNN_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tan Wang,  Jianqiang Huang,  Hanwang Zhang,  Qianru Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0381</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>What Deep CNNs Benefit From Global Covariance Pooling: An Optimization Perspective <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_What_Deep_CNNs_Benefit_From_Global_Covariance_Pooling_An_Optimization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qilong Wang,  Li Zhang,  Banggu Wu,  Dongwei Ren,  Peihua Li,  Wangmeng Zuo,  Qinghua Hu</td> <td>0.0120</td> <td>0.0000</td> <td>0.0286</td> <td>0.0000</td> <td>0.0346</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0004</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>EfficientDet: Scalable and Efficient Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mingxing Tan,  Ruoming Pang,  Quoc V. Le</td> <td>0.0315</td> <td>0.0000</td> <td>0.0186</td> <td>0.0011</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0386</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Fast Template Matching and Update for Video Object Tracking and Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Fast_Template_Matching_and_Update_for_Video_Object_Tracking_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mingjie Sun,  Jimin Xiao,  Eng Gee Lim,  Bingfeng Zhang,  Yao Zhao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0132</td></tr>
<tr><td>Counterfactual Samples Synthesizing for Robust Visual Question Answering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Counterfactual_Samples_Synthesizing_for_Robust_Visual_Question_Answering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Long Chen,  Xin Yan,  Jun Xiao,  Hanwang Zhang,  Shiliang Pu,  Yueting Zhuang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Local-Global Video-Text Interactions for Temporal Grounding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mun_Local-Global_Video-Text_Interactions_for_Temporal_Grounding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jonghwan Mun,  Minsu Cho,  Bohyung Han</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0858</td></tr>
<tr><td>Set-Constrained Viterbi for Set-Supervised Action Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Set-Constrained_Viterbi_for_Set-Supervised_Action_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jun Li,  Sinisa Todorovic</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Probabilistic Video Prediction From Noisy Data With a Posterior Confidence <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Probabilistic_Video_Prediction_From_Noisy_Data_With_a_Posterior_Confidence_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunbo Wang,  Jiajun Wu,  Mingsheng Long,  Joshua B. Tenenbaum</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0087</td></tr>
<tr><td>Beyond Short-Term Snippet: Video Relation Detection With Spatio-Temporal Global Context <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Beyond_Short-Term_Snippet_Video_Relation_Detection_With_Spatio-Temporal_Global_Context_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenchen Liu,  Yang Jin,  Kehan Xu,  Guoqiang Gong,  Yadong Mu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0112</td></tr>
<tr><td>Visual Grounding in Video for Unsupervised Word Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sigurdsson_Visual_Grounding_in_Video_for_Unsupervised_Word_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gunnar A. Sigurdsson,  Jean-Baptiste Alayrac,  Aida Nematzadeh,  Lucas Smaira,  Mateusz Malinowski,  Joao Carreira,  Phil Blunsom,  Andrew Zisserman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Two Causal Principles for Improving Visual Dialog <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Two_Causal_Principles_for_Improving_Visual_Dialog_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiaxin Qi,  Yulei Niu,  Jianqiang Huang,  Hanwang Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0609</td> <td>0.0000</td></tr>
<tr><td>Spatio-Temporal Graph for Video Captioning With Knowledge Distillation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Boxiao Pan,  Haoye Cai,  De-An Huang,  Kuan-Hui Lee,  Adrien Gaidon,  Ehsan Adeli,  Juan Carlos Niebles</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0323</td></tr>
<tr><td>A Real-Time Cross-Modality Correlation Filtering Method for Referring Expression Comprehension <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_A_Real-Time_Cross-Modality_Correlation_Filtering_Method_for_Referring_Expression_Comprehension_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yue Liao,  Si Liu,  Guanbin Li,  Fei Wang,  Yanjie Chen,  Chen Qian,  Bo Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Better Captioning With Sequence-Level Exploration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Better_Captioning_With_Sequence-Level_Exploration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jia Chen,  Qin Jin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0425</td></tr>
<tr><td>Violin: A Large-Scale Dataset for Video-and-Language Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Violin_A_Large-Scale_Dataset_for_Video-and-Language_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingzhou Liu,  Wenhu Chen,  Yu Cheng,  Zhe Gan,  Licheng Yu,  Yiming Yang,  Jingjing Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RiFeGAN: Rich Feature Generation for Text-to-Image Synthesis From Prior Knowledge <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_RiFeGAN_Rich_Feature_Generation_for_Text-to-Image_Synthesis_From_Prior_Knowledge_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jun Cheng,  Fuxiang Wu,  Yanling Tian,  Lei Wang,  Dapeng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1443</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0124</td></tr>
<tr><td>Graph Structured Network for Image-Text Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Graph_Structured_Network_for_Image-Text_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chunxiao Liu,  Zhendong Mao,  Tianzhu Zhang,  Hongtao Xie,  Bin Wang,  Yongdong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0151</td></tr>
<tr><td>Straight to the Point: Fast-Forwarding Videos via Reinforcement Learning Using Textual Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ramos_Straight_to_the_Point_Fast-Forwarding_Videos_via_Reinforcement_Learning_Using_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Washington Ramos,  Michel Silva,  Edson Araujo,  Leandro Soriano Marcolino,  Erickson Nascimento</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0039</td></tr>
<tr><td>Multi-Modality Cross Attention Network for Image and Sentence Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Modality_Cross_Attention_Network_for_Image_and_Sentence_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xi Wei,  Tianzhu Zhang,  Yan Li,  Yongdong Zhang,  Feng Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0089</td></tr>
<tr><td>Generalized ODIN: Detecting Out-of-Distribution Image Without Learning From Out-of-Distribution Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hsu_Generalized_ODIN_Detecting_Out-of-Distribution_Image_Without_Learning_From_Out-of-Distribution_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yen-Chang Hsu,  Yilin Shen,  Hongxia Jin,  Zsolt Kira</td> <td>0.0000</td> <td>0.0000</td> <td>0.0724</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Augmentation Network via Influence Functions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_Learning_Augmentation_Network_via_Influence_Functions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Donghoon Lee,  Hyunsin Park,  Trung Pham,  Chang D. Yoo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0149</td></tr>
<tr><td>X-Linear Attention Networks for Image Captioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_X-Linear_Attention_Networks_for_Image_Captioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yingwei Pan,  Ting Yao,  Yehao Li,  Tao Mei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0385</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Person Re-Identification via Multi-Label Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Unsupervised_Person_Re-Identification_via_Multi-Label_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongkai Wang,  Shiliang Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0050</td></tr>
<tr><td>Overcoming Classifier Imbalance for Long-Tail Object Detection With Balanced Group Softmax <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Overcoming_Classifier_Imbalance_for_Long-Tail_Object_Detection_With_Balanced_Group_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu Li,  Tao Wang,  Bingyi Kang,  Sheng Tang,  Chunfeng Wang,  Jintao Li,  Jiashi Feng</td> <td>0.0600</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0601</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0532</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td></tr>
<tr><td>What You See is What You Get: Exploiting Visibility for 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_What_You_See_is_What_You_Get_Exploiting_Visibility_for_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peiyun Hu,  Jason Ziglar,  David Held,  Deva Ramanan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0333</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deep Structure-Revealed Network for Texture Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhai_Deep_Structure-Revealed_Network_for_Texture_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wei Zhai,  Yang Cao,  Zheng-Jun Zha,  HaiYong Xie,  Feng Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0316</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0167</td></tr>
<tr><td>Online Knowledge Distillation via Collaborative Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiushan Guo,  Xinjiang Wang,  Yichao Wu,  Zhipeng Yu,  Ding Liang,  Xiaolin Hu,  Ping Luo</td> <td>0.0176</td> <td>0.0000</td> <td>0.0373</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0361</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Dynamic Convolution: Attention Over Convolution Kernels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Dynamic_Convolution_Attention_Over_Convolution_Kernels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yinpeng Chen,  Xiyang Dai,  Mengchen Liu,  Dongdong Chen,  Lu Yuan,  Zicheng Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0353</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>3DSSD: Point-Based 3D Single Stage Object Detector <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_3DSSD_Point-Based_3D_Single_Stage_Object_Detector_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zetong Yang,  Yanan Sun,  Shu Liu,  Jiaya Jia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0204</td></tr>
<tr><td>Deep Degradation Prior for Low-Quality Image Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Deep_Degradation_Prior_for_Low-Quality_Image_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Wang,  Yang Cao,  Zheng-Jun Zha,  Jing Zhang,  Zhiwei Xiong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0325</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ViBE: Dressing for Diverse Body Shapes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hsiao_ViBE_Dressing_for_Diverse_Body_Shapes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wei-Lin Hsiao,  Kristen Grauman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Singh_Dont_Judge_an_Object_by_Its_Context_Learning_to_Overcome_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Krishna Kumar Singh,  Dhruv Mahajan,  Kristen Grauman,  Yong Jae Lee,  Matt Feiszli,  Deepti Ghadiyaram</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0014</td></tr>
<tr><td>SESS: Self-Ensembling Semi-Supervised 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_SESS_Self-Ensembling_Semi-Supervised_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Na Zhao,  Tat-Seng Chua,  Gim Hee Lee</td> <td>0.2797</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0353</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Combining Detection and Tracking for Human Pose Estimation in Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Combining_Detection_and_Tracking_for_Human_Pose_Estimation_in_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Manchen Wang,  Joseph Tighe,  Davide Modolo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0794</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0012</td></tr>
<tr><td>SAPIEN: A SimulAted Part-Based Interactive ENvironment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiang_SAPIEN_A_SimulAted_Part-Based_Interactive_ENvironment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fanbo Xiang,  Yuzhe Qin,  Kaichun Mo,  Yikuan Xia,  Hao Zhu,  Fangchen Liu,  Minghua Liu,  Hanxiao Jiang,  Yifu Yuan,  He Wang,  Li Yi,  Angel X. Chang,  Leonidas J. Guibas,  Hao Su</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_RandLA-Net_Efficient_Semantic_Segmentation_of_Large-Scale_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qingyong Hu,  Bo Yang,  Linhai Xie,  Stefano Rosa,  Yulan Guo,  Zhihua Wang,  Niki Trigoni,  Andrew Markham</td> <td>0.0000</td> <td>0.1546</td> <td>0.0000</td> <td>0.0194</td> <td>0.0000</td> <td>0.0000</td> <td>0.0927</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_SurfelGAN_Synthesizing_Realistic_Sensor_Data_for_Autonomous_Driving_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhenpei Yang,  Yuning Chai,  Dragomir Anguelov,  Yin Zhou,  Pei Sun,  Dumitru Erhan,  Sean Rafferty,  Henrik Kretzschmar</td> <td>0.0028</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Programmatic and Semantic Approach to Explaining and Debugging Neural Network Based Object Detectors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_A_Programmatic_and_Semantic_Approach_to_Explaining_and_Debugging_Neural_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Edward Kim,  Divya Gopinath,  Corina Pasareanu,  Sanjit A. Seshia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0890</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Roddick_Predicting_Semantic_Map_Representations_From_Images_Using_Pyramid_Occupancy_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thomas Roddick,  Roberto Cipolla</td> <td>0.1107</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td></tr>
<tr><td>Efficient Derivative Computation for Cumulative B-Splines on Lie Groups <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sommer_Efficient_Derivative_Computation_for_Cumulative_B-Splines_on_Lie_Groups_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christiane Sommer,  Vladyslav Usenko,  David Schubert,  Nikolaus Demmel,  Daniel Cremers</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RL-CycleGAN: Reinforcement Learning Aware Simulation-to-Real <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_RL-CycleGAN_Reinforcement_Learning_Aware_Simulation-to-Real_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kanishka Rao,  Chris Harris,  Alex Irpan,  Sergey Levine,  Julian Ibarz,  Mohi Khansari</td> <td>0.0000</td> <td>0.0000</td> <td>0.0358</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Manivasagam_LiDARsim_Realistic_LiDAR_Simulation_by_Leveraging_the_Real_World_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sivabalan Manivasagam,  Shenlong Wang,  Kelvin Wong,  Wenyuan Zeng,  Mikita Sazanovich,  Shuhan Tan,  Bin Yang,  Wei-Chiu Ma,  Raquel Urtasun</td> <td>0.0000</td> <td>0.0735</td> <td>0.0331</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Just Go With the Flow: Self-Supervised Scene Flow Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mittal_Just_Go_With_the_Flow_Self-Supervised_Scene_Flow_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Himangi Mittal,  Brian Okorn,  David Held</td> <td>0.0078</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0190</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0044</td></tr>
<tr><td>TITAN: Future Forecast Using Action Priors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Malla_TITAN_Future_Forecast_Using_Action_Priors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Srikanth Malla,  Behzad Dariush,  Chiho Choi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0148</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Robust Learning Through Cross-Task Consistency <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zamir_Robust_Learning_Through_Cross-Task_Consistency_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Amir R. Zamir,  Alexander Sax,  Nikhil Cheerla,  Rohan Suri,  Zhangjie Cao,  Jitendra Malik,  Leonidas J. Guibas</td> <td>0.0414</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0258</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Dynamic Refinement Network for Oriented and Densely Packed Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Dynamic_Refinement_Network_for_Oriented_and_Densely_Packed_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xingjia Pan,  Yuqiang Ren,  Kekai Sheng,  Weiming Dong,  Haolei Yuan,  Xiaowei Guo,  Chongyang Ma,  Changsheng Xu</td> <td>0.0245</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0108</td></tr>
<tr><td>AOWS: Adaptive and Optimal Network Width Search With Latency Constraints <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Berman_AOWS_Adaptive_and_Optimal_Network_Width_Search_With_Latency_Constraints_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maxim Berman,  Leonid Pishchulin,  Ning Xu,  Matthew B. Blaschko,  Gerard Medioni</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2100</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>High-Dimensional Convolutional Networks for Geometric Pattern Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Choy_High-Dimensional_Convolutional_Networks_for_Geometric_Pattern_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Christopher Choy,  Junha Lee,  Rene Ranftl,  Jaesik Park,  Vladlen Koltun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0480</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Singh_Filter_Response_Normalization_Layer_Eliminating_Batch_Dependence_in_the_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Saurabh Singh,  Shankar Krishnan</td> <td>0.0161</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td></tr>
<tr><td>Deep Iterative Surface Normal Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lenssen_Deep_Iterative_Surface_Normal_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jan Eric Lenssen,  Christian Osendorfer,  Jonathan Masci</td> <td>0.0000</td> <td>0.0000</td> <td>0.0195</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0011</td></tr>
<tr><td>Dataless Model Selection With the Deep Frame Potential <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Murdock_Dataless_Model_Selection_With_the_Deep_Frame_Potential_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Calvin Murdock,  Simon Lucey</td> <td>0.0000</td> <td>0.0000</td> <td>0.0603</td> <td>0.0044</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>UNAS: Differentiable Architecture Search Meets Reinforcement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Vahdat_UNAS_Differentiable_Architecture_Search_Meets_Reinforcement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Arash Vahdat,  Arun Mallya,  Ming-Yu Liu,  Jan Kautz</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2362</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0667</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Local Context Normalization: Revisiting Local Normalization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ortiz_Local_Context_Normalization_Revisiting_Local_Normalization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anthony Ortiz,  Caleb Robinson,  Dan Morris,  Olac Fuentes,  Christopher Kiekintveld,  Md Mahmudulla Hassan,  Nebojsa Jojic</td> <td>0.0160</td> <td>0.0000</td> <td>0.0304</td> <td>0.0000</td> <td>0.0522</td> <td>0.0000</td> <td>0.0344</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ACNe: Attentive Context Normalization for Robust Permutation-Equivariant Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_ACNe_Attentive_Context_Normalization_for_Robust_Permutation-Equivariant_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weiwei Sun,  Wei Jiang,  Eduard Trulls,  Andrea Tagliasacchi,  Kwang Moo Yi</td> <td>0.0000</td> <td>0.0790</td> <td>0.0002</td> <td>0.0000</td> <td>0.0000</td> <td>0.0260</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0082</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0363</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td></tr>
<tr><td>Learning Situational Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ohn-Bar_Learning_Situational_Driving_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eshed Ohn-Bar,  Aditya Prakash,  Aseem Behl,  Kashyap Chitta,  Andreas Geiger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>From Depth What Can You See? Depth Completion via Auxiliary Image Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_From_Depth_What_Can_You_See_Depth_Completion_via_Auxiliary_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaiyue Lu,  Nick Barnes,  Saeed Anwar,  Liang Zheng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0397</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Symmetry and Group in Attribute-Object Compositions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Symmetry_and_Group_in_Attribute-Object_Compositions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yong-Lu Li,  Yue Xu,  Xiaohan Mao,  Cewu Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0600</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1032</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td></tr>
<tr><td>Noise-Aware Fully Webly Supervised Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Noise-Aware_Fully_Webly_Supervised_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunhang Shen,  Rongrong Ji,  Zhiwei Chen,  Xiaopeng Hong,  Feng Zheng,  Jianzhuang Liu,  Mingliang Xu,  Qi Tian</td> <td>0.0151</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0205</td> <td>0.0142</td></tr>
<tr><td>3D Part Guided Image Editing for Fine-Grained Object Understanding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_3D_Part_Guided_Image_Editing_for_Fine-Grained_Object_Understanding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zongdai Liu,  Feixiang Lu,  Peng Wang,  Hui Miao,  Liangjun Zhang,  Ruigang Yang,  Bin Zhou</td> <td>0.0040</td> <td>0.0000</td> <td>0.0439</td> <td>0.0000</td> <td>0.0726</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0812</td> <td>0.0000</td></tr>
<tr><td>STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and Trajectory Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_STINet_Spatio-Temporal-Interactive_Network_for_Pedestrian_Detection_and_Trajectory_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhishuai Zhang,  Jiyang Gao,  Junhua Mao,  Yukai Liu,  Dragomir Anguelov,  Congcong Li</td> <td>0.0292</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0250</td></tr>
<tr><td>Rethinking Performance Estimation in Neural Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Rethinking_Performance_Estimation_in_Neural_Architecture_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiawu Zheng,  Rongrong Ji,  Qiang Wang,  Qixiang Ye,  Zhenguo Li,  Yonghong Tian,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3415</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0726</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Feature-Metric Registration: A Fast Semi-Supervised Approach for Robust Point Cloud Registration Without Correspondences <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Feature-Metric_Registration_A_Fast_Semi-Supervised_Approach_for_Robust_Point_Cloud_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoshui Huang,  Guofeng Mei,  Jian Zhang</td> <td>0.0000</td> <td>0.0894</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0729</td></tr>
<tr><td>Learning Multi-View Camera Relocalization With Graph Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xue_Learning_Multi-View_Camera_Relocalization_With_Graph_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fei Xue,  Xin Wu,  Shaojun Cai,  Junqiu Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0337</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0242</td></tr>
<tr><td>MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_MotionNet_Joint_Perception_and_Motion_Prediction_for_Autonomous_Driving_Based_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pengxiang Wu,  Siheng Chen,  Dimitris N. Metaxas</td> <td>0.0048</td> <td>0.0523</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0524</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0452</td></tr>
<tr><td>EcoNAS: Finding Proxies for Economical Neural Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_EcoNAS_Finding_Proxies_for_Economical_Neural_Architecture_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dongzhan Zhou,  Xinchi Zhou,  Wenwei Zhang,  Chen Change Loy,  Shuai Yi,  Xuesen Zhang,  Wanli Ouyang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2473</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0732</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Hit-Detector_Hierarchical_Trinity_Architecture_Search_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianyuan Guo,  Kai Han,  Yunhe Wang,  Chao Zhang,  Zhaohui Yang,  Han Wu,  Xinghao Chen,  Chang Xu</td> <td>0.0327</td> <td>0.0000</td> <td>0.0000</td> <td>0.2440</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0102</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Geometrically Principled Connections in Graph Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gong_Geometrically_Principled_Connections_in_Graph_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shunwang Gong,  Mehdi Bahri,  Michael M. Bronstein,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0241</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On Vocabulary Reliance in Scene Text Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_On_Vocabulary_Reliance_in_Scene_Text_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhaoyi Wan,  Jielei Zhang,  Liang Zhang,  Jiebo Luo,  Cong Yao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Generating Accurate Pseudo-Labels in Semi-Supervised Learning and Avoiding Overconfident Predictions via Hermite Polynomial Activations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lokhande_Generating_Accurate_Pseudo-Labels_in_Semi-Supervised_Learning_and_Avoiding_Overconfident_Predictions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vishnu Suresh Lokhande,  Songwong Tasneeyapant,  Abhay Venkatesh,  Sathya N. Ravi,  Vikas Singh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hao-Shu Fang,  Chenxi Wang,  Minghao Gou,  Cewu Lu</td> <td>0.0000</td> <td>0.0255</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0257</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0239</td> <td>0.0026</td></tr>
<tr><td>PFRL: Pose-Free Reinforcement Learning for 6D Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_PFRL_Pose-Free_Reinforcement_Learning_for_6D_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jianzhun Shao,  Yuhang Jiang,  Gu Wang,  Zhigang Li,  Xiangyang Ji</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0245</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0302</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Through Fog High-Resolution Imaging Using Millimeter Wave Radar <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guan_Through_Fog_High-Resolution_Imaging_Using_Millimeter_Wave_Radar_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junfeng Guan,  Sohrab Madani,  Suraj Jog,  Saurabh Gupta,  Haitham Hassanieh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0029</td></tr>
<tr><td>Disentangling Physical Dynamics From Unknown Factors for Unsupervised Video Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Le_Guen_Disentangling_Physical_Dynamics_From_Unknown_Factors_for_Unsupervised_Video_Prediction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vincent Le Guen,  Nicolas Thome</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0209</td></tr>
<tr><td>D2Det: Towards High Quality Object Detection and Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiale Cao,  Hisham Cholakkal,  Rao Muhammad Anwer,  Fahad Shahbaz Khan,  Yanwei Pang,  Ling Shao</td> <td>0.0314</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1840</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0829</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0025</td></tr>
<tr><td>LiDAR-Based Online 3D Video Object Detection With Graph-Based Message Passing and Spatiotemporal Transformer Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_LiDAR-Based_Online_3D_Video_Object_Detection_With_Graph-Based_Message_Passing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Junbo Yin,  Jianbing Shen,  Chenye Guan,  Dingfu Zhou,  Ruigang Yang</td> <td>0.0385</td> <td>0.1034</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0525</td></tr>
<tr><td>Orthogonal Convolutional Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Orthogonal_Convolutional_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiayun Wang,  Yubei Chen,  Rudrasis Chakraborty,  Stella X. Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0522</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Robust 3D Point Recognition via Gather-Vector Guidance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Self-Robust_3D_Point_Recognition_via_Gather-Vector_Guidance_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoyi Dong,  Dongdong Chen,  Hang Zhou,  Gang Hua,  Weiming Zhang,  Nenghai Yu</td> <td>0.0000</td> <td>0.0893</td> <td>0.0233</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0918</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0262</td></tr>
<tr><td>VectorNet: Encoding HD Maps and Agent Dynamics From Vectorized Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_VectorNet_Encoding_HD_Maps_and_Agent_Dynamics_From_Vectorized_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiyang Gao,  Chen Sun,  Hang Zhao,  Yi Shen,  Dragomir Anguelov,  Congcong Li,  Cordelia Schmid</td> <td>0.0000</td> <td>0.0000</td> <td>0.0495</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0045</td></tr>
<tr><td>ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qilong Wang,  Banggu Wu,  Pengfei Zhu,  Peihua Li,  Wangmeng Zuo,  Qinghua Hu</td> <td>0.0117</td> <td>0.0000</td> <td>0.0333</td> <td>0.0000</td> <td>0.0380</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0042</td></tr>
<tr><td>MTL-NAS: Task-Agnostic Neural Architecture Search Towards General-Purpose Multi-Task Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_MTL-NAS_Task-Agnostic_Neural_Architecture_Search_Towards_General-Purpose_Multi-Task_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuan Gao,  Haoping Bai,  Zequn Jie,  Jiayi Ma,  Kui Jia,  Wei Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2076</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0097</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liang_PnPNet_End-to-End_Perception_and_Prediction_With_Tracking_in_the_Loop_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ming Liang,  Bin Yang,  Wenyuan Zeng,  Yun Chen,  Rui Hu,  Sergio Casas,  Raquel Urtasun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Revisiting the Sibling Head in Object Detector <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Song_Revisiting_the_Sibling_Head_in_Object_Detector_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guanglu Song,  Yu Liu,  Xiaogang Wang</td> <td>0.0464</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Visual Reaction: Learning to Play Catch With Your Drone <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Visual_Reaction_Learning_to_Play_Catch_With_Your_Drone_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kuo-Hao Zeng,  Roozbeh Mottaghi,  Luca Weihs,  Ali Farhadi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Prime Sample Attention in Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_Prime_Sample_Attention_in_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuhang Cao,  Kai Chen,  Chen Change Loy,  Dahua Lin</td> <td>0.0441</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0715</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Du_SpineNet_Learning_Scale-Permuted_Backbone_for_Recognition_and_Localization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xianzhi Du,  Tsung-Yi Lin,  Pengchong Jin,  Golnaz Ghiasi,  Mingxing Tan,  Yin Cui,  Quoc V. Le,  Xiaodan Song</td> <td>0.0738</td> <td>0.0000</td> <td>0.0297</td> <td>0.1003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_KeyPose_Multi-View_3D_Labeling_and_Keypoint_Estimation_for_Transparent_Objects_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xingyu Liu,  Rico Jonschkowski,  Anelia Angelova,  Kurt Konolige</td> <td>0.0000</td> <td>0.0000</td> <td>0.0297</td> <td>0.0000</td> <td>0.0000</td> <td>0.0555</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0578</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SegGCN: Efficient 3D Point Cloud Segmentation With Fuzzy Spherical Kernel <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lei_SegGCN_Efficient_3D_Point_Cloud_Segmentation_With_Fuzzy_Spherical_Kernel_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Huan Lei,  Naveed Akhtar,  Ajmal Mian</td> <td>0.0000</td> <td>0.0959</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td></tr>
<tr><td>nuScenes: A Multimodal Dataset for Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Holger Caesar,  Varun Bankiti,  Alex H. Lang,  Sourabh Vora,  Venice Erin Liong,  Qiang Xu,  Anush Krishnan,  Yu Pan,  Giancarlo Baldan,  Oscar Beijbom</td> <td>0.0248</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0420</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yisheng He,  Wei Sun,  Haibin Huang,  Jianran Liu,  Haoqiang Fan,  Jian Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0542</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0120</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0607</td></tr>
<tr><td>Probabilistic Pixel-Adaptive Refinement Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wannenwetsch_Probabilistic_Pixel-Adaptive_Refinement_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anne S. Wannenwetsch,  Stefan Roth</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0581</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0823</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Discovering Human Interactions With Novel Objects via Zero-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Discovering_Human_Interactions_With_Novel_Objects_via_Zero-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Suchen Wang,  Kim-Hui Yap,  Junsong Yuan,  Yap-Peng Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Equalization Loss for Long-Tailed Object Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_Equalization_Loss_for_Long-Tailed_Object_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingru Tan,  Changbao Wang,  Buyu Li,  Quanquan Li,  Wanli Ouyang,  Changqing Yin,  Junjie Yan</td> <td>0.0313</td> <td>0.0000</td> <td>0.0356</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0121</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Depth-Guided Convolutions for Monocular 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ding_Learning_Depth-Guided_Convolutions_for_Monocular_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mingyu Ding,  Yuqi Huo,  Hongwei Yi,  Zhe Wang,  Jianping Shi,  Zhiwu Lu,  Ping Luo</td> <td>0.2088</td> <td>0.0404</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0399</td> <td>0.0000</td> <td>0.0000</td> <td>0.1821</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mario Bijelic,  Tobias Gruber,  Fahim Mannan,  Florian Kraus,  Werner Ritter,  Klaus Dietmayer,  Felix Heide</td> <td>0.0177</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0096</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0290</td> <td>0.0194</td></tr>
<tr><td>Don't Even Look Once: Synthesizing Features for Zero-Shot Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Dont_Even_Look_Once_Synthesizing_Features_for_Zero-Shot_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pengkai Zhu,  Hanxiao Wang,  Venkatesh Saligrama</td> <td>0.0390</td> <td>0.0000</td> <td>0.0224</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>EPOS: Estimating 6D Pose of Objects With Symmetries <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hodan_EPOS_Estimating_6D_Pose_of_Objects_With_Symmetries_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tomas Hodan,  Daniel Barath,  Jiri Matas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0067</td></tr>
<tr><td>Train in Germany, Test in the USA: Making 3D Object Detectors Generalize <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Train_in_Germany_Test_in_the_USA_Making_3D_Object_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yan Wang,  Xiangyu Chen,  Yurong You,  Li Erran Li,  Bharath Hariharan,  Mark Campbell,  Kilian Q. Weinberger,  Wei-Lun Chao</td> <td>0.3235</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0515</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0195</td></tr>
<tr><td>Exploring Categorical Regularization for Domain Adaptive Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Exploring_Categorical_Regularization_for_Domain_Adaptive_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chang-Dong Xu,  Xing-Ran Zhao,  Xin Jin,  Xiu-Shen Wei</td> <td>0.0147</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1616</td> <td>0.0000</td> <td>0.0091</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Neural Implicit Embedding for Point Cloud Analysis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fujiwara_Neural_Implicit_Embedding_for_Point_Cloud_Analysis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kent Fujiwara,  Taiichi Hashimoto</td> <td>0.0000</td> <td>0.1178</td> <td>0.0291</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td></tr>
<tr><td>Pose-Guided Visible Part Matching for Occluded Person ReID <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Pose-Guided_Visible_Part_Matching_for_Occluded_Person_ReID_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shang Gao,  Jingya Wang,  Huchuan Lu,  Zimo Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0726</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0635</td></tr>
<tr><td>ContourNet: Taking a Further Step Toward Accurate Arbitrary-Shaped Scene Text Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_ContourNet_Taking_a_Further_Step_Toward_Accurate_Arbitrary-Shaped_Scene_Text_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuxin Wang,  Hongtao Xie,  Zheng-Jun Zha,  Mengting Xing,  Zilong Fu,  Yongdong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0496</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0211</td></tr>
<tr><td>Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Prakash_Exploring_Data_Aggregation_in_Policy_Learning_for_Vision-Based_Urban_Autonomous_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aditya Prakash,  Aseem Behl,  Eshed Ohn-Bar,  Kashyap Chitta,  Andreas Geiger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Look-Into-Object: Self-Supervised Structure Modeling for Object Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Look-Into-Object_Self-Supervised_Structure_Modeling_for_Object_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohan Zhou,  Yalong Bai,  Wei Zhang,  Tiejun Zhao,  Tao Mei</td> <td>0.0159</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0017</td></tr>
<tr><td>Recognizing Objects From Any View With Object and Viewer-Centered Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Recognizing_Objects_From_Any_View_With_Object_and_Viewer-Centered_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sainan Liu,  Vincent Nguyen,  Isaac Rehg,  Zhuowen Tu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0134</td> <td>0.0000</td> <td>0.0275</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0474</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0218</td></tr>
<tr><td>Gated Channel Transformation for Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Gated_Channel_Transformation_for_Visual_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zongxin Yang,  Linchao Zhu,  Yu Wu,  Yi Yang</td> <td>0.0232</td> <td>0.0000</td> <td>0.0397</td> <td>0.0000</td> <td>0.0658</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0148</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0041</td></tr>
<tr><td>Non-Local Neural Networks With Grouped Bilinear Attentional Transforms <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chi_Non-Local_Neural_Networks_With_Grouped_Bilinear_Attentional_Transforms_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lu Chi,  Zehuan Yuan,  Yadong Mu,  Changhu Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0455</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Generative-Discriminative Feature Representations for Open-Set Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Perera_Generative-Discriminative_Feature_Representations_for_Open-Set_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pramuditha Perera,  Vlad I. Morariu,  Rajiv Jain,  Varun Manjunatha,  Curtis Wigington,  Vicente Ordonez,  Vishal M. Patel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td></tr>
<tr><td>RPM-Net: Robust Point Matching Using Learned Features <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yew_RPM-Net_Robust_Point_Matching_Using_Learned_Features_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zi Jian Yew,  Gim Hee Lee</td> <td>0.0000</td> <td>0.1003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0212</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0236</td></tr>
<tr><td>Sideways: Depth-Parallel Training of Video Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Malinowski_Sideways_Depth-Parallel_Training_of_Video_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mateusz Malinowski,  Grzegorz Swirszcz,  Joao Carreira,  Viorica Patraucean</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Basis Prediction Networks for Effective Burst Denoising With Large Kernels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xia_Basis_Prediction_Networks_for_Effective_Burst_Denoising_With_Large_Kernels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihao Xia,  Federico Perazzi,  Michael Gharbi,  Kalyan Sunkavalli,  Ayan Chakrabarti</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0117</td></tr>
<tr><td>Private-kNN: Practical Differential Privacy for Computer Vision <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Private-kNN_Practical_Differential_Privacy_for_Computer_Vision_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuqing Zhu,  Xiang Yu,  Manmohan Chandraker,  Yu-Xiang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1006</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SP-NAS: Serial-to-Parallel Backbone Search for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_SP-NAS_Serial-to-Parallel_Backbone_Search_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenhan Jiang,  Hang Xu,  Wei Zhang,  Xiaodan Liang,  Zhenguo Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2512</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Structure Aware Single-Stage 3D Object Detection From Point Cloud <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chenhang He,  Hui Zeng,  Jianqiang Huang,  Xian-Sheng Hua,  Lei Zhang</td> <td>0.0592</td> <td>0.1327</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>"Looking at the Right Stuff" - Guided Semantic-Gaze for Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pal_Looking_at_the_Right_Stuff_-_Guided_Semantic-Gaze_for_Autonomous_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anwesan Pal,  Sayan Mondal,  Henrik I. Christensen</td> <td>0.0037</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0212</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>What's Hidden in a Randomly Weighted Neural Network? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vivek Ramanujan,  Mitchell Wortsman,  Aniruddha Kembhavi,  Ali Farhadi,  Mohammad Rastegari</td> <td>0.0000</td> <td>0.0000</td> <td>0.0766</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Structured Multi-Hashing for Model Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Eban_Structured_Multi-Hashing_for_Model_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Elad Eban,  Yair Movshovitz-Attias,  Hao Wu,  Mark Sandler,  Andrew Poon,  Yerlan Idelbayev,  Miguel A. Carreira-Perpinan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0425</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DOPS: Learning to Detect 3D Objects and Predict Their 3D Shapes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Najibi_DOPS_Learning_to_Detect_3D_Objects_and_Predict_Their_3D_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mahyar Najibi,  Guangda Lai,  Abhijit Kundu,  Zhichao Lu,  Vivek Rathod,  Thomas Funkhouser,  Caroline Pantofaru,  David Ross,  Larry S. Davis,  Alireza Fathi</td> <td>0.2097</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0361</td></tr>
<tr><td>AutoTrack: Towards High-Performance Visual Tracking for UAV With Automatic Spatio-Temporal Regularization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_AutoTrack_Towards_High-Performance_Visual_Tracking_for_UAV_With_Automatic_Spatio-Temporal_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiming Li,  Changhong Fu,  Fangqiang Ding,  Ziyuan Huang,  Geng Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0725</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GP-NAS: Gaussian Process Based Neural Architecture Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_GP-NAS_Gaussian_Process_Based_Neural_Architecture_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihang Li,  Teng Xi,  Jiankang Deng,  Gang Zhang,  Shengzhao Wen,  Ran He</td> <td>0.0000</td> <td>0.0000</td> <td>0.0363</td> <td>0.2053</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0655</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>NAS-FCOS: Fast Neural Architecture Search for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_NAS-FCOS_Fast_Neural_Architecture_Search_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ning Wang,  Yang Gao,  Hao Chen,  Peng Wang,  Zhi Tian,  Chunhua Shen,  Yanning Zhang</td> <td>0.0736</td> <td>0.0000</td> <td>0.0263</td> <td>0.2007</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0014</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>TCTS: A Task-Consistent Two-Stage Framework for Person Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_TCTS_A_Task-Consistent_Two-Stage_Framework_for_Person_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cheng Wang,  Bingpeng Ma,  Hong Chang,  Shiguang Shan,  Xilin Chen</td> <td>0.0007</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SCATTER: Selective Context Attentional Scene Text Recognizer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Litman_SCATTER_Selective_Context_Attentional_Scene_Text_Recognizer_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ron Litman,  Oron Anschel,  Shahar Tsiper,  Roee Litman,  Shai Mazor,  R. Manmatha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Dengsheng Chen,  Jun Li,  Zheng Wang,  Kai Xu</td> <td>0.0000</td> <td>0.1000</td> <td>0.0341</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Hierarchical Scene Coordinate Classification and Regression for Visual Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Hierarchical_Scene_Coordinate_Classification_and_Regression_for_Visual_Localization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaotian Li,  Shuzhe Wang,  Yi Zhao,  Jakob Verbeek,  Juho Kannala</td> <td>0.0000</td> <td>0.0000</td> <td>0.0238</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0212</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0184</td></tr>
<tr><td>MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_MiLeNAS_Efficient_Neural_Architecture_Search_via_Mixed-Level_Reformulation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaoyang He,  Haishan Ye,  Li Shen,  Tong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2895</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0236</td></tr>
<tr><td>Scalable Uncertainty for Computer Vision With Functional Variational Inference <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Carvalho_Scalable_Uncertainty_for_Computer_Vision_With_Functional_Variational_Inference_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eduardo D. C. Carvalho,  Ronald Clark,  Andrea Nicastro,  Paul H. J. Kelly</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0678</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0223</td> <td>0.0000</td> <td>0.0595</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td></tr>
<tr><td>Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Eldesokey_Uncertainty-Aware_CNNs_for_Depth_Completion_Uncertainty_from_Beginning_to_End_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abdelrahman Eldesokey,  Michael Felsberg,  Karl Holmquist,  Michael Persson</td> <td>0.0000</td> <td>0.0000</td> <td>0.0270</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0548</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td></tr>
<tr><td>Butterfly Transform: An Efficient FFT Based Neural Architecture Design <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/vahid_Butterfly_Transform_An_Efficient_FFT_Based_Neural_Architecture_Design_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Keivan Alizadeh vahid,  Anish Prabhu,  Ali Farhadi,  Mohammad Rastegari</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0776</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0076</td></tr>
<tr><td>A Certifiably Globally Optimal Solution to Generalized Essential Matrix Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_A_Certifiably_Globally_Optimal_Solution_to_Generalized_Essential_Matrix_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ji Zhao,  Wanting Xu,  Laurent Kneip</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MUXConv: Information Multiplexing in Convolutional Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_MUXConv_Information_Multiplexing_in_Convolutional_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhichao Lu,  Kalyanmoy Deb,  Vishnu Naresh Boddeti</td> <td>0.0277</td> <td>0.0000</td> <td>0.0412</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PointGMM: A Neural GMM Network for Point Clouds <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hertz_PointGMM_A_Neural_GMM_Network_for_Point_Clouds_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Amir Hertz,  Rana Hanocka,  Raja Giryes,  Daniel Cohen-Or</td> <td>0.0000</td> <td>0.0852</td> <td>0.0158</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Noisier2Noise: Learning to Denoise From Unpaired Noisy Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Moran_Noisier2Noise_Learning_to_Denoise_From_Unpaired_Noisy_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nick Moran,  Dan Schmidt,  Yu Zhong,  Patrick Coady</td> <td>0.0000</td> <td>0.0000</td> <td>0.0116</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0328</td> <td>0.0000</td></tr>
<tr><td>TRPLP - Trifocal Relative Pose From Lines at Points <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fabbri_TRPLP_-_Trifocal_Relative_Pose_From_Lines_at_Points_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ricardo Fabbri,  Timothy Duff,  Hongyi Fan,  Margaret H. Regan,  David da Costa de Pinho,  Elias Tsigaridas,  Charles W. Wampler,  Jonathan D. Hauenstein,  Peter J. Giblin,  Benjamin Kimia,  Anton Leykin,  Tomas Pajdla</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0367</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DSNAS: Direct Neural Architecture Search Without Parameter Retraining <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_DSNAS_Direct_Neural_Architecture_Search_Without_Parameter_Retraining_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shoukang Hu,  Sirui Xie,  Hehui Zheng,  Chunxiao Liu,  Jianping Shi,  Xunying Liu,  Dahua Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0274</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0760</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_MonoPair_Monocular_3D_Object_Detection_Using_Pairwise_Spatial_Relationships_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yongjian Chen,  Lei Tai,  Kai Sun,  Mingyang Li</td> <td>0.2337</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0022</td></tr>
<tr><td>Regularization on Spatio-Temporally Smoothed Feature for Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Regularization_on_Spatio-Temporally_Smoothed_Feature_for_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jinhyung Kim,  Seunghwan Cha,  Dongyoon Wee,  Soonmin Bae,  Junmo Kim</td> <td>0.0000</td> <td>0.0000</td> <td>0.0324</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1840</td> <td>0.0000</td> <td>0.0067</td></tr>
<tr><td>Towards Accurate Scene Text Recognition With Semantic Reasoning Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Towards_Accurate_Scene_Text_Recognition_With_Semantic_Reasoning_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Deli Yu,  Xuan Li,  Chengquan Zhang,  Tao Liu,  Junyu Han,  Jingtuo Liu,  Errui Ding</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0113</td></tr>
<tr><td>Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Unsupervised_Reinforcement_Learning_of_Transferable_Meta-Skills_for_Embodied_Navigation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Juncheng Li,  Xin Wang,  Siliang Tang,  Haizhou Shi,  Fei Wu,  Yueting Zhuang,  William Yang Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0216</td> <td>0.0069</td></tr>
<tr><td>Inferring Attention Shift Ranks of Objects for Image Saliency <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Siris_Inferring_Attention_Shift_Ranks_of_Objects_for_Image_Saliency_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Avishek Siris,  Jianbo Jiao,  Gary K.L. Tam,  Xianghua Xie,  Rynson W.H. Lau</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0059</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0132</td></tr>
<tr><td>Camera On-Boarding for Person Re-Identification Using Hypothesis Transfer Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ahmed_Camera_On-Boarding_for_Person_Re-Identification_Using_Hypothesis_Transfer_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sk Miraj Ahmed,  Aske R. Lejbolle,  Rameswar Panda,  Amit K. Roy-Chowdhury</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0114</td></tr>
<tr><td>Joint Graph-Based Depth Refinement and Normal Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rossi_Joint_Graph-Based_Depth_Refinement_and_Normal_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mattia Rossi,  Mireille El Gheche,  Andreas Kuhn,  Pascal Frossard</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3180</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DR Loss: Improving Object Detection by Distributional Ranking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qian_DR_Loss_Improving_Object_Detection_by_Distributional_Ranking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Qian,  Lei Chen,  Hao Li,  Rong Jin</td> <td>0.0250</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0110</td></tr>
<tr><td>Self-Trained Deep Ordinal Regression for End-to-End Video Anomaly Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_Self-Trained_Deep_Ordinal_Regression_for_End-to-End_Video_Anomaly_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Guansong Pang,  Cheng Yan,  Chunhua Shen,  Anton van den Hengel,  Xiao Bai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0605</td> <td>0.0756</td></tr>
<tr><td>Few-Shot Class-Incremental Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Few-Shot_Class-Incremental_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoyu Tao,  Xiaopeng Hong,  Xinyuan Chang,  Songlin Dong,  Xing Wei,  Yihong Gong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1015</td></tr>
<tr><td>PolarMask: Single Shot Instance Segmentation With Polar Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Enze Xie,  Peize Sun,  Xiaoge Song,  Wenhai Wang,  Xuebo Liu,  Ding Liang,  Chunhua Shen,  Ping Luo</td> <td>0.0188</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3467</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover's Distance and Structured Classifiers <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chi Zhang,  Yujun Cai,  Guosheng Lin,  Chunhua Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Detection in Crowded Scenes: One Proposal, Multiple Predictions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chu_Detection_in_Crowded_Scenes_One_Proposal_Multiple_Predictions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xuangeng Chu,  Anlin Zheng,  Xiangyu Zhang,  Jian Sun</td> <td>0.0027</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0374</td></tr>
<tr><td>Autolabeling 3D Objects With Differentiable Rendering of SDF Shape Priors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zakharov_Autolabeling_3D_Objects_With_Differentiable_Rendering_of_SDF_Shape_Priors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sergey Zakharov,  Wadim Kehl,  Arjun Bhargava,  Adrien Gaidon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Interactive Object Segmentation With Inside-Outside Guidance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Interactive_Object_Segmentation_With_Inside-Outside_Guidance_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shiyin Zhang,  Jun Hao Liew,  Yunchao Wei,  Shikui Wei,  Yao Zhao</td> <td>0.0080</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0843</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Mnemonics Training: Multi-Class Incremental Learning Without Forgetting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Mnemonics_Training_Multi-Class_Incremental_Learning_Without_Forgetting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaoyao Liu,  Yuting Su,  An-An Liu,  Bernt Schiele,  Qianru Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Segment 3D Point Clouds in 2D Image Space <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lyu_Learning_to_Segment_3D_Point_Clouds_in_2D_Image_Space_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yecheng Lyu,  Xinming Huang,  Ziming Zhang</td> <td>0.0000</td> <td>0.2395</td> <td>0.0534</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0748</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Smooth Shells: Multi-Scale Shape Registration With Functional Maps <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Eisenberger_Smooth_Shells_Multi-Scale_Shape_Registration_With_Functional_Maps_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Marvin Eisenberger,  Zorah Lahner,  Daniel Cremers</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Self-Supervised_Equivariant_Attention_Mechanism_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yude Wang,  Jie Zhang,  Meina Kan,  Shiguang Shan,  Xilin Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0978</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0605</td></tr>
<tr><td>Efficient Neural Vision Systems Based on Convolutional Image Acquisition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pad_Efficient_Neural_Vision_Systems_Based_on_Convolutional_Image_Acquisition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Pedram Pad,  Simon Narduzzi,  Clement Kundig,  Engin Turetken,  Siavash A. Bigdeli,  L. Andrea Dunbar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0453</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0121</td></tr>
<tr><td>Visual Chirality <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Visual_Chirality_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhiqiu Lin,  Jin Sun,  Abe Davis,  Noah Snavely</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0540</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_What_Machines_See_Is_Not_What_They_Get_Fooling_Scene_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xing Xu,  Jiefu Chen,  Jinhui Xiao,  Lianli Gao,  Fumin Shen,  Heng Tao Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0276</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0822</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0127</td></tr>
<tr><td>Dynamic Traffic Modeling From Overhead Imagery <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Workman_Dynamic_Traffic_Modeling_From_Overhead_Imagery_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Scott Workman,  Nathan Jacobs</td> <td>0.0000</td> <td>0.0000</td> <td>0.0421</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0324</td></tr>
<tr><td>Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vivien Sainte Fare Garnot,  Loic Landrieu,  Sebastien Giordano,  Nesrine Chehata</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0356</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DAVD-Net_Deep_Audio-Aided_Video_Decompression_of_Talking_Heads_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xi Zhang,  Xiaolin Wu,  Xinliang Zhai,  Xianye Ben,  Chengjie Tu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0261</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0252</td></tr>
<tr><td>Learning When and Where to Zoom With Deep Reinforcement Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Uzkent_Learning_When_and_Where_to_Zoom_With_Deep_Reinforcement_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Burak Uzkent,  Stefano Ermon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cross-Domain Detection via Graph-Induced Prototype Alignment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Cross-Domain_Detection_via_Graph-Induced_Prototype_Alignment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Minghao Xu,  Hang Wang,  Bingbing Ni,  Qi Tian,  Wenjun Zhang</td> <td>0.0279</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1263</td> <td>0.0000</td> <td>0.0568</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Meta-Learning of Neural Architectures for Few-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Elsken_Meta-Learning_of_Neural_Architectures_for_Few-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thomas Elsken,  Benedikt Staffler,  Jan Hendrik Metzen,  Frank Hutter</td> <td>0.0154</td> <td>0.0000</td> <td>0.0000</td> <td>0.2366</td> <td>0.0000</td> <td>0.0000</td> <td>0.0375</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Towards Inheritable Models for Open-Set Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Towards_Inheritable_Models_for_Open-Set_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jogendra Nath Kundu,  Naveen Venkat,  Ambareesh Revanur,  Rahul M V,  R. Venkatesh Babu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2476</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning From Synthetic Animals <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mu_Learning_From_Synthetic_Animals_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiteng Mu,  Weichao Qiu,  Gregory D. Hager,  Alan L. Yuille</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0109</td> <td>0.0231</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Distilling Cross-Task Knowledge via Relationship Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Distilling_Cross-Task_Knowledge_via_Relationship_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Han-Jia Ye,  Su Lu,  De-Chuan Zhan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0572</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0011</td></tr>
<tr><td>Open Compound Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Open_Compound_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziwei Liu,  Zhongqi Miao,  Xingang Pan,  Xiaohang Zhan,  Dahua Lin,  Stella X. Yu,  Boqing Gong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0444</td> <td>0.2274</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Context Prior for Scene Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Context_Prior_for_Scene_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Changqian Yu,  Jingbo Wang,  Changxin Gao,  Gang Yu,  Chunhua Shen,  Nong Sang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0215</td> <td>0.0000</td> <td>0.0000</td> <td>0.0305</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Tangent Images for Mitigating Spherical Distortion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Eder_Tangent_Images_for_Mitigating_Spherical_Distortion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Marc Eder,  Mykhailo Shvets,  John Lim,  Jan-Michael Frahm</td> <td>0.0000</td> <td>0.0000</td> <td>0.0254</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0978</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning a Dynamic Map of Visual Appearance <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Salem_Learning_a_Dynamic_Map_of_Visual_Appearance_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tawfiq Salem,  Scott Workman,  Nathan Jacobs</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Webly Supervised Knowledge Embedding Model for Visual Reasoning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Webly_Supervised_Knowledge_Embedding_Model_for_Visual_Reasoning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenbo Zheng,  Lan Yan,  Chao Gou,  Fei-Yue Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0207</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0118</td></tr>
<tr><td>Gradually Vanishing Bridge for Adversarial Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cui_Gradually_Vanishing_Bridge_for_Adversarial_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shuhao Cui,  Shuhui Wang,  Junbao Zhuo,  Chi Su,  Qingming Huang,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1325</td> <td>0.0000</td> <td>0.0528</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Active Speakers in Context <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Alcazar_Active_Speakers_in_Context_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Juan Leon Alcazar,  Fabian Caba,  Long Mai,  Federico Perazzi,  Joon-Young Lee,  Pablo Arbelaez,  Bernard Ghanem</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Panoptic-DeepLab_A_Simple_Strong_and_Fast_Baseline_for_Bottom-Up_Panoptic_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bowen Cheng,  Maxwell D. Collins,  Yukun Zhu,  Ting Liu,  Thomas S. Huang,  Hartwig Adam,  Liang-Chieh Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1537</td> <td>0.0000</td> <td>0.1191</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Inter-Region Affinity Distillation for Road Marking Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hou_Inter-Region_Affinity_Distillation_for_Road_Marking_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuenan Hou,  Zheng Ma,  Chunxiao Liu,  Tak-Wai Hui,  Chen Change Loy</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0529</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0116</td></tr>
<tr><td>Unified Dynamic Convolutional Network for Super-Resolution With Variational Degradations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Unified_Dynamic_Convolutional_Network_for_Super-Resolution_With_Variational_Degradations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu-Syuan Xu,  Shou-Yao Roy Tseng,  Yu Tseng,  Hsien-Kai Kuo,  Yi-Min Tsai</td> <td>0.0000</td> <td>0.0000</td> <td>0.0367</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0060</td></tr>
<tr><td>Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bertinetto_Making_Better_Mistakes_Leveraging_Class_Hierarchies_With_Deep_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Luca Bertinetto,  Romain Mueller,  Konstantinos Tertikas,  Sina Samangooei,  Nicholas A. Lord</td> <td>0.0000</td> <td>0.0000</td> <td>0.0559</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0564</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Data-Free_Knowledge_Amalgamation_via_Group-Stack_Dual-GAN_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jingwen Ye,  Yixin Ji,  Xinchao Wang,  Xin Gao,  Mingli Song</td> <td>0.0000</td> <td>0.0000</td> <td>0.0395</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1696</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0745</td> <td>0.0314</td></tr>
<tr><td>Screencast Tutorial Video Understanding <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Screencast_Tutorial_Video_Understanding_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kunpeng Li,  Chen Fang,  Zhaowen Wang,  Seokhwan Kim,  Hailin Jin,  Yun Fu</td> <td>0.0125</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0229</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0052</td></tr>
<tr><td>DSGN: Deep Stereo Geometry Network for 3D Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_DSGN_Deep_Stereo_Geometry_Network_for_3D_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yilun Chen,  Shu Liu,  Xiaoyong Shen,  Jiaya Jia</td> <td>0.1771</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0094</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly-Supervised Salient Object Detection via Scribble Annotations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Weakly-Supervised_Salient_Object_Detection_via_Scribble_Annotations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jing Zhang,  Xin Yu,  Aixuan Li,  Peipei Song,  Bowen Liu,  Yuchao Dai</td> <td>0.0531</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0076</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0210</td></tr>
<tr><td>Learning to Learn Single Domain Generalization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Learning_to_Learn_Single_Domain_Generalization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fengchun Qiao,  Long Zhao,  Xi Peng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0053</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0043</td></tr>
<tr><td>Severity-Aware Semantic Segmentation With Reinforced Wasserstein Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Severity-Aware_Semantic_Segmentation_With_Reinforced_Wasserstein_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaofeng Liu,  Wenxuan Ji,  Jane You,  Georges El Fakhri,  Jonghye Woo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0402</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0460</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Boosting Few-Shot Learning With Adaptive Margin Loss <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Boosting_Few-Shot_Learning_With_Adaptive_Margin_Loss_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Aoxue Li,  Weiran Huang,  Xu Lan,  Jiashi Feng,  Zhenguo Li,  Liwei Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0439</td></tr>
<tr><td>JA-POLS: A Moving-Camera Background Model via Joint Alignment and Partially-Overlapping Local Subspaces <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chelly_JA-POLS_A_Moving-Camera_Background_Model_via_Joint_Alignment_and_Partially-Overlapping_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Irit Chelly,  Vlad Winter,  Dor Litvak,  David Rosen,  Oren Freifeld</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0762</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0474</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>AugFPN: Improving Multi-Scale Feature Learning for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_AugFPN_Improving_Multi-Scale_Feature_Learning_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaoxu Guo,  Bin Fan,  Qian Zhang,  Shiming Xiang,  Chunhong Pan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0415</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jaritz_xMUDA_Cross-Modal_Unsupervised_Domain_Adaptation_for_3D_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Maximilian Jaritz,  Tuan-Hung Vu,  Raoul de Charette,  Emilie Wirbel,  Patrick Perez</td> <td>0.0000</td> <td>0.0796</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0609</td> <td>0.1523</td> <td>0.0000</td> <td>0.0755</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Norm-Aware Embedding for Efficient Person Search <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Di Chen,  Shanshan Zhang,  Jian Yang,  Bernt Schiele</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Intelligent Home 3D: Automatic 3D-House Design From Linguistic Descriptions Only <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Intelligent_Home_3D_Automatic_3D-House_Design_From_Linguistic_Descriptions_Only_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Chen,  Qi Wu,  Rui Tang,  Yuhan Wang,  Shuai Wang,  Mingkui Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Differential_Treatment_for_Stuff_and_Things_A_Simple_Unsupervised_Domain_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhonghao Wang,  Mo Yu,  Yunchao Wei,  Rogerio Feris,  Jinjun Xiong,  Wen-mei Hwu,  Thomas S. Huang,  Honghui Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0297</td> <td>0.3753</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0054</td></tr>
<tr><td>Robust Object Detection Under Occlusion With Context-Aware CompositionalNets <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Robust_Object_Detection_Under_Occlusion_With_Context-Aware_CompositionalNets_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Angtian Wang,  Yihong Sun,  Adam Kortylewski,  Alan L. Yuille</td> <td>0.0222</td> <td>0.0000</td> <td>0.0199</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0066</td></tr>
<tr><td>IMRAM: Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_IMRAM_Iterative_Matching_With_Recurrent_Attention_Memory_for_Cross-Modal_Image-Text_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hui Chen,  Guiguang Ding,  Xudong Liu,  Zijia Lin,  Ji Liu,  Jungong Han</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0235</td></tr>
<tr><td>Domain-Aware Visual Bias Eliminating for Generalized Zero-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Min_Domain-Aware_Visual_Bias_Eliminating_for_Generalized_Zero-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaobo Min,  Hantao Yao,  Hongtao Xie,  Chaoqun Wang,  Zheng-Jun Zha,  Yongdong Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0179</td></tr>
<tr><td>Semi-Supervised Semantic Segmentation With Cross-Consistency Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ouali_Semi-Supervised_Semantic_Segmentation_With_Cross-Consistency_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yassine Ouali,  Celine Hudelot,  Myriam Tami</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0918</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0491</td></tr>
<tr><td>Learning to Learn Cropping Models for Different Aspect Ratio Requirements <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Learning_to_Learn_Cropping_Models_for_Different_Aspect_Ratio_Requirements_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Debang Li,  Junge Zhang,  Kaiqi Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0781</td></tr>
<tr><td>What Makes Training Multi-Modal Classification Networks Hard? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_What_Makes_Training_Multi-Modal_Classification_Networks_Hard_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weiyao Wang,  Du Tran,  Matt Feiszli</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1896</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Selective Transfer With Reinforced Transfer Network for Partial Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Selective_Transfer_With_Reinforced_Transfer_Network_for_Partial_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhihong Chen,  Chao Chen,  Zhaowei Cheng,  Boyuan Jiang,  Ke Fang,  Xinyu Jin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1219</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0036</td></tr>
<tr><td>Semi-Supervised Semantic Image Segmentation With Self-Correcting Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ibrahim_Semi-Supervised_Semantic_Image_Segmentation_With_Self-Correcting_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mostafa S. Ibrahim,  Arash Vahdat,  Mani Ranjbar,  William G. Macready</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1174</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Exemplar Normalization for Learning Deep Representation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Exemplar_Normalization_for_Learning_Deep_Representation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ruimao Zhang,  Zhanglin Peng,  Lingyun Wu,  Zhen Li,  Ping Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0114</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0289</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Imitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mengshi Qi,  Jie Qin,  Yu Wu,  Yi Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0080</td></tr>
<tr><td>Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Multi-Modal_Graph_Neural_Network_for_Joint_Reasoning_on_Vision_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Difei Gao,  Ke Li,  Ruiping Wang,  Shiguang Shan,  Xilin Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0130</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_StereoGAN_Bridging_Synthetic-to-Real_Domain_Gap_by_Joint_Optimization_of_Domain_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rui Liu,  Chengxi Yang,  Wenxiu Sun,  Xiaogang Wang,  Hongsheng Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0145</td></tr>
<tr><td>Self-Supervised Domain-Aware Generative Network for Generalized Zero-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Self-Supervised_Domain-Aware_Generative_Network_for_Generalized_Zero-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiamin Wu,  Tianzhu Zhang,  Zheng-Jun Zha,  Jiebo Luo,  Yongdong Zhang,  Feng Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1255</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0012</td></tr>
<tr><td>Sparse Layered Graphs for Multi-Object Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jeppesen_Sparse_Layered_Graphs_for_Multi-Object_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Niels Jeppesen,  Anders N. Christensen,  Vedrana A. Dahl,  Anders B. Dahl</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Visual-Semantic Matching by Exploring High-Order Attention and Distraction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Visual-Semantic_Matching_by_Exploring_High-Order_Attention_and_Distraction_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yongzhi Li,  Duo Zhang,  Yadong Mu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0347</td> <td>0.0000</td> <td>0.0000</td> <td>0.0578</td> <td>0.0000</td> <td>0.0271</td></tr>
<tr><td>End-to-End 3D Point Cloud Instance Segmentation Without Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_End-to-End_3D_Point_Cloud_Instance_Segmentation_Without_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haiyong Jiang,  Feilong Yan,  Jianfei Cai,  Jianmin Zheng,  Jun Xiao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1847</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0224</td></tr>
<tr><td>Deep Adversarial Decomposition: A Unified Framework for Separating Superimposed Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zou_Deep_Adversarial_Decomposition_A_Unified_Framework_for_Separating_Superimposed_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhengxia Zou,  Sen Lei,  Tianyang Shi,  Zhenwei Shi,  Jieping Ye</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0026</td> <td>0.0736</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0212</td></tr>
<tr><td>Differentiable Adaptive Computation Time for Visual Reasoning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Eyzaguirre_Differentiable_Adaptive_Computation_Time_for_Visual_Reasoning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Cristobal Eyzaguirre,  Alvaro Soto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DeepLPF: Deep Local Parametric Filters for Image Enhancement <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Moran_DeepLPF_Deep_Local_Parametric_Filters_for_Image_Enhancement_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sean Moran,  Pierre Marza,  Steven McDonagh,  Sarah Parisot,  Gregory Slabaugh</td> <td>0.0000</td> <td>0.0000</td> <td>0.0423</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Instance Credibility Inference for Few-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yikai Wang,  Chengming Xu,  Chen Liu,  Li Zhang,  Yanwei Fu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0614</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0406</td> <td>0.0115</td></tr>
<tr><td>Learning From Web Data With Self-Organizing Memory Module <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tu_Learning_From_Web_Data_With_Self-Organizing_Memory_Module_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yi Tu,  Li Niu,  Junjie Chen,  Dawei Cheng,  Liqing Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0436</td></tr>
<tr><td>TransMatch: A Transfer-Learning Scheme for Semi-Supervised Few-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_TransMatch_A_Transfer-Learning_Scheme_for_Semi-Supervised_Few-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhongjie Yu,  Lin Chen,  Zhongwei Cheng,  Jiebo Luo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0477</td></tr>
<tr><td>Learning the Redundancy-Free Features for Generalized Zero-Shot Object Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_Learning_the_Redundancy-Free_Features_for_Generalized_Zero-Shot_Object_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zongyan Han,  Zhenyong Fu,  Jian Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0001</td></tr>
<tr><td>Neural Topological SLAM for Visual Navigation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chaplot_Neural_Topological_SLAM_for_Visual_Navigation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Devendra Singh Chaplot,  Ruslan Salakhutdinov,  Abhinav Gupta,  Saurabh Gupta</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0033</td></tr>
<tr><td>WaveletStereo: Learning Wavelet Coefficients of Disparity Map in Stereo Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_WaveletStereo_Learning_Wavelet_Coefficients_of_Disparity_Map_in_Stereo_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Menglong Yang,  Fangrui Wu,  Wei Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0803</td></tr>
<tr><td>Robust Superpixel-Guided Attentional Adversarial Attack <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Robust_Superpixel-Guided_Attentional_Adversarial_Attack_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiaoyi Dong,  Jiangfan Han,  Dongdong Chen,  Jiayang Liu,  Huanyu Bian,  Zehua Ma,  Hongsheng Li,  Xiaogang Wang,  Weiming Zhang,  Nenghai Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0278</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0603</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0048</td></tr>
<tr><td>BEDSR-Net: A Deep Shadow Removal Network From a Single Document Image <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_BEDSR-Net_A_Deep_Shadow_Removal_Network_From_a_Single_Document_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yun-Hsuan Lin,  Wen-Chin Chen,  Yung-Yu Chuang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cross-Domain Document Object Detection: Benchmark Suite and Method <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Cross-Domain_Document_Object_Detection_Benchmark_Suite_and_Method_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Li,  Curtis Wigington,  Chris Tensmeyer,  Handong Zhao,  Nikolaos Barmpalios,  Vlad I. Morariu,  Varun Manjunatha,  Tong Sun,  Yun Fu</td> <td>0.0186</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1570</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0294</td></tr>
<tr><td>Explaining Knowledge Distillation by Quantifying the Knowledge <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xu Cheng,  Zhefan Rao,  Yilan Chen,  Quanshi Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0307</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Exploring Bottom-Up and Top-Down Cues With Attentive Learning for Webly Supervised Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Exploring_Bottom-Up_and_Top-Down_Cues_With_Attentive_Learning_for_Webly_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhonghua Wu,  Qingyi Tao,  Guosheng Lin,  Jianfei Cai</td> <td>0.0422</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0110</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0316</td></tr>
<tr><td>Enhancing Generic Segmentation With Learned Region Representations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Isaacs_Enhancing_Generic_Segmentation_With_Learned_Region_Representations_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Or Isaacs,  Oran Shayer,  Michael Lindenbaum</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1339</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adaptive Hierarchical Down-Sampling for Point Cloud Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Nezhadarya_Adaptive_Hierarchical_Down-Sampling_for_Point_Cloud_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ehsan Nezhadarya,  Ehsan Taghavi,  Ryan Razani,  Bingbing Liu,  Jun Luo</td> <td>0.0173</td> <td>0.2152</td> <td>0.0628</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0285</td> <td>0.0096</td></tr>
<tr><td>FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_FBNetV2_Differentiable_Neural_Architecture_Search_for_Spatial_and_Channel_Dimensions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alvin Wan,  Xiaoliang Dai,  Peizhao Zhang,  Zijian He,  Yuandong Tian,  Saining Xie,  Bichen Wu,  Matthew Yu,  Tao Xu,  Kan Chen,  Peter Vajda,  Joseph E. Gonzalez</td> <td>0.0000</td> <td>0.0000</td> <td>0.0126</td> <td>0.2517</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0004</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Learning_Texture_Invariant_Representation_for_Domain_Adaptation_of_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Myeongjin Kim,  Hyeran Byun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0891</td> <td>0.0626</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Putting Visual Object Recognition in Context <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Putting_Visual_Object_Recognition_in_Context_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mengmi Zhang,  Claire Tseng,  Gabriel Kreiman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0127</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0113</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SLV: Spatial Likelihood Voting for Weakly Supervised Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_SLV_Spatial_Likelihood_Voting_for_Weakly_Supervised_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ze Chen,  Zhihang Fu,  Rongxin Jiang,  Yaowu Chen,  Xian-Sheng Hua</td> <td>0.0294</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0090</td></tr>
<tr><td>Universal Weighting Metric Learning for Cross-Modal Matching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Universal_Weighting_Metric_Learning_for_Cross-Modal_Matching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiwei Wei,  Xing Xu,  Yang Yang,  Yanli Ji,  Zheng Wang,  Heng Tao Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0342</td></tr>
<tr><td>IDA-3D: Instance-Depth-Aware 3D Object Detection From Stereo Vision for Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_IDA-3D_Instance-Depth-Aware_3D_Object_Detection_From_Stereo_Vision_for_Autonomous_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wanli Peng,  Hao Pan,  He Liu,  Yi Sun</td> <td>0.2405</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0418</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0234</td> <td>0.0000</td></tr>
<tr><td>Label Decoupling Framework for Salient Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Label_Decoupling_Framework_for_Salient_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jun Wei,  Shuhui Wang,  Zhe Wu,  Chi Su,  Qingming Huang,  Qi Tian</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0095</td></tr>
<tr><td>Transform and Tell: Entity-Aware News Image Captioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tran_Transform_and_Tell_Entity-Aware_News_Image_Captioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Alasdair Tran,  Alexander Mathews,  Lexing Xie</td> <td>0.0000</td> <td>0.0000</td> <td>0.0175</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HAMBox: Delving Into Mining High-Quality Anchors on Face Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_HAMBox_Delving_Into_Mining_High-Quality_Anchors_on_Face_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Liu,  Xu Tang,  Junyu Han,  Jingtuo Liu,  Dinger Rui,  Xiang Wu</td> <td>0.0027</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0360</td></tr>
<tr><td>Hierarchical Feature Embedding for Attribute Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Hierarchical_Feature_Embedding_for_Attribute_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jie Yang,  Jiarou Fan,  Yiru Wang,  Yige Wang,  Weihao Gan,  Lin Liu,  Wei Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0083</td></tr>
<tr><td>Squeeze-and-Attention Networks for Semantic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhong_Squeeze-and-Attention_Networks_for_Semantic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zilong Zhong,  Zhong Qiu Lin,  Rene Bidart,  Xiaodan Hu,  Ibrahim Ben Daya,  Zhifeng Li,  Wei-Shi Zheng,  Jonathan Li,  Alexander Wong</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0623</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0271</td></tr>
<tr><td>Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Beery_Context_R-CNN_Long_Term_Temporal_Context_for_Per-Camera_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sara Beery,  Guanhang Wu,  Vivek Rathod,  Ronny Votel,  Jonathan Huang</td> <td>0.0278</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Mixture Dense Regression for Object Detection and Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Varamesh_Mixture_Dense_Regression_for_Object_Detection_and_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ali Varamesh,  Tinne Tuytelaars</td> <td>0.0441</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1877</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0275</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Syntax-Aware Action Targeting for Video Captioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Syntax-Aware_Action_Targeting_for_Video_Captioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Zheng,  Chaoyue Wang,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0516</td> <td>0.0112</td></tr>
<tr><td>Learning Visual Emotion Representations From Web Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Learning_Visual_Emotion_Representations_From_Web_Data_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zijun Wei,  Jianming Zhang,  Zhe Lin,  Joon-Young Lee,  Niranjan Balasubramanian,  Minh Hoai,  Dimitris Samaras</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0034</td></tr>
<tr><td>The Edge of Depth: Explicit Constraints Between Segmentation and Depth <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_The_Edge_of_Depth_Explicit_Constraints_Between_Segmentation_and_Depth_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shengjie Zhu,  Garrick Brazil,  Xiaoming Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1392</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1297</td> <td>0.0000</td> <td>0.0611</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>A Context-Aware Loss Function for Action Spotting in Soccer Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cioppa_A_Context-Aware_Loss_Function_for_Action_Spotting_in_Soccer_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anthony Cioppa,  Adrien Deliege,  Silvio Giancola,  Bernard Ghanem,  Marc Van Droogenbroeck,  Rikke Gade,  Thomas B. Moeslund</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0518</td></tr>
<tr><td>Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hao_Towards_Learning_a_Generic_Agent_for_Vision-and-Language_Navigation_via_Pre-Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Weituo Hao,  Chunyuan Li,  Xiujun Li,  Lawrence Carin,  Jianfeng Gao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0262</td> <td>0.0000</td></tr>
<tr><td>Video Instance Segmentation Tracking With a Modified VAE Architecture <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Video_Instance_Segmentation_Tracking_With_a_Modified_VAE_Architecture_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chung-Ching Lin,  Ying Hung,  Rogerio Feris,  Linglin He</td> <td>0.0209</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0609</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Deformation-Aware Unpaired Image Translation for Pose Estimation on Laboratory Animals <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deformation-Aware_Unpaired_Image_Translation_for_Pose_Estimation_on_Laboratory_Animals_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Siyuan Li,  Semih Gunel,  Mirela Ostrek,  Pavan Ramdya,  Pascal Fua,  Helge Rhodin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1214</td> <td>0.0000</td> <td>0.1016</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0284</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ZeroQ: A Novel Zero Shot Quantization Framework <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_ZeroQ_A_Novel_Zero_Shot_Quantization_Framework_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yaohui Cai,  Zhewei Yao,  Zhen Dong,  Amir Gholami,  Michael W. Mahoney,  Kurt Keutzer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0115</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0216</td></tr>
<tr><td>Disparity-Aware Domain Adaptation in Stereo Image Restoration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Disparity-Aware_Domain_Adaptation_in_Stereo_Image_Restoration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Yan,  Chenxi Ma,  Bahetiyaer Bare,  Weimin Tan,  Steven C. H. Hoi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0012</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Offset Bin Classification Network for Accurate Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qiu_Offset_Bin_Classification_Network_for_Accurate_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Heqian Qiu,  Hongliang Li,  Qingbo Wu,  Hengcan Shi</td> <td>0.0507</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0691</td></tr>
<tr><td>TBT: Targeted Neural Network Attack With Bit Trojan <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rakin_TBT_Targeted_Neural_Network_Attack_With_Bit_Trojan_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Adnan Siraj Rakin,  Zhezhi He,  Deliang Fan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0446</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0101</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0015</td></tr>
<tr><td>Maintaining Discrimination and Fairness in Class Incremental Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Maintaining_Discrimination_and_Fairness_in_Class_Incremental_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bowen Zhao,  Xi Xiao,  Guojun Gan,  Bin Zhang,  Shu-Tao Xia</td> <td>0.0000</td> <td>0.0000</td> <td>0.0233</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0516</td></tr>
<tr><td>Background Data Resampling for Outlier-Aware Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Background_Data_Resampling_for_Outlier-Aware_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yi Li,  Nuno Vasconcelos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0667</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>STEFANN: Scene Text Editor Using Font Adaptive Neural Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Roy_STEFANN_Scene_Text_Editor_Using_Font_Adaptive_Neural_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Prasun Roy,  Saumik Bhattacharya,  Subhankar Ghosh,  Umapada Pal</td> <td>0.0000</td> <td>0.0000</td> <td>0.0187</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Geometry and Learning Co-Supported Normal Estimation for Unstructured Point Cloud <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Geometry_and_Learning_Co-Supported_Normal_Estimation_for_Unstructured_Point_Cloud_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Haoran Zhou,  Honghua Chen,  Yidan Feng,  Qiong Wang,  Jing Qin,  Haoran Xie,  Fu Lee Wang,  Mingqiang Wei,  Jun Wang</td> <td>0.0000</td> <td>0.0382</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0033</td></tr>
<tr><td>Sequential Motif Profiles and Topological Plots for Offline Signature Verification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zois_Sequential_Motif_Profiles_and_Topological_Plots_for_Offline_Signature_Verification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Elias N. Zois,  Evangelos Zervas,  Dimitrios Tsourounis,  George Economou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0361</td></tr>
<tr><td>Optical Flow in Dense Foggy Scenes Using Semi-Supervised Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Optical_Flow_in_Dense_Foggy_Scenes_Using_Semi-Supervised_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wending Yan,  Aashish Sharma,  Robby T. Tan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3026</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0250</td></tr>
<tr><td>A Spatial RNN Codec for End-to-End Image Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_A_Spatial_RNN_Codec_for_End-to-End_Image_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaoyi Lin,  Jiabao Yao,  Fangdong Chen,  Li Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0089</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0684</td></tr>
<tr><td>Object Relational Graph With Teacher-Recommended Learning for Video Captioning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object_Relational_Graph_With_Teacher-Recommended_Learning_for_Video_Captioning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ziqi Zhang,  Yaya Shi,  Chunfeng Yuan,  Bing Li,  Peijin Wang,  Weiming Hu,  Zheng-Jun Zha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0121</td></tr>
<tr><td>MMTM: Multimodal Transfer Module for CNN Fusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Joze_MMTM_Multimodal_Transfer_Module_for_CNN_Fusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hamid Reza Vaezi Joze,  Amirreza Shaban,  Michael L. Iuzzolino,  Kazuhito Koishida</td> <td>0.0000</td> <td>0.0000</td> <td>0.0767</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0544</td> <td>0.0000</td> <td>0.0198</td></tr>
<tr><td>Generalized Zero-Shot Learning via Over-Complete Distribution <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Keshari_Generalized_Zero-Shot_Learning_via_Over-Complete_Distribution_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rohit Keshari,  Richa Singh,  Mayank Vatsa</td> <td>0.0000</td> <td>0.0000</td> <td>0.0257</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Gait Recognition via Semi-supervised Disentangled Representation Learning to Identity and Covariate Features <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Gait_Recognition_via_Semi-supervised_Disentangled_Representation_Learning_to_Identity_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiang Li,  Yasushi Makihara,  Chi Xu,  Yasushi Yagi,  Mingwu Ren</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0911</td></tr>
<tr><td>Unifying Training and Inference for Panoptic Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Unifying_Training_and_Inference_for_Panoptic_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qizhu Li,  Xiaojuan Qi,  Philip H.S. Torr</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0547</td> <td>0.0000</td> <td>0.0397</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Associate-3Ddet_Perceptual-to-Conceptual_Association_for_3D_Point_Cloud_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Liang Du,  Xiaoqing Ye,  Xiao Tan,  Jianfeng Feng,  Zhenbo Xu,  Errui Ding,  Shilei Wen</td> <td>0.1152</td> <td>0.1498</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0613</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>Interactive Image Segmentation With First Click Attention <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Interactive_Image_Segmentation_With_First_Click_Attention_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zheng Lin,  Zhao Zhang,  Lin-Zhuo Chen,  Ming-Ming Cheng,  Shao-Ping Lu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0081</td></tr>
<tr><td>NETNet: Neighbor Erasing and Transferring Network for Better Single Shot Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_NETNet_Neighbor_Erasing_and_Transferring_Network_for_Better_Single_Shot_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yazhao Li,  Yanwei Pang,  Jianbing Shen,  Jiale Cao,  Ling Shao</td> <td>0.0481</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Scale-Equalizing Pyramid Convolution for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Scale-Equalizing_Pyramid_Convolution_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xinjiang Wang,  Shilong Zhang,  Zhuoran Yu,  Litong Feng,  Wayne Zhang</td> <td>0.0006</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0118</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Cluster Faces via Confidence and Connectivity Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_to_Cluster_Faces_via_Confidence_and_Connectivity_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lei Yang,  Dapeng Chen,  Xiaohang Zhan,  Rui Zhao,  Chen Change Loy,  Dahua Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0065</td></tr>
<tr><td>Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Cross-Modality_Person_Re-Identification_With_Shared-Specific_Feature_Transfer_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yan Lu,  Yue Wu,  Bin Liu,  Tianzhu Zhang,  Baopu Li,  Qi Chu,  Nenghai Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>DPGN: Distribution Propagation Graph Network for Few-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ling Yang,  Liangliang Li,  Zilun Zhang,  Xinyu Zhou,  Erjin Zhou,  Yu Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0241</td></tr>
<tr><td>Density-Aware Graph for Deep Semi-Supervised Visual Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Density-Aware_Graph_for_Deep_Semi-Supervised_Visual_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Suichan Li,  Bin Liu,  Dongdong Chen,  Qi Chu,  Lu Yuan,  Nenghai Yu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0244</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0125</td></tr>
<tr><td>Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Arar_Unsupervised_Multi-Modal_Image_Registration_via_Geometry_Preserving_Image-to-Image_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Moab Arar,  Yiftach Ginger,  Dov Danon,  Amit H. Bermano,  Daniel Cohen-Or</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Binarizing MobileNet via Evolution-Based Searching <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Phan_Binarizing_MobileNet_via_Evolution-Based_Searching_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hai Phan,  Zechun Liu,  Dang Huynh,  Marios Savvides,  Kwang-Ting Cheng,  Zhiqiang Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0522</td> <td>0.0481</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0012</td></tr>
<tr><td>Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Temporal-Context_Enhanced_Detection_of_Heavily_Occluded_Pedestrians_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jialian Wu,  Chunluan Zhou,  Ming Yang,  Qian Zhang,  Yuan Li,  Junsong Yuan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0348</td></tr>
<tr><td>Orderless Recurrent Models for Multi-Label Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yazici_Orderless_Recurrent_Models_for_Multi-Label_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vacit Oguz Yazici,  Abel Gonzalez-Garcia,  Arnau Ramisa,  Bartlomiej Twardowski,  Joost van de Weijer</td> <td>0.0000</td> <td>0.0000</td> <td>0.0130</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0659</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Gold Seeker: Information Gain From Policy Distributions for Goal-Oriented Vision-and-Langauge Reasoning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abbasnejad_Gold_Seeker_Information_Gain_From_Policy_Distributions_for_Goal-Oriented_Vision-and-Langauge_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ehsan Abbasnejad,  Iman Abbasnejad,  Qi Wu,  Javen Shi,  Anton van den Hengel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1104</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Rethinking the Route Towards Weakly Supervised Object Localization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Rethinking_the_Route_Towards_Weakly_Supervised_Object_Localization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chen-Lin Zhang,  Yun-Hao Cao,  Jianxin Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adversarial Feature Hallucination Networks for Few-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kai Li,  Yulun Zhang,  Kunpeng Li,  Yun Fu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1029</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0204</td></tr>
<tr><td>Conditional Gaussian Distribution Learning for Open Set Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Conditional_Gaussian_Distribution_Learning_for_Open_Set_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xin Sun,  Zhenning Yang,  Chi Zhang,  Keck-Voon Ling,  Guohao Peng</td> <td>0.0000</td> <td>0.0000</td> <td>0.0440</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0282</td></tr>
<tr><td>Connect-and-Slice: An Hybrid Approach for Reconstructing 3D Objects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_Connect-and-Slice_An_Hybrid_Approach_for_Reconstructing_3D_Objects_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hao Fang,  Florent Lafarge</td> <td>0.0000</td> <td>0.0674</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0220</td></tr>
<tr><td>Attentive Weights Generation for Few Shot Learning via Information Maximization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Attentive_Weights_Generation_for_Few_Shot_Learning_via_Information_Maximization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yiluan Guo,  Ngai-Man Cheung</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0887</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Assessing_Eye_Aesthetics_for_Automatic_Multi-Reference_Eye_In-Painting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Yan,  Qing Lin,  Weimin Tan,  Shili Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0143</td></tr>
<tr><td>PuppeteerGAN: Arbitrary Portrait Animation With Semantic-Aware Appearance Transformation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_PuppeteerGAN_Arbitrary_Portrait_Animation_With_Semantic-Aware_Appearance_Transformation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhuo Chen,  Chaoyue Wang,  Bo Yuan,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1942</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_SEED_Semantics_Enhanced_Encoder-Decoder_Framework_for_Scene_Text_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhi Qiao,  Yu Zhou,  Dongbao Yang,  Yucan Zhou,  Weiping Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0292</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0279</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0167</td></tr>
<tr><td>Texture and Shape Biased Two-Stream Networks for Clothing Classification and Attribute Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Texture_and_Shape_Biased_Two-Stream_Networks_for_Clothing_Classification_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuwei Zhang,  Peng Zhang,  Chun Yuan,  Zhi Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Distortion Agnostic Deep Watermarking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Distortion_Agnostic_Deep_Watermarking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiyang Luo,  Ruohan Zhan,  Huiwen Chang,  Feng Yang,  Peyman Milanfar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0028</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0032</td></tr>
<tr><td>RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_RMP-SNN_Residual_Membrane_Potential_Neuron_for_Enabling_Deeper_High-Accuracy_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bing Han,  Gopalakrishnan Srinivasan,  Kaushik Roy</td> <td>0.0000</td> <td>0.0000</td> <td>0.0417</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>BFBox: Searching Face-Appropriate Backbone and Feature Pyramid Network for Face Detector <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_BFBox_Searching_Face-Appropriate_Backbone_and_Feature_Pyramid_Network_for_Face_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Liu,  Xu Tang</td> <td>0.0266</td> <td>0.0000</td> <td>0.0000</td> <td>0.0731</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0204</td></tr>
<tr><td>PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_PFCNN_Convolutional_Neural_Networks_on_3D_Surfaces_Using_Parallel_Frames_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yuqi Yang,  Shilin Liu,  Hao Pan,  Yang Liu,  Xin Tong</td> <td>0.0000</td> <td>0.0458</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>iTAML: An Incremental Task-Agnostic Meta-learning Approach <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Rajasegaran_iTAML_An_Incremental_Task-Agnostic_Meta-learning_Approach_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jathushan Rajasegaran,  Salman Khan,  Munawar Hayat,  Fahad Shahbaz Khan,  Mubarak Shah</td> <td>0.0000</td> <td>0.0000</td> <td>0.0408</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Optimal least-squares solution to the hand-eye calibration problem <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Dekel_Optimal_least-squares_solution_to_the_hand-eye_calibration_problem_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Amit Dekel,  Linus Harenstam-Nielsen,  Sergio Caccamo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MnasFPN: Learning Latency-Aware Pyramid Architecture for Object Detection on Mobile Devices <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_MnasFPN_Learning_Latency-Aware_Pyramid_Architecture_for_Object_Detection_on_Mobile_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Chen,  Golnaz Ghiasi,  Hanxiao Liu,  Tsung-Yi Lin,  Dmitry Kalenichenko,  Hartwig Adam,  Quoc V. Le</td> <td>0.0489</td> <td>0.0000</td> <td>0.0000</td> <td>0.2890</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0041</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ulutan_VSGNet_Spatial_Attention_Network_for_Detecting_Human_Object_Interactions_Using_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Oytun Ulutan,  A S M Iftekhar,  B. S. Manjunath</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0242</td></tr>
<tr><td>End-to-End Camera Calibration for Broadcast Videos <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sha_End-to-End_Camera_Calibration_for_Broadcast_Videos_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Long Sha,  Jennifer Hobbs,  Panna Felsen,  Xinyu Wei,  Patrick Lucey,  Sujoy Ganguly</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0238</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0105</td></tr>
<tr><td>Regularizing CNN Transfer Learning With Randomised Regression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhong_Regularizing_CNN_Transfer_Learning_With_Randomised_Regression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang Zhong,  Atsuto Maki</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0805</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0539</td> <td>0.0067</td></tr>
<tr><td>KeypointNet: A Large-Scale 3D Keypoint Dataset Aggregated From Numerous Human Annotations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/You_KeypointNet_A_Large-Scale_3D_Keypoint_Dataset_Aggregated_From_Numerous_Human_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yang You,  Yujing Lou,  Chengkun Li,  Zhoujun Cheng,  Liangwei Li,  Lizhuang Ma,  Cewu Lu,  Weiming Wang</td> <td>0.0487</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0637</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0316</td></tr>
<tr><td>Hierarchical Clustering With Hard-Batch Triplet Loss for Person Re-Identification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zeng_Hierarchical_Clustering_With_Hard-Batch_Triplet_Loss_for_Person_Re-Identification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kaiwei Zeng,  Munan Ning,  Yaohua Wang,  Yang Guo</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0714</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Joint Semantic Segmentation and Boundary Detection Using Iterative Pyramid Contexts <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhen_Joint_Semantic_Segmentation_and_Boundary_Detection_Using_Iterative_Pyramid_Contexts_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mingmin Zhen,  Jinglu Wang,  Lei Zhou,  Shiwei Li,  Tianwei Shen,  Jiaxiang Shang,  Tian Fang,  Long Quan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1325</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0004</td></tr>
<tr><td>Attention-Guided Hierarchical Structure Aggregation for Image Matting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Attention-Guided_Hierarchical_Structure_Aggregation_for_Image_Matting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu Qiao,  Yuhao Liu,  Xin Yang,  Dongsheng Zhou,  Mingliang Xu,  Qiang Zhang,  Xiaopeng Wei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0160</td></tr>
<tr><td>MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_MetaFuse_A_Pre-trained_Fusion_Model_for_Human_Pose_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rongchang Xie,  Chunyu Wang,  Yizhou Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0923</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Prior Guided GAN Based Semantic Inpainting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lahiri_Prior_Guided_GAN_Based_Semantic_Inpainting_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Avisek Lahiri,  Arnav Kumar Jain,  Sanskar Agrawal,  Pabitra Mitra,  Prabir Kumar Biswas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Weakly Supervised Semantic Point Cloud Segmentation: Towards 10x Fewer Labels <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Weakly_Supervised_Semantic_Point_Cloud_Segmentation_Towards_10x_Fewer_Labels_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xun Xu,  Gim Hee Lee</td> <td>0.0000</td> <td>0.1195</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0275</td> <td>0.0122</td></tr>
<tr><td>Physically Realizable Adversarial Examples for LiDAR Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tu_Physically_Realizable_Adversarial_Examples_for_LiDAR_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>James Tu,  Mengye Ren,  Sivabalan Manivasagam,  Ming Liang,  Bin Yang,  Richard Du,  Frank Cheng,  Raquel Urtasun</td> <td>0.0000</td> <td>0.0894</td> <td>0.0037</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0387</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0470</td> <td>0.0000</td></tr>
<tr><td>Combating Noisy Labels by Agreement: A Joint Training Method with Co-Regularization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Combating_Noisy_Labels_by_Agreement_A_Joint_Training_Method_with_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hongxin Wei,  Lei Feng,  Xiangyu Chen,  Bo An</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0107</td></tr>
<tr><td>Light-weight Calibrator: A Separable Component for Unsupervised Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Light-weight_Calibrator_A_Separable_Component_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shaokai Ye,  Kailu Wu,  Mu Zhou,  Yunfei Yang,  Sia Huat Tan,  Kaidi Xu,  Jiebo Song,  Chenglong Bao,  Kaisheng Ma</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0327</td> <td>0.3468</td> <td>0.0000</td> <td>0.0087</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0061</td></tr>
<tr><td>Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Learn_to_Augment_Joint_Data_Augmentation_and_Network_Optimization_for_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Canjie Luo,  Yuanzhi Zhu,  Lianwen Jin,  Yongpan Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td></tr>
<tr><td>Learning Selective Self-Mutual Attention for RGB-D Saliency Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Learning_Selective_Self-Mutual_Attention_for_RGB-D_Saliency_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nian Liu,  Ni Zhang,  Junwei Han</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0101</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0382</td></tr>
<tr><td>Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Cross-domain_Object_Detection_through_Coarse-to-Fine_Feature_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yangtao Zheng,  Di Huang,  Songtao Liu,  Yunhong Wang</td> <td>0.0555</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0113</td></tr>
<tr><td>Estimating Low-Rank Region Likelihood Maps <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Csurka_Estimating_Low-Rank_Region_Likelihood_Maps_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gabriela Csurka,  Zoltan Kato,  Andor Juhasz,  Martin Humenberger</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0009</td></tr>
<tr><td>Neural Head Reenactment with Latent Pose Descriptors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Burkov_Neural_Head_Reenactment_with_Latent_Pose_Descriptors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Egor Burkov,  Igor Pasechnik,  Artur Grigorev,  Victor Lempitsky</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Prajwal_Learning_Individual_Speaking_Styles_for_Accurate_Lip_to_Speech_Synthesis_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>K R Prajwal,  Rudrabha Mukhopadhyay,  Vinay P. Namboodiri,  C.V. Jawahar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Self-Supervised Learning of Video-Induced Visual Invariances <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tschannen_Self-Supervised_Learning_of_Video-Induced_Visual_Invariances_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michael Tschannen,  Josip Djolonga,  Marvin Ritter,  Aravindh Mahendran,  Neil Houlsby,  Sylvain Gelly,  Mario Lucic</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Two-Stage Peer-Regularized Feature Recombination for Arbitrary Image Style Transfer <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Svoboda_Two-Stage_Peer-Regularized_Feature_Recombination_for_Arbitrary_Image_Style_Transfer_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jan Svoboda,  Asha Anoosheh,  Christian Osendorfer,  Jonathan Masci</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bernard_MINA_Convex_Mixed-Integer_Programming_for_Non-Rigid_Shape_Alignment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Florian Bernard,  Zeeshan Khan Suri,  Christian Theobalt</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0029</td></tr>
<tr><td>Improving One-Shot NAS by Suppressing the Posterior Fading <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Improving_One-Shot_NAS_by_Suppressing_the_Posterior_Fading_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xiang Li,  Chen Lin,  Chuming Li,  Ming Sun,  Wei Wu,  Junjie Yan,  Wanli Ouyang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0141</td> <td>0.1812</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Incremental Few-Shot Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Perez-Rua_Incremental_Few-Shot_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Juan-Manuel Perez-Rua,  Xiatian Zhu,  Timothy M. Hospedales,  Tao Xiang</td> <td>0.0421</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0229</td> <td>0.0000</td></tr>
<tr><td>Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chang_Synthetic_Learning_Learn_From_Distributed_Asynchronized_Discriminator_GAN_Without_Sharing_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Chang,  Hui Qu,  Yikai Zhang,  Mert Sabuncu,  Chao Chen,  Tong Zhang,  Dimitris N. Metaxas</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0120</td> <td>0.0000</td> <td>0.0000</td> <td>0.0029</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0101</td></tr>
<tr><td>Exploring Category-Agnostic Clusters for Open-Set Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Exploring_Category-Agnostic_Clusters_for_Open-Set_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yingwei Pan,  Ting Yao,  Yehao Li,  Chong-Wah Ngo,  Tao Mei</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3730</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Regularizing Class-Wise Predictions via Self-Knowledge Distillation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yun_Regularizing_Class-Wise_Predictions_via_Self-Knowledge_Distillation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sukmin Yun,  Jongjin Park,  Kimin Lee,  Jinwoo Shin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0805</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0013</td></tr>
<tr><td>Hierarchical Graph Attention Network for Visual Relationship Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mi_Hierarchical_Graph_Attention_Network_for_Visual_Relationship_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Li Mi,  Zhenzhong Chen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>M2m: Imbalanced Classification via Major-to-Minor Translation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_M2m_Imbalanced_Classification_via_Major-to-Minor_Translation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jaehyung Kim,  Jongheon Jeong,  Jinwoo Shin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0501</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0591</td></tr>
<tr><td>CenterMask: Real-Time Anchor-Free Instance Segmentation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lee_CenterMask_Real-Time_Anchor-Free_Instance_Segmentation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Youngwan Lee,  Jongyoul Park</td> <td>0.0001</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1543</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0614</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0063</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td></tr>
<tr><td>Multi-Path Learning for Object Pose Estimation Across Domains <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sundermeyer_Multi-Path_Learning_for_Object_Pose_Estimation_Across_Domains_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Martin Sundermeyer,  Maximilian Durner,  En Yen Puang,  Zoltan-Csaba Marton,  Narunas Vaskevicius,  Kai O. Arras,  Rudolph Triebel</td> <td>0.0308</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0403</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Incremental Learning in Online Scenario <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Incremental_Learning_in_Online_Scenario_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jiangpeng He,  Runyu Mao,  Zeman Shao,  Fengqing Zhu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0039</td></tr>
<tr><td>Enhanced Transport Distance for Unsupervised Domain Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Enhanced_Transport_Distance_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mengxue Li,  Yi-Ming Zhai,  You-Wei Luo,  Peng-Fei Ge,  Chuan-Xian Ren</td> <td>0.0000</td> <td>0.0000</td> <td>0.0344</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2932</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0267</td></tr>
<tr><td>TESA: Tensor Element Self-Attention via Matricization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Babiloni_TESA_Tensor_Element_Self-Attention_via_Matricization_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Francesca Babiloni,  Ioannis Marras,  Gregory Slabaugh,  Stefanos Zafeiriou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0576</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0337</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0047</td></tr>
<tr><td>Training a Steerable CNN for Guidewire Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Training_a_Steerable_CNN_for_Guidewire_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Donghang Li,  Adrian Barbu</td> <td>0.0000</td> <td>0.0000</td> <td>0.1026</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0284</td></tr>
<tr><td>Superpixel Segmentation With Fully Convolutional Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Superpixel_Segmentation_With_Fully_Convolutional_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Fengting Yang,  Qian Sun,  Hailin Jin,  Zihan Zhou</td> <td>0.0000</td> <td>0.0000</td> <td>0.0361</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0297</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0237</td></tr>
<tr><td>SharinGAN: Combining Synthetic and Real Data for Unsupervised Geometry Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/PNVR_SharinGAN_Combining_Synthetic_and_Real_Data_for_Unsupervised_Geometry_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Koutilya PNVR,  Hao Zhou,  David Jacobs</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0329</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0003</td></tr>
<tr><td>Label Distribution Learning on Auxiliary Label Space Graphs for Facial Expression Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Label_Distribution_Learning_on_Auxiliary_Label_Space_Graphs_for_Facial_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shikai Chen,  Jianfeng Wang,  Yuedong Chen,  Zhongchao Shi,  Xin Geng,  Yong Rui</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0316</td></tr>
<tr><td>Deep Residual Flow for Out of Distribution Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zisselman_Deep_Residual_Flow_for_Out_of_Distribution_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Ev Zisselman,  Aviv Tamar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0108</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0295</td> <td>0.0053</td></tr>
<tr><td>FeatureFlow: Robust Video Interpolation via Structure-to-Texture Generation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Gui_FeatureFlow_Robust_Video_Interpolation_via_Structure-to-Texture_Generation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shurui Gui,  Chaoyue Wang,  Qihua Chen,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0595</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1592</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0091</td></tr>
<tr><td>Learning Nanoscale Motion Patterns of Vesicles in Living Cells <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sekh_Learning_Nanoscale_Motion_Patterns_of_Vesicles_in_Living_Cells_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Arif Ahmed Sekh,  Ida Sundvor Opstad,  Asa Birna Birgisdottir,  Truls Myrmel,  Balpreet Singh Ahluwalia,  Krishna Agarwal,  Dilip K. Prasad</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0871</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0038</td></tr>
<tr><td>Improving Action Segmentation via Graph-Based Temporal Reasoning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Improving_Action_Segmentation_via_Graph-Based_Temporal_Reasoning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yifei Huang,  Yusuke Sugano,  Yoichi Sato</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0047</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0134</td></tr>
<tr><td>Episode-Based Prototype Generating Network for Zero-Shot Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Episode-Based_Prototype_Generating_Network_for_Zero-Shot_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yunlong Yu,  Zhong Ji,  Jungong Han,  Zhongfei Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0168</td></tr>
<tr><td>Learning to Segment the Tail <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Learning_to_Segment_the_Tail_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xinting Hu,  Yi Jiang,  Kaihua Tang,  Jingyuan Chen,  Chunyan Miao,  Hanwang Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0010</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning to Evaluate Perception Models Using Planner-Centric Metrics <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Philion_Learning_to_Evaluate_Perception_Models_Using_Planner-Centric_Metrics_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jonah Philion,  Amlan Kar,  Sanja Fidler</td> <td>0.1599</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0459</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>Where, What, Whether: Multi-Modal Learning Meets Pedestrian Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Where_What_Whether_Multi-Modal_Learning_Meets_Pedestrian_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yan Luo,  Chongyang Zhang,  Muming Zhao,  Hao Zhou,  Jun Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0559</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>CoverNet: Multimodal Behavior Prediction Using Trajectory Sets <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Phan-Minh_CoverNet_Multimodal_Behavior_Prediction_Using_Trajectory_Sets_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tung Phan-Minh,  Elena Corina Grigore,  Freddy A. Boulton,  Oscar Beijbom,  Eric M. Wolff</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0339</td></tr>
<tr><td>Real-World Person Re-Identification via Degradation Invariance Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Real-World_Person_Re-Identification_via_Degradation_Invariance_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yukun Huang,  Zheng-Jun Zha,  Xueyang Fu,  Richang Hong,  Liang Li</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Defending_and_Harnessing_the_Bit-Flip_Based_Adversarial_Weight_Attack_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhezhi He,  Adnan Siraj Rakin,  Jingtao Li,  Chaitali Chakrabarti,  Deliang Fan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0887</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0061</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adversarial Latent Autoencoders <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pidhorskyi_Adversarial_Latent_Autoencoders_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Stanislav Pidhorskyi,  Donald A. Adjeroh,  Gianfranco Doretto</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0742</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Adaptive Fractional Dilated Convolution Network for Image Aesthetics Assessment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Adaptive_Fractional_Dilated_Convolution_Network_for_Image_Aesthetics_Assessment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiuyu Chen,  Wei Zhang,  Ning Zhou,  Peng Lei,  Yi Xu,  Yu Zheng,  Jianping Fan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1191</td></tr>
<tr><td>Deep Generative Model for Robust Imbalance Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Deep_Generative_Model_for_Robust_Imbalance_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xinyue Wang,  Yilin Lyu,  Liping Jing</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0246</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0323</td></tr>
<tr><td>Learning Deep Network for Detecting 3D Object Keypoints and 6D Poses <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Learning_Deep_Network_for_Detecting_3D_Object_Keypoints_and_6D_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wanqing Zhao,  Shaobo Zhang,  Ziyu Guan,  Wei Zhao,  Jinye Peng,  Jianping Fan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0106</td> <td>0.0000</td> <td>0.0000</td> <td>0.0075</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_MetaIQA_Deep_Meta-Learning_for_No-Reference_Image_Quality_Assessment_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hancheng Zhu,  Leida Li,  Jinjian Wu,  Weisheng Dong,  Guangming Shi</td> <td>0.0000</td> <td>0.0000</td> <td>0.0338</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0151</td></tr>
<tr><td>Sketchformer: Transformer-Based Representation for Sketched Structure <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ribeiro_Sketchformer_Transformer-Based_Representation_for_Sketched_Structure_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Leo Sampaio Ferraz Ribeiro,  Tu Bui,  John Collomosse,  Moacir Ponti</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Cylindrical Convolutional Networks for Joint Object Detection and Viewpoint Estimation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Joung_Cylindrical_Convolutional_Networks_for_Joint_Object_Detection_and_Viewpoint_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sunghun Joung,  Seungryong Kim,  Hanjae Kim,  Minsu Kim,  Ig-Jae Kim,  Junghyun Cho,  Kwanghoon Sohn</td> <td>0.0270</td> <td>0.0000</td> <td>0.0468</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0006</td></tr>
<tr><td>Learning a Unified Sample Weighting Network for Object Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Learning_a_Unified_Sample_Weighting_Network_for_Object_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qi Cai,  Yingwei Pan,  Yu Wang,  Jingen Liu,  Ting Yao,  Tao Mei</td> <td>0.0441</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0120</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Old Is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zaheer_Old_Is_Gold_Redefining_the_Adversarially_Learned_One-Class_Classifier_Training_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Muhammad Zaigham Zaheer,  Jin-Ha Lee,  Marcella Astrid,  Seung-Ik Lee</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0252</td> <td>0.0000</td> <td>0.0010</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0078</td></tr>
<tr><td>An Adaptive Neural Network for Unsupervised Mosaic Consistency Analysis in Image Forensics <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bammey_An_Adaptive_Neural_Network_for_Unsupervised_Mosaic_Consistency_Analysis_in_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Quentin Bammey,  Rafael Grompone von Gioi,  Jean-Michel Morel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0166</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>McFlow: Monte Carlo Flow Models for Data Imputation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Richardson_McFlow_Monte_Carlo_Flow_Models_for_Data_Imputation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Trevor W. Richardson,  Wencheng Wu,  Lei Lin,  Beilei Xu,  Edgar A. Bernal</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0505</td> <td>0.0505</td></tr>
<tr><td>Learning to See Through Obstructions <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Learning_to_See_Through_Obstructions_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Yu-Lun Liu,  Wei-Sheng Lai,  Ming-Hsuan Yang,  Yung-Yu Chuang,  Jia-Bin Huang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0420</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1030</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0491</td></tr>
<tr><td>GaitPart: Temporal Part-Based Model for Gait Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_GaitPart_Temporal_Part-Based_Model_for_Gait_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chao Fan,  Yunjie Peng,  Chunshui Cao,  Xu Liu,  Saihui Hou,  Jiannan Chi,  Yongzhen Huang,  Qing Li,  Zhiqiang He</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0052</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0577</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege's Principle <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mittal_EmotiCon_Context-Aware_Multimodal_Emotion_Recognition_Using_Freges_Principle_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Trisha Mittal,  Pooja Guhan,  Uttaran Bhattacharya,  Rohan Chandra,  Aniket Bera,  Dinesh Manocha</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0616</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Can Deep Learning Recognize Subtle Human Activities? <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Jacquot_Can_Deep_Learning_Recognize_Subtle_Human_Activities_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Vincent Jacquot,  Zhuofan Ying,  Gabriel Kreiman</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0007</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2111</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kong_PhysGAN_Generating_Physical-World-Resilient_Adversarial_Examples_for_Autonomous_Driving_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zelun Kong,  Junfeng Guo,  Ang Li,  Cong Liu</td> <td>0.0107</td> <td>0.0000</td> <td>0.0328</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2115</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>ILFO: Adversarial Attack on Adaptive Neural Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Haque_ILFO_Adversarial_Attack_on_Adaptive_Neural_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mirazul Haque,  Anki Chauhan,  Cong Liu,  Wei Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0755</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>On Translation Invariance in CNNs: Convolutional Layers Can Exploit Absolute Spatial Location <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kayhan_On_Translation_Invariance_in_CNNs_Convolutional_Layers_Can_Exploit_Absolute_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Osman Semih Kayhan,  Jan C. van Gemert</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Diverse Image Generation via Self-Conditioned GANs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diverse_Image_Generation_via_Self-Conditioned_GANs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Steven Liu,  Tongzhou Wang,  David Bau,  Jun-Yan Zhu,  Antonio Torralba</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0016</td></tr>
<tr><td>Inducing Hierarchical Compositional Model by Sparsifying Generator Network <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xing_Inducing_Hierarchical_Compositional_Model_by_Sparsifying_Generator_Network_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xianglei Xing,  Tianfu Wu,  Song-Chun Zhu,  Ying Nian Wu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0022</td> <td>0.0142</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0352</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0390</td></tr>
<tr><td>CARP: Compression Through Adaptive Recursive Partitioning for Multi-Dimensional Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CARP_Compression_Through_Adaptive_Recursive_Partitioning_for_Multi-Dimensional_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Rongjie Liu,  Meng Li,  Li Ma</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GrappaNet: Combining Parallel Imaging With Deep Learning for Multi-Coil MRI Reconstruction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Sriram_GrappaNet_Combining_Parallel_Imaging_With_Deep_Learning_for_Multi-Coil_MRI_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anuroop Sriram,  Jure Zbontar,  Tullie Murrell,  C. Lawrence Zitnick,  Aaron Defazio,  Daniel K. Sodickson</td> <td>0.0000</td> <td>0.0000</td> <td>0.0461</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0154</td></tr>
<tr><td>Can Weight Sharing Outperform Random Architecture Search? An Investigation With TuNAS <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bender_Can_Weight_Sharing_Outperform_Random_Architecture_Search_An_Investigation_With_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gabriel Bender,  Hanxiao Liu,  Bo Chen,  Grace Chu,  Shuyang Cheng,  Pieter-Jan Kindermans,  Quoc V. Le</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2376</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0338</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Context Aware Graph Convolution for Skeleton-Based Action Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Context_Aware_Graph_Convolution_for_Skeleton-Based_Action_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Xikun Zhang,  Chang Xu,  Dacheng Tao</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2111</td> <td>0.0000</td> <td>0.0197</td></tr>
<tr><td>Fast(er) Reconstruction of Shredded Text Documents via Self-Supervised Deep Asymmetric Metric Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Paixao_Faster_Reconstruction_of_Shredded_Text_Documents_via_Self-Supervised_Deep_Asymmetric_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Thiago M. Paixao,  Rodrigo F. Berriel,  Maria C. S. Boeres,  Alessandro L. Koerich,  Claudine Badue,  Alberto F. De Souza,  Thiago Oliveira-Santos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0012</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0002</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0145</td></tr>
<tr><td>Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_Revisiting_Pose-Normalization_for_Fine-Grained_Few-Shot_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Luming Tang,  Davis Wertheimer,  Bharath Hariharan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0676</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>RankMI: A Mutual Information Maximizing Ranking Loss <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kemertas_RankMI_A_Mutual_Information_Maximizing_Ranking_Loss_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mete Kemertas,  Leila Pishdad,  Konstantinos G. Derpanis,  Afsaneh Fazly</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td></tr>
<tr><td>Learning Memory-Guided Normality for Anomaly Detection <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Park_Learning_Memory-Guided_Normality_for_Anomaly_Detection_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Hyunjong Park,  Jongyoun Noh,  Bumsub Ham</td> <td>0.0000</td> <td>0.0000</td> <td>0.0332</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0174</td></tr>
<tr><td>Appearance Shock Grammar for Fast Medial Axis Extraction From Real Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Camaro_Appearance_Shock_Grammar_for_Fast_Medial_Axis_Extraction_From_Real_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Charles-Olivier Dufresne Camaro,  Morteza Rezanejad,  Stavros Tsogkas,  Kaleem Siddiqi,  Sven Dickinson</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0081</td></tr>
<tr><td>Generalizing Hand Segmentation in Egocentric Videos With Uncertainty-Guided Model Adaptation <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Generalizing_Hand_Segmentation_in_Egocentric_Videos_With_Uncertainty-Guided_Model_Adaptation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Minjie Cai,  Feng Lu,  Yoichi Sato</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0152</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0056</td></tr>
<tr><td>DeFeat-Net: General Monocular Depth via Simultaneous Unsupervised Representation Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Spencer_DeFeat-Net_General_Monocular_Depth_via_Simultaneous_Unsupervised_Representation_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Jaime Spencer,  Richard Bowden,  Simon Hadfield</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0369</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Visual Motion Segmentation Using Event Surfaces <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mitrokhin_Learning_Visual_Motion_Segmentation_Using_Event_Surfaces_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Anton Mitrokhin,  Zhiyuan Hua,  Cornelia Fermuller,  Yiannis Aloimonos</td> <td>0.0000</td> <td>0.0000</td> <td>0.0270</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Abduallah Mohamed,  Kun Qian,  Mohamed Elhoseiny,  Christian Claudel</td> <td>0.0000</td> <td>0.0000</td> <td>0.0393</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0673</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0410</td> <td>0.0000</td></tr>
<tr><td>Discriminative Multi-Modality Speech Recognition <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Discriminative_Multi-Modality_Speech_Recognition_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Bo Xu,  Cheng Lu,  Yandong Guo,  Jacob Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0117</td></tr>
<tr><td>Clean-Label Backdoor Attacks on Video Recognition Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Shihao Zhao,  Xingjun Ma,  Xiang Zheng,  James Bailey,  Jingjing Chen,  Yu-Gang Jiang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0219</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0296</td> <td>0.0000</td></tr>
<tr><td>Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Cohen_Detecting_Adversarial_Samples_Using_Influence_Functions_and_Nearest_Neighbors_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gilad Cohen,  Guillermo Sapiro,  Raja Giryes</td> <td>0.0000</td> <td>0.0000</td> <td>0.0544</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0555</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.2283</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Unsupervised Model Personalization While Preserving Privacy and Scalability: An Open Problem <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/De_Lange_Unsupervised_Model_Personalization_While_Preserving_Privacy_and_Scalability_An_Open_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Matthias De Lange,  Xu Jia,  Sarah Parisot,  Ales Leonardis,  Gregory Slabaugh,  Tinne Tuytelaars</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0561</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>GIFnets: Differentiable GIF Encoding Framework <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_GIFnets_Differentiable_GIF_Encoding_Framework_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Innfarn Yoo,  Xiyang Luo,  Yilin Wang,  Feng Yang,  Peyman Milanfar</td> <td>0.0000</td> <td>0.0000</td> <td>0.0273</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Invariant Representation for Unsupervised Image Restoration <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Learning_Invariant_Representation_for_Unsupervised_Image_Restoration_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenchao Du,  Hu Chen,  Hongyu Yang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0278</td></tr>
<tr><td>Improved Few-Shot Visual Classification <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Bateni_Improved_Few-Shot_Visual_Classification_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Peyman Bateni,  Raghav Goyal,  Vaden Masrani,  Frank Wood,  Leonid Sigal</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0451</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0014</td></tr>
<tr><td>Learning Weighted Submanifolds With Variational Autoencoders and Riemannian Variational Autoencoders <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Miolane_Learning_Weighted_Submanifolds_With_Variational_Autoencoders_and_Riemannian_Variational_Autoencoders_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Nina Miolane,  Susan Holmes</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Learning Geocentric Object Pose in Oblique Monocular Images <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Christie_Learning_Geocentric_Object_Pose_in_Oblique_Monocular_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Gordon Christie,  Rodrigo Rene Rai Munoz Abujder,  Kevin Foster,  Shea Hagstrom,  Gregory D. Hager,  Myron Z. Brown</td> <td>0.0203</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0398</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0188</td> <td>0.0574</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Understanding_Adversarial_Examples_From_the_Mutual_Influence_of_Images_and_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Chaoning Zhang,  Philipp Benz,  Tooba Imtiaz,  In So Kweon</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1813</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0306</td> <td>0.0000</td></tr>
<tr><td>Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Daras_Your_Local_GAN_Designing_Two_Dimensional_Local_Attention_Mechanisms_for_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Giannis Daras,  Augustus Odena,  Han Zhang,  Alexandros G. Dimakis</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.1618</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>MoreFusion: Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Wada_MoreFusion_Multi-object_Reasoning_for_6D_Pose_Estimation_from_Volumetric_Fusion_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Kentaro Wada,  Edgar Sucar,  Stephen James,  Daniel Lenton,  Andrew J. Davison</td> <td>0.0312</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>HCNAF: Hyper-Conditioned Neural Autoregressive Flow and its Application for Probabilistic Occupancy Map Forecasting <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Oh_HCNAF_Hyper-Conditioned_Neural_Autoregressive_Flow_and_its_Application_for_Probabilistic_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Geunseob Oh,  Jean-Sebastien Valois</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0041</td></tr>
<tr><td>Detail-recovery Image Deraining via Context Aggregation Networks <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_Detail-recovery_Image_Deraining_via_Context_Aggregation_Networks_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Sen Deng,  Mingqiang Wei,  Jun Wang,  Yidan Feng,  Luming Liang,  Haoran Xie,  Fu Lee Wang,  Meng Wang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0013</td></tr>
<tr><td>MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_MCEN_Bridging_Cross-Modal_Gap_between_Cooking_Recipes_and_Dish_Images_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Han Fu,  Rui Wu,  Chenghao Liu,  Jianling Sun</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0181</td></tr>
<tr><td>Hypergraph Attention Networks for Multimodal Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Hypergraph_Attention_Networks_for_Multimodal_Learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Eun-Sol Kim,  Woo Young Kang,  Kyoung-Woon On,  Yu-Jung Heo,  Byoung-Tak Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Moving in the Right Direction: A Regularization for Deep Metric Learning <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Mohan_Moving_in_the_Right_Direction_A_Regularization_for_Deep_Metric_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Deen Dayal Mohan,  Nishant Sankaran,  Dennis Fedorishin,  Srirangaraj Setlur,  Venu Govindaraju</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0150</td></tr>
<tr><td>Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Haase_Rethinking_Depthwise_Separable_Convolutions_How_Intra-Kernel_Correlations_Lead_to_Improved_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Daniel Haase,  Manuel Amthor</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Seeing without Looking: Contextual Rescoring of Object Detections for AP Maximization <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Pato_Seeing_without_Looking_Contextual_Rescoring_of_Object_Detections_for_AP_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Lourenco V. Pato,  Renato Negrinho,  Pedro M. Q. Aguiar</td> <td>0.0443</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0780</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>End-to-End Adversarial-Attention Network for Multi-Modal Clustering <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_End-to-End_Adversarial-Attention_Network_for_Multi-Modal_Clustering_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Runwu Zhou,  Yi-Dong Shen</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0014</td></tr>
<tr><td>Fast Sparse ConvNets <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Erich Elsen,  Marat Dukhan,  Trevor Gale,  Karen Simonyan</td> <td>0.0000</td> <td>0.0000</td> <td>0.0228</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Few Sample Knowledge Distillation for Efficient Network Compression <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Few_Sample_Knowledge_Distillation_for_Efficient_Network_Compression_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Tianhong Li,  Jianguo Li,  Zhuang Liu,  Changshui Zhang</td> <td>0.0000</td> <td>0.0000</td> <td>0.0413</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0347</td> <td>0.0000</td></tr>
<tr><td>Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth Estimation Using Displacement Fields <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Ramamonjisoa_Predicting_Sharp_and_Accurate_Occlusion_Boundaries_in_Monocular_Depth_Estimation_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Michael Ramamonjisoa,  Yuming Du,  Vincent Lepetit</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.3132</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Shape correspondence using anisotropic Chebyshev spectral CNNs <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Shape_correspondence_using_anisotropic_Chebyshev_spectral_CNNs_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qinsong Li,  Shengjun Liu,  Ling Hu,  Xinru Liu</td> <td>0.0000</td> <td>0.0000</td> <td>0.0439</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0182</td></tr>
<tr><td>RetinaTrack: Online Single Stage Joint Detection and Tracking <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_RetinaTrack_Online_Single_Stage_Joint_Detection_and_Tracking_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Zhichao Lu,  Vivek Rathod,  Ronny Votel,  Jonathan Huang</td> <td>0.0442</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>Multimodal Categorization of Crisis Events in Social Media <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Abavisani_Multimodal_Categorization_of_Crisis_Events_in_Social_Media_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mahdi Abavisani,  Liwei Wu,  Shengli Hu,  Joel Tetreault,  Alejandro Jaimes</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0544</td> <td>0.0067</td></tr>
<tr><td>SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Han_SPARE3D_A_Dataset_for_SPAtial_REasoning_on_Three-View_Line_Drawings_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Wenyu Han,  Siyuan Xiang,  Chenhui Liu,  Ruoyu Wang,  Chen Feng</td> <td>0.0169</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>SwapText: Image Based Texts Transfer in Scenes <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_SwapText_Image_Based_Texts_Transfer_in_Scenes_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Qiangpeng Yang,  Jun Huang,  Wei Lin</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0026</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page Text Recognition by learning to unfold <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Yousef_OrigamiNet_Weakly-Supervised_Segmentation-Free_One-Step_Full_Page_Text_Recognition_by_learning_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Mohamed Yousef,  Tom E. Bishop</td> <td>0.0000</td> <td>0.0000</td> <td>0.0140</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0177</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0723</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>
<tr><td>FroDO: From Detections to 3D Objects <a href='http://openaccess.thecvf.com/content_CVPR_2020/papers/Runz_FroDO_From_Detections_to_3D_Objects_CVPR_2020_paper.pdf'>[pdf]</a></td> <td>Martin Runz,  Kejie Li,  Meng Tang,  Lingni Ma,  Chen Kong,  Tanner Schmidt,  Ian Reid,  Lourdes Agapito,  Julian Straub,  Steven Lovegrove,  Richard Newcombe</td> <td>0.0075</td> <td>0.0368</td> <td>0.0000</td> <td>0.0000</td> <td>0.0049</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td></tr>