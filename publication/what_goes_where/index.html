<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>What Goes Where: Predicting Object Distributions From Above</title>

  
  <meta name="description" content="">

  
  <meta itemprop="name" content="What Goes Where: Predicting Object Distributions From Above">
  <meta itemprop="description" content="">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="What Goes Where: Predicting Object Distributions From Above">
  <meta name="twitter:description" content="">
  

  
  <meta name="og:title" content="What Goes Where: Predicting Object Distributions From Above">
  <meta name="og:description" content="">
  <meta name="og:type" content="website">

  <link rel="stylesheet" type="text/css" href="/css/tufte.css"/>
  <link rel="stylesheet" type="text/css" href="/css/hugo-tufte.css"/>
  <link rel="stylesheet" type="text/css" href="/css/latex.css"/>
  <link rel="stylesheet" type="text/css" href="/css/connor.css"/>
</head>

<body>


<nav class="menu">
<ul>

    <li>
        <a href="/">Home</a>
    </li>

    <li>
        <a href="/publication">Publications</a>
    </li>

    <li>
        <a href="/pdfs/cv.pdf">CV</a>
    </li>

</ul>
</nav>


<section>

<h1>What Goes Where: Predicting Object Distributions From Above</h1>
<p class="subtitle"> Authors: Connor Greenwell, Scott Workman, Nathan Jacobs.  </p>
<p class="subtitle"> In: IEEE International Geoscience and Remote Sensing Symposium.  </p>
<p class="subtitle"> May 4, 2018 </p>



<h2 id="abstract">Abstract</h2>

<p>In this work, we propose a cross-view learning approach,
in which images captured from a ground-level view are used
as weakly supervised annotations for interpreting overhead
imagery. The outcome is a convolutional neural network for
overhead imagery that is capable of predicting the type and
count of objects that are likely to be seen from a ground-level
perspective. We demonstrate our approach on a large dataset
of geotagged ground-level and overhead imagery and find that
our network captures semantically meaningful features, despite
being trained without manual annotations.</p>

<h2 id="bibtex">Bibtex</h2>

<pre><code>@inproceedings{greenwell2018objects,
  author = {Greenwell, Connor and Workman, Scott and Jacobs, Nathan},
  title = {What Goes Where: Predicting Object Distributions From Above},
  year = {2018},
  booktitle = {IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
  annotation = {CAREER}
}
</code></pre>


</section>
<footer>
	<hr>
	<div class="copyright">
  <p> &copy; 2014-2018 Connor Greenwell.  All rights reserved.  </p>
  </div>
</footer>

</body>
</html>

